{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct Solve\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=0, weights=array([ 1.99359097, 10.00293406,  3.99871943, -6.00280085]), adam_cache1=array([0., 0., 0., 0.]), adam_cache2=array([0., 0., 0., 0.]))\n",
      "Mean Squared Error\n",
      "3.950023558895854\n",
      "\n",
      "Linear Gradient Solve\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=0, weights=array([0., 0., 0., 0.]), adam_cache1=array([0., 0., 0., 0.]), adam_cache2=array([0., 0., 0., 0.]))\n",
      "Mean Squared Error\n",
      "5331.1088139477515\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=1, weights=array([ 0.49999975,  0.5       ,  0.5       , -0.5       ]), adam_cache1=array([ -0.19955846, -35.01126951, -13.99591788,  21.01040324]), adam_cache2=array([3.98235776e-03, 1.22578899e+02, 1.95885717e+01, 4.41437044e+01]))\n",
      "Mean Squared Error\n",
      "4655.437326573884\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=2, weights=array([ 0.99123728,  0.99916787,  0.99715723, -0.99839682]), adam_cache1=array([ -0.32911109, -64.77136207, -24.84219398,  38.16971617]), adam_cache2=array([6.21365401e-03, 2.33087193e+02, 3.45651112e+01, 8.11956815e+01]))\n",
      "Mean Squared Error\n",
      "4034.46638019073\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=3, weights=array([ 1.46279304,  1.49688418,  1.48889413, -1.49388988]), adam_cache1=array([ -0.39653559, -89.80830792, -32.86374242,  51.86865909]), adam_cache2=array([7.21416371e-03, 3.32167842e+02, 4.55676619e+01, 1.11795212e+02]))\n",
      "Mean Squared Error\n",
      "3468.8617603229804\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=4, weights=array([ 1.8981264 ,  1.99247834,  1.97198421, -1.98497793]), adam_cache1=array([  -0.4100149 , -110.5995023 ,  -38.3620077 ,   62.46343247]), adam_cache2=array([7.48925977e-03, 4.20473023e+02, 5.32390833e+01, 1.36589431e+02]))\n",
      "Mean Squared Error\n",
      "2958.812729328048\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=5, weights=array([ 2.2753808 ,  2.48522674,  2.44244696, -2.46993835]), adam_cache1=array([  -0.37856942, -127.57694813,  -41.61958287,   70.27987121]), adam_cache2=array([7.49090223e-03, 4.98662108e+02, 5.82180100e+01, 1.56229025e+02]))\n",
      "Mean Squared Error\n",
      "2503.899429442244\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=6, weights=array([ 2.5720341 ,  2.97435136,  2.8954776 , -2.94680929]), adam_cache1=array([  -0.31250531, -141.1319807 ,  -42.90473385,   75.61725611]), adam_cache2=array([7.56297572e-03, 5.67399408e+02, 6.11268919e+01, 1.71363038e+02]))\n",
      "Mean Squared Error\n",
      "2102.9704995547686\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=7, weights=array([ 2.77334985,  3.45901884,  3.3254569 , -3.4133779 ]), adam_cache1=array([  -0.22335262, -151.61952495,  -42.47571721,   78.75180654]), adam_cache2=array([7.89067872e-03, 6.27351660e+02, 6.25568498e+01, 1.82632708e+02]))\n",
      "Mean Squared Error\n",
      "1754.0650303035452\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=8, weights=array([ 2.87794378,  3.93834032,  3.72609929, -3.86717859]), adam_cache1=array([-1.22963499e-01, -1.59361930e+02, -4.05846317e+01,  7.99398651e+01]), adam_cache2=array([8.49202860e-03, 6.79185269e+02, 6.30495957e+01, 1.90664305e+02]))\n",
      "Mean Squared Error\n",
      "1454.3852760951736\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=9, weights=array([ 2.89571967,  4.41137213,  4.09079172, -4.30550631]), adam_cache1=array([-2.21434331e-02, -1.64652422e+02, -3.74803663e+01,  7.94207701e+01]), adam_cache2=array([9.26718140e-03, 7.23563297e+02, 6.30775954e+01, 1.96061041e+02]))\n",
      "Mean Squared Error\n",
      "1200.3115496811727\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=10, weights=array([ 2.84201377,  4.87711761,  4.41313531, -4.72545025]), adam_cache1=array([ 7.03739923e-02, -1.67758205e+02, -3.34100674e+01,  7.74193937e+01]), adam_cache2=array([1.00733789e-02, 7.61142240e+02, 6.30249031e+01, 1.99394173e+02]))\n",
      "Mean Squared Error\n",
      "987.4886058581621\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=11, weights=array([ 2.73317836,  5.33453025,  4.68762228, -5.12395296]), adam_cache1=array([ 1.48263715e-01, -1.68923255e+02, -2.86185636e+01,  7.41483091e+01]), adam_cache2=array([1.07845671e-02, 7.92568580e+02, 6.31722724e+01, 2.01193633e+02]))\n",
      "Mean Squared Error\n",
      "811.0259213794602\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=12, weights=array([ 2.58475632,  5.78251838,  4.91029545, -5.49789774]), adam_cache1=array([   0.20747004, -168.37080972,  -23.34547841,   69.8095337 ]), adam_cache2=array([1.13218666e-02, 8.18475179e+02, 6.36905026e+01, 2.01938651e+02]))\n",
      "Mean Squared Error\n",
      "665.8035758804732\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=13, weights=array([ 2.41121514,  6.21995166,  5.07920656, -5.84422392]), adam_cache1=array([   0.24589869, -166.30560568,  -17.82032334,   64.5957917 ]), adam_cache2=array([1.16607205e-02, 8.39477539e+02, 6.46448095e+01, 2.02049016e+02]))\n",
      "Mean Squared Error\n",
      "546.8161360278532\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=14, weights=array([ 2.22620246,  6.64566956,  5.19455139, -6.16006428]), adam_cache1=array([   0.263113  , -162.91586182,  -12.25647799,   58.69124762]), adam_cache2=array([1.18238187e-02, 8.56169984e+02, 6.60103757e+01, 2.01877773e+02]))\n",
      "Mean Squared Error\n",
      "449.46789592257227\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=15, weights=array([ 2.04272723,  7.05849192,  5.25848467, -6.44289313]), adam_cache1=array([   0.26008611, -158.37503713,   -6.84529875,   52.2716851 ]), adam_cache2=array([1.18662112e-02, 8.69121854e+02, 6.76962327e+01, 2.01706194e+02]))\n",
      "Mean Squared Error\n",
      "369.76081086448846\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=16, weights=array([ 1.87297583,  7.45723175,  5.27472474, -6.69066919]), adam_cache1=array([   0.23899604, -152.84337534,   -1.75146457,   45.50414959]), adam_cache2=array([1.18567642e-02, 8.78873770e+02, 6.95727329e+01, 2.01741760e+02]))\n",
      "Mean Squared Error\n",
      "304.365862674506\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=17, weights=array([ 1.72769558,  7.84071008,  5.24809171, -6.90195558]), adam_cache1=array([   0.20302286, -146.46925048,    2.88982809,   38.54612664]), adam_cache2=array([1.18594846e-02, 8.85934085e+02, 7.14978063e+01, 2.02119676e+02]))\n",
      "Mean Squared Error\n",
      "250.60605516134922\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=18, weights=array([ 1.61531365,  8.20777291,  5.1840897 , -7.07600171]), adam_cache1=array([   0.15610444, -139.3903256 ,    6.97377318,   31.54438249]), adam_cache2=array([1.19184669e-02, 8.90775614e+02, 7.33385584e+01, 2.02908000e+02]))\n",
      "Mean Squared Error\n",
      "206.3894972855493\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=19, weights=array([ 1.54114911,  8.55730973,  5.08858634, -7.21277815]), adam_cache1=array([ 1.02628439e-01, -1.31734537e+02,  1.04253104e+01,  2.46336339e+01]), adam_cache2=array([1.20499285e-02, 8.93832753e+02, 7.49865689e+01, 2.04116079e+02]))\n",
      "Mean Squared Error\n",
      "170.1211728292436\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=20, weights=array([ 1.50706299,  8.88827329,  4.96759401, -7.31296362]), adam_cache1=array([ 4.70761636e-02, -1.23620913e+02,  1.31974225e+01,  1.79352290e+01]), adam_cache2=array([1.22429919e-02, 8.95499108e+02, 7.63667326e+01, 2.05705521e+02]))\n",
      "Mean Squared Error\n",
      "140.6074534487247\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=21, weights=array([ 1.51165436,  9.19969993,  4.82713166, -7.37789116]), adam_cache1=array([-6.33290426e-03, -1.15160246e+02,  1.52688382e+01,  1.15560053e+01]), adam_cache2=array([1.24679320e-02, 8.96125720e+02, 7.74403611e+01, 2.07602680e+02]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error\n",
      "116.95852749746362\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=22, weights=array([ 1.55084209,  9.49072976,  4.67313681, -7.40946521]), adam_cache1=array([-5.39414687e-02, -1.06455621e+02,  1.66414800e+01,  5.58745119e+00]), adam_cache2=array([1.26881917e-02, 8.96019991e+02, 7.82036456e+01, 2.09711530e+02]))\n",
      "Mean Squared Error\n",
      "98.49216481543766\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=23, weights=array([ 1.61857358,  9.76062592,  4.51139963, -7.41006341]), adam_cache1=array([-9.28664850e-02, -9.76028250e+01,  1.73378603e+01,  1.05240133e-01]), adam_cache2=array([1.28719224e-02, 8.95445372e+02, 7.86826514e+01, 2.11925870e+02]))\n",
      "Mean Squared Error\n",
      "84.643983959659\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=24, weights=array([ 1.70747836, 10.00879198,  4.34749685, -7.38243484]), adam_cache1=array([ -0.12111908, -88.69064524,  17.39850619,  -4.83084357]), adam_cache2=array([1.29999699e-02, 8.94621855e+02, 7.89259673e+01, 2.14140058e+02]))\n",
      "Mean Squared Error\n",
      "74.89100351148356\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=25, weights=array([ 1.80941667, 10.2347868 ,  4.18671137, -7.32960382]), adam_cache1=array([ -0.13764704, -79.80107743,  16.8794114 ,  -9.17661615]), adam_cache2=array([1.30689942e-02, 8.93727275e+02, 7.89960658e+01, 2.16257704e+02]))\n",
      "Mean Squared Error\n",
      "68.69520477354938\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=26, weights=array([ 1.91596488, 10.43833615,  4.03393202, -7.25478402]), adam_cache1=array([ -0.14231819, -71.0094619 ,  15.84946086, -12.90289762]), adam_cache2=array([1.30899132e-02, 8.92899402e+02, 7.89603649e+01, 2.18198067e+02]))\n",
      "Mean Squared Error\n",
      "65.47202253936752\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=27, weights=array([ 2.01890425, 10.61934045,  3.89353624, -7.16130483]), adam_cache1=array([ -0.13585674, -62.38456487,  14.38776236, -15.99467415]), adam_cache2=array([1.30828612e-02, 8.92238745e+02, 7.88829236e+01, 2.19900119e+02]))\n",
      "Mean Squared Error\n",
      "64.58547753983693\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=28, weights=array([ 2.11074849, 10.77787861,  3.76926631, -7.05254915]), adam_cache1=array([ -0.11973721, -53.98862438,  12.58083446, -18.45008651]), adam_cache2=array([1.30704204e-02, 8.91811980e+02, 7.88175942e+01, 2.21324424e+02]))\n",
      "Mean Squared Error\n",
      "65.36758596333358\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=29, weights=array([ 2.18528442, 10.91420761,  3.66411442, -6.93190061]), adam_cache1=array([ -0.09603602, -45.87737853,  10.51964216, -20.27930189]), adam_cache2=array([1.30711033e-02, 8.91655870e+02, 7.88032750e+01, 2.22453092e+02]))\n",
      "Mean Squared Error\n",
      "67.1555191653047\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=30, weights=array([ 2.23804413, 11.02875816,  3.58023337, -6.8026975 ]), adam_cache1=array([ -0.0672439 , -38.10009213,   8.29652693, -21.50331379]), adam_cache2=array([1.30948521e-02, 8.91781537e+02, 7.88616312e+01, 2.23288152e+02]))\n",
      "Mean Squared Error\n",
      "69.33683903499045\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=31, weights=array([ 2.26661156, 11.12212679,  3.51888633, -6.66819088]), adam_cache1=array([ -0.03604975, -30.69959597,   6.00213116, -22.15270069]), adam_cache2=array([1.31416342e-02, 8.92178915e+02, 7.89973168e+01, 2.23848706e+02]))\n",
      "Mean Squared Error\n",
      "71.39209603677611\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=32, weights=array([ 2.27070324, 11.19506472,  3.48044248, -6.53150479]), adam_cache1=array([-5.11541627e-03, -2.37123499e+01,  3.72245422e+00, -2.22663623e+01]), adam_cache2=array([1.32031819e-02, 8.92821249e+02, 7.92003793e+01, 2.24167249e+02]))\n",
      "Mean Squared Error\n",
      "72.92566084401714\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=33, weights=array([ 2.25202748, 11.24846437,  3.46441777, -6.39559727]), adam_cache1=array([  0.02313506, -17.16853839,   1.53618765, -21.89024274]), adam_cache2=array([1.32669236e-02, 8.93669467e+02, 7.94502462e+01, 2.24285523e+02]))\n",
      "Mean Squared Error\n",
      "73.67937871452676\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=34, weights=array([ 2.2139773 , 11.28334387,  3.4695536 , -6.26322166]), adam_cache1=array([  0.04669105, -11.0922039 ,  -0.48754035, -21.07604522]), adam_cache2=array([1.33205797e-02, 8.94676305e+02, 7.97205268e+01, 2.24250253e+02]))\n",
      "Mean Squared Error\n",
      "73.5281709566977\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=35, weights=array([ 2.16122593, 11.30083037,  3.49392273, -6.13688874]), adam_cache1=array([  0.06408262,  -5.50142114,  -2.29091963, -19.87993959]), adam_cache2=array([1.33559265e-02, 8.95790069e+02, 7.99838461e+01, 2.24109085e+02]))\n",
      "Mean Squared Error\n",
      "72.46061225790568\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=36, weights=array([ 2.09927608, 11.30214271,  3.53505132, -6.01883068]), adam_cache1=array([  0.07445461,  -0.40851215,  -3.8286666 , -18.36126668]), adam_cache2=array([1.33707283e-02, 8.96957952e+02, 8.02160342e+01, 2.23907002e+02]))\n",
      "Mean Squared Error\n",
      "70.54988213519275\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=37, weights=array([ 2.03398983, 11.28857382,  3.5900482 , -5.91096855]), adam_cache1=array([  0.07758823,   4.17969926,  -5.06868471, -16.58124603]), adam_cache2=array([1.33685492e-02, 8.98128842e+02, 8.03991937e+01, 2.23683410e+02]))\n",
      "Mean Squared Error\n",
      "67.92116737452935\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=38, weights=array([ 1.97111319, 11.26147338,  3.65573522, -5.81488533]), adam_cache1=array([  0.07387333,   8.26159705,  -5.9922064 , -14.60169922]), adam_cache2=array([1.33568160e-02, 8.99255595e+02, 8.05233961e+01, 2.23470058e+02]))\n",
      "Mean Squared Error\n",
      "64.72094628363641\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=39, weights=array([ 1.91581447, 11.222231  ,  3.72877529, -5.7318061 ]), adam_cache1=array([  0.06423597,  11.84045082,  -6.59346477, -12.4838062 ]), adam_cache2=array([1.33439655e-02, 9.00296753e+02, 8.05869877e+01, 2.23289848e+02]))\n",
      "Mean Squared Error\n",
      "61.09215784981163\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=40, weights=array([ 1.87227085, 11.17226007,  3.80579669, -5.66258706]), adam_cache1=array([  0.05002695,  14.92406696,  -6.87894978, -10.28691687]), adam_cache2=array([1.33366828e-02, 9.01217750e+02, 8.05956714e+01, 2.23156525e+02]))\n",
      "Mean Squared Error\n",
      "57.15753676264754\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=41, weights=array([ 1.84334849, 11.11298248,  3.88351254, -5.60771409]), adam_cache1=array([ 0.03288011, 17.52441822, -6.86630367, -8.0674429 ]), adam_cache2=array([1.33380941e-02, 9.01991599e+02, 8.05606718e+01, 2.23075165e+02]))\n",
      "Mean Squared Error\n",
      "53.01173094737911\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=42, weights=array([ 1.83041736, 11.04581435,  3.95883379, -5.56731123]), adam_cache1=array([ 1.45528266e-02,  1.96572569e+01, -6.58290893e+00, -5.87785547e+00]), adam_cache2=array([1.33473740e-02, 9.02599148e+02, 8.04963710e+01, 2.23043315e+02]))\n",
      "Mean Squared Error\n",
      "48.721430983079536\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=43, weights=array([ 1.83332121, 10.97215258,  4.02897327, -5.5411584 ]), adam_cache1=array([-3.23613490e-03,  2.13417165e+01, -6.06422175e+00, -3.76581272e+00]), adam_cache2=array([1.33607055e-02, 9.03028934e+02, 8.04178236e+01, 2.23052608e+02]))\n",
      "Mean Squared Error\n",
      "44.33173451391239\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=44, weights=array([ 1.85049612, 10.89336254,  4.09153688, -5.52871755]), adam_cache1=array([-1.89555244e-02,  2.25999066e+01, -5.35190809e+00, -1.77343673e+00]), adam_cache2=array([1.33730826e-02, 9.03276717e+02, 8.03385270e+01, 2.23090634e+02]))\n",
      "Mean Squared Error\n",
      "39.87635817985425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=45, weights=array([ 1.87921018, 10.81076661,  4.14459769, -5.52916564]), adam_cache1=array([-0.03138377, 23.45650463, -4.49184692,  0.06324589]), adam_cache2=array([1.33802267e-02, 9.03344752e+02, 8.02687426e+01, 2.23142884e+02]))\n",
      "Mean Squared Error\n",
      "35.38906796932144\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=46, weights=array([ 1.91588867, 10.72563377,  4.18674869, -5.54143294]), adam_cache1=array([-0.03969491, 23.93834887, -3.53207374,  1.7146919 ]), adam_cache2=array([1.33799556e-02, 9.03240880e+02, 8.02145439e+01, 2.23194562e+02]))\n",
      "Mean Squared Error\n",
      "30.913805530712473\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=47, weights=array([ 1.95648974, 10.63917009,  4.21713133, -5.56424501]), adam_cache1=array([-0.04350342, 24.07403522, -2.52074515,  3.15805652]), adam_cache2=array([1.33726253e-02, 9.02977487e+02, 8.01776417e+01, 2.23232136e+02]))\n",
      "Mean Squared Error\n",
      "26.51143223881728\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=48, weights=array([ 1.99689947, 10.55251029,  4.23543837, -5.59616686]), adam_cache1=array([-0.04286691, 23.89352144, -1.50420713,  4.37724014]), adam_cache2=array([1.33606320e-02, 9.02570414e+02, 8.01559045e+01, 2.23244523e+02]))\n",
      "Mean Squared Error\n",
      "22.261753287377704\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=49, weights=array([ 2.03331553, 10.4667102 ,  4.24189188, -5.63564795]), adam_cache1=array([-0.03824904, 23.42774106, -0.52524647,  5.36277573]), adam_cache2=array([1.33472823e-02, 9.02037856e+02, 8.01443965e+01, 2.23223845e+02]))\n",
      "Mean Squared Error\n",
      "18.26041198084683\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=50, weights=array([ 2.06258722, 10.38274031,  4.23719898, -5.68106698]), adam_cache1=array([-0.03044771, 22.70822981,  0.37840607,  6.11157002]), adam_cache2=array([1.33355162e-02, 9.01399316e+02, 8.01366939e+01, 2.23165762e+02]))\n",
      "Mean Squared Error\n",
      "14.611194570055654\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=51, weights=array([ 2.0824792 , 10.3014804 ,  4.22248887, -5.73077557]), adam_cache1=array([-2.04964112e-02,  2.17667667e+01,  1.17526774e+00,  6.62651371e+00]), adam_cache2=array([1.33269507e-02, 9.00674637e+02, 8.01262300e+01, 2.23069407e+02]))\n",
      "Mean Squared Error\n",
      "11.415088928233583\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=52, weights=array([ 2.09183229, 10.22371526,  4.19923582, -5.78313989]), adam_cache1=array([-9.54905899e-03,  2.06350321e+01,  1.84095640e+00,  6.91597800e+00]), adam_cache2=array([1.33215407e-02, 8.99883153e+02, 8.01074464e+01, 2.22936990e+02]))\n",
      "Mean Squared Error\n",
      "8.757973162830963\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=53, weights=array([ 2.09060696, 10.1501315 ,  4.16917248, -5.83657971]), adam_cache1=array([1.23980293e-03, 1.93442851e+01, 2.35868819e+00, 6.99321551e+00]), adam_cache2=array([1.33178898e-02, 8.99042985e+02, 8.00765952e+01, 2.22773164e+02]))\n",
      "Mean Squared Error\n",
      "6.69901436617565\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=54, weights=array([ 2.07981091, 10.08131568,  4.13419799, -5.8896041 ]), adam_cache1=array([1.08271230e-02, 1.79250624e+01, 2.71942210e+00, 6.87568457e+00]), adam_cache2=array([1.33140029e-02, 8.98170485e+02, 8.00321120e+01, 2.22584239e+02]))\n",
      "Mean Squared Error\n",
      "5.261727800921262\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=55, weights=array([ 2.06132653, 10.01775346,  4.0962847 , -5.94084325]), adam_cache1=array([ 0.01837503, 16.40689964,  2.92166839,  6.58431603]), adam_cache2=array([1.33081376e-02, 8.97279841e+02, 7.99745654e+01, 2.22377352e+02]))\n",
      "Mean Squared Error\n",
      "4.429215561754765\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=56, weights=array([ 2.03766296,  9.95983005,  4.05738737, -5.98907563]), adam_cache1=array([ 0.02331785, 14.81807906,  2.97098974,  6.14274221]), adam_cache2=array([1.32994268e-02, 8.96382830e+02, 7.99062523e+01, 2.22159677e+02]))\n",
      "Mean Squared Error\n",
      "4.144437359479125\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=57, weights=array([ 2.01166211,  9.90783178,  4.0193581 , -6.03325019]), adam_cache1=array([ 0.02539767, 13.18540279,  2.87923442,  5.57650762]), adam_cache2=array([1.32880736e-02, 8.95488724e+02, 7.98305626e+01, 2.21937748e+02]))\n",
      "Mean Squared Error\n",
      "4.3155647181666605\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=58, weights=array([ 1.98618908,  9.86194881,  3.9838703 , -6.07250302]), adam_cache1=array([ 0.02466683, 11.53399501,  2.66354839,  4.91228111]), adam_cache2=array([1.32751127e-02, 8.94604315e+02, 7.97512539e+01, 2.21716946e+02]))\n",
      "Mean Squared Error\n",
      "4.82566091819637\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=59, weights=array([ 1.96383766,  9.82227889,  3.95235464, -6.10616819]), adam_cache1=array([0.02145921, 9.88713303, 2.34522012, 4.1770884 ]), adam_cache2=array([1.32618925e-02, 8.93734061e+02, 7.96717727e+01, 2.21501181e+02]))\n",
      "Mean Squared Error\n",
      "5.545249176471396\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=60, weights=array([ 1.9466796 ,  9.78883207,  3.92594941, -6.13378267]), adam_cache1=array([0.01633499, 8.26610856, 1.94841669, 3.39758353]), adam_cache2=array([1.32495176e-02, 8.92880309e+02, 7.95947345e+01, 2.21292770e+02]))\n",
      "Mean Squared Error\n",
      "6.3459021686816515\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=61, weights=array([ 1.93608268,  9.76153634,  3.90546723, -6.15508543]), adam_cache1=array([0.01000566, 6.69011933, 1.49887266, 2.59937571]), adam_cache2=array([1.32384732e-02, 8.92043585e+02, 7.95216271e+01, 2.21092495e+02]))\n",
      "Mean Squared Error\n",
      "7.112886816671979\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=62, weights=array([ 1.93261423,  9.74024397,  3.89137904, -6.17001096]), adam_cache1=array([3.24851199e-03, 5.17619124e+00, 1.02259339e+00, 1.80642686e+00]), adam_cache2=array([1.32285486e-02, 8.91222930e+02, 7.94527586e+01, 2.20899812e+02]))\n",
      "Mean Squared Error\n",
      "7.755145490745861\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=63, weights=array([ 1.9360368 ,  9.72473851,  3.88381528, -6.17867762]), adam_cache1=array([-3.18011085e-03,  3.73913052e+00,  5.44631941e-01,  1.04053207e+00]), adam_cache2=array([1.32190456e-02, 8.90416245e+02, 7.93874211e+01, 2.20713164e+02]))\n",
      "Mean Squared Error\n",
      "8.21143146566583\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=64, weights=array([ 1.94539085,  9.71474231,  3.88258356, -6.18137158]), adam_cache1=array([-0.00862327,  2.39150521,  0.08799272,  0.32089257]), adam_cache2=array([1.32091457e-02, 8.89620640e+02, 7.93242082e+01, 2.20530346e+02]))\n",
      "Mean Squared Error\n",
      "8.452123079671829\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=65, weights=array([ 1.95915009,  9.7099244 ,  3.88720078, -6.17852691]), adam_cache1=array([-0.01258578,  1.14365475, -0.32729371, -0.33621212]), adam_cache2=array([1.31982644e-02, 8.88832767e+02, 7.92614072e+01, 2.20348880e+02]))\n",
      "Mean Squared Error\n",
      "8.476966341766875\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=66, weights=array([ 1.97542781,  9.70990858,  3.89693725, -6.17070277]), adam_cache1=array([-0.01477473,  0.00372616, -0.68489076, -0.91764971]), adam_cache2=array([1.31862547e-02, 8.88049112e+02, 7.91973813e+01, 2.20166361e+02]))\n",
      "Mean Squared Error\n",
      "8.309597280558526\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=67, weights=array([ 1.99220895,  9.71428167,  3.91086995, -6.15855844]), adam_cache1=array([-0.01511539, -1.02226494, -0.97264949, -1.41355825]), adam_cache2=array([1.31733990e-02, 8.87266252e+02, 7.91308751e+01, 2.19980731e+02]))\n",
      "Mean Squared Error\n",
      "7.990073910224694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=68, weights=array([ 2.00758157,  9.72260172,  3.92794172, -6.14282719]), adam_cache1=array([-0.01374219, -1.93035067, -1.18286649, -1.81736957]), adam_cache2=array([1.31602275e-02, 8.86481059e+02, 7.90611988e+01, 2.19790471e+02]))\n",
      "Mean Squared Error\n",
      "7.56676466796805\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=69, weights=array([ 2.01994226,  9.73440609,  3.94702303, -6.12428964]), adam_cache1=array([-0.01096752, -2.71850683, -1.31230892, -2.12573883]), adam_cache2=array([1.31472634e-02, 8.85690852e+02, 7.89882745e+01, 2.19594700e+02]))\n",
      "Mean Squared Error\n",
      "7.088816003756412\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=70, weights=array([ 2.02815421,  9.74921932,  3.96697339, -6.10374752]), adam_cache1=array([-0.007233  , -3.3865309 , -1.36202061, -2.33838787]), adam_cache2=array([1.31348119e-02, 8.84893497e+02, 7.89125603e+01, 2.19393187e+02]))\n",
      "Mean Squared Error\n",
      "6.600120520295204\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=71, weights=array([ 2.03164206,  9.76656067,  3.98669926, -6.08199856]), adam_cache1=array([-3.04992012e-03, -3.93590477e+00, -1.33693286e+00, -2.45787254e+00]), adam_cache2=array([1.31228741e-02, 8.84087463e+02, 7.88348824e+01, 2.19186278e+02]))\n",
      "Mean Squared Error\n",
      "6.135313933435814\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=72, weights=array([ 2.03041659,  9.78595121,  4.00520586, -6.05981309]), adam_cache1=array([ 1.06398595e-03, -4.36964482e+00, -1.24531136e+00, -2.48928520e+00]), adam_cache2=array([1.31112021e-02, 8.83271823e+02, 7.87562245e+01, 2.18974775e+02]))\n",
      "Mean Squared Error\n",
      "5.717928361435934\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=73, weights=array([ 2.0250301 ,  9.80692051,  4.02164066, -6.03791299]), adam_cache1=array([ 4.64383129e-03, -4.69214200e+00, -1.09807706e+00, -2.43990522e+00]), adam_cache2=array([1.30994497e-02, 8.82446229e+02, 7.86775198e+01, 2.18759783e+02]))\n",
      "Mean Squared Error\n",
      "5.360487686581517\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=74, weights=array([ 2.01647245,  9.82901268,  4.03532679, -6.01695346]), adam_cache1=array([ 0.0073265 , -4.90899483, -0.90804276, -2.3188107 ]), adam_cache2=array([1.30873406e-02, 8.81610852e+02, 7.85994859e+01, 2.18542533e+02]))\n",
      "Mean Squared Error\n",
      "5.06608916973277\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=75, weights=array([ 2.00602337,  9.85179187,  4.04578517, -5.99750798]), adam_cache1=array([ 0.00888429, -5.02683759, -0.68910907, -2.13646519]), adam_cache2=array([1.30747779e-02, 8.80766298e+02, 7.85225281e+01, 2.18324236e+02]))\n",
      "Mean Squared Error\n",
      "4.830890053199848\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=76, weights=array([ 1.995081  ,  9.87484714,  4.05274491, -5.98005672]), adam_cache1=array([ 0.00924034, -5.05316661, -0.45546336, -1.90429311]), adam_cache2=array([1.30618580e-02, 8.79913517e+02, 7.84467194e+01, 2.18105946e+02]))\n",
      "Mean Squared Error\n",
      "4.646904038853377\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=77, weights=array([ 1.98498829,  9.89779662,  4.05614199, -5.96497862]), adam_cache1=array([ 0.00846546, -4.99616698, -0.22082245, -1.6342571 ]), adam_cache2=array([1.30487984e-02, 8.79053702e+02, 7.83718483e+01, 2.17888474e+02]))\n",
      "Mean Squared Error\n",
      "4.504591471450948\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=78, weights=array([ 1.97687757,  9.92029096,  4.05610724, -5.95254706]), adam_cache1=array([ 6.75778583e-03, -4.86454185e+00,  2.24451592e-03, -1.33844982e+00]), adam_cache2=array([1.30358237e-02, 8.78188190e+02, 7.82975160e+01, 2.17672338e+02]))\n",
      "Mean Squared Error\n",
      "4.394871244252368\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=79, weights=array([ 1.97155092,  9.94201617,  4.05294459, -5.94292919]), adam_cache1=array([ 4.40899552e-03, -4.66734679e+00,  2.02883121e-01, -1.02871155e+00]), adam_cache2=array([1.30230678e-02, 8.77318369e+02, 7.82232530e+01, 2.17457759e+02]))\n",
      "Mean Squared Error\n",
      "4.310360074811642\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=80, weights=array([ 1.96940844,  9.96269564,  4.04710178, -5.93618862]), adam_cache1=array([ 1.76188619e-03, -4.41383082e+00,  3.72388291e-01, -7.16283628e-01]), adam_cache2=array([1.30105315e-02, 8.76445597e+02, 7.81486319e+01, 2.17244693e+02]))\n",
      "Mean Squared Error\n",
      "4.245823538851228\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=81, weights=array([ 1.97043032,  9.98209156,  4.03913523, -5.93229118]), adam_cache1=array([-8.34973826e-04, -4.11328623e+00,  5.04492535e-01, -4.11505824e-01]), adam_cache2=array([1.29981069e-02, 8.75571135e+02, 7.80733510e+01, 2.17032884e+02]))\n",
      "Mean Squared Error\n",
      "4.197973699754667\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=82, weights=array([ 1.97421127, 10.0000056 ,  4.02967156, -5.93111341]), adam_cache1=array([-3.06985791e-03, -3.77490845e+00,  5.95502624e-01, -1.23564343e-01]), adam_cache2=array([1.29856463e-02, 8.74696096e+02, 7.79972787e+01, 2.16821942e+02]))\n",
      "Mean Squared Error\n",
      "4.16484752995766\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=83, weights=array([ 1.98003989, 10.01627903,  4.019368  , -5.93245342]), adam_cache1=array([-0.00470278, -3.40766751,  0.6442879 ,  0.13970527]), adam_cache2=array([1.29730370e-02, 8.73821411e+02, 7.79204551e+01, 2.16611416e+02]))\n",
      "Mean Squared Error\n",
      "4.145036106014584\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=84, weights=array([ 1.98701006, 10.03079212,  4.00887391, -5.93604333]), adam_cache1=array([-0.00558897, -3.02019204,  0.65213116,  0.37195779]), adam_cache2=array([1.29602479e-02, 8.72947807e+02, 7.78430570e+01, 2.16400867e+02]))\n",
      "Mean Squared Error\n",
      "4.137007335234735\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=85, weights=array([ 1.99414856, 10.04346312,  3.99879544, -5.94156318]), adam_cache1=array([-0.00568882, -2.62066683,  0.62245973,  0.56842   ]), adam_cache2=array([1.29473311e-02, 8.72075810e+02, 7.77653403e+01, 2.16189926e+02]))\n",
      "Mean Squared Error\n",
      "4.138688581423056\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=86, weights=array([ 2.00054199, 10.05424659,  3.98966509, -5.94865544]), adam_cache1=array([-0.00506412, -2.21674438,  0.56047982,  0.72591594]), adam_cache2=array([1.29343840e-02, 8.71205747e+02, 7.76875749e+01, 2.15978330e+02]))\n",
      "Mean Squared Error\n",
      "4.1473739283467275\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=87, weights=array([ 2.00544707, 10.06313138,  3.98191752, -5.95693972]), adam_cache1=array([-0.00386191, -1.81547095,  0.47274075,  0.84283868]), adam_cache2=array([1.29214981e-02, 8.70337767e+02, 7.76099878e+01, 2.15765943e+02]))\n",
      "Mean Squared Error\n",
      "4.159921765597001\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=88, weights=array([ 2.00837163, 10.07013815,  3.9758726 , -5.96602723]), adam_cache1=array([-0.00228893, -1.42322723,  0.36665831,  0.91907335]), adam_cache2=array([1.29087174e-02, 8.69471868e+02, 7.75327236e+01, 2.15552754e+02]))\n",
      "Mean Squared Error\n",
      "4.173136147772208\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=89, weights=array([ 2.0091177 , 10.07531664,  3.97172618, -5.97553449]), adam_cache1=array([-5.80489618e-04, -1.04568347e+00,  2.50026280e-01,  9.55877345e-01]), adam_cache2=array([1.28960276e-02, 8.68607929e+02, 7.74558304e+01, 2.15338858e+02]))\n",
      "Mean Squared Error\n",
      "4.184194320537126\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=90, weights=array([ 2.00778386, 10.07874255,  3.96954859, -5.9850958 ]), adam_cache1=array([ 0.00103178, -0.68776887,  0.13054458,  0.9557246 ]), adam_cache2=array([1.28833731e-02, 8.67745740e+02, 7.73792672e+01, 2.15124430e+02]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error\n",
      "4.190994294404467\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=91, weights=array([ 2.00472924, 10.08051436,  3.9692904 , -5.99437433]), adam_cache1=array([ 0.00234931, -0.3536547 ,  0.01538927,  0.92212156]), adam_cache2=array([1.28706916e-02, 8.66885034e+02, 7.73029304e+01, 2.14909689e+02]))\n",
      "Mean Squared Error\n",
      "4.1923398450044544\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=92, weights=array([ 2.00050638, 10.08074992,  3.97079472, -6.00307131]), adam_cache1=array([ 0.00322932, -0.04675044, -0.0891542 ,  0.85940304]), adam_cache2=array([1.28579452e-02, 8.66025523e+02, 7.72266884e+01, 2.14694866e+02]))\n",
      "Mean Squared Error\n",
      "4.187938072014309\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=93, weights=array([ 1.99577393, 10.079583  ,  3.97381479, -6.01093333]), adam_cache1=array([ 0.00359862,  0.23028789, -0.17797805,  0.7725161 ]), adam_cache2=array([1.28451352e-02, 8.65166915e+02, 7.71504170e+01, 2.14480172e+02]))\n",
      "Mean Squared Error\n",
      "4.178238049223686\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=94, weights=array([ 1.99120185, 10.07715983,  3.97803554, -6.01775754]), adam_cache1=array([ 0.00345727,  0.47553805, -0.24734896,  0.66679999]), adam_cache2=array([1.28322948e-02, 8.64308946e+02, 7.70740265e+01, 2.14265773e+02]))\n",
      "Mean Squared Error\n",
      "4.164174355898419\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=95, weights=array([ 1.98738214, 10.07363568,  3.98309752, -6.02339471]), adam_cache1=array([ 0.0028724 ,  0.68778185, -0.29500975,  0.54777008]), adam_cache2=array([1.28194683e-02, 8.63451386e+02, 7.69974765e+01, 2.14051781e+02]))\n",
      "Mean Squared Error\n",
      "4.146890574429912\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=96, weights=array([ 1.98475652, 10.06917157,  3.98862171, -6.02775019]), adam_cache1=array([ 0.00196365,  0.86646641, -0.32018703,  0.42091251]), adam_cache2=array([1.28066874e-02, 8.62594059e+02, 7.69207780e+01, 2.13838249e+02]))\n",
      "Mean Squared Error\n",
      "4.127507335038479\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=97, weights=array([ 1.98356948, 10.06393104,  3.99423365, -6.03078293]), adam_cache1=array([ 8.82957546e-04,  1.01165766e+00, -3.23511348e-01,  2.91496073e-01]), adam_cache2=array([1.27939589e-02, 8.61736839e+02, 7.68439822e+01, 2.13625173e+02]))\n",
      "Mean Squared Error\n",
      "4.106974847010079\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=98, weights=array([ 1.98385128, 10.05807725,  3.99958547, -6.03250264]), adam_cache1=array([-2.08489575e-04,  1.12398742e+00, -3.06860879e-01,  1.64406391e-01]), adam_cache2=array([1.27812656e-02, 8.60879661e+02, 7.67671628e+01, 2.13412507e+02]))\n",
      "Mean Squared Error\n",
      "4.086020655059304\n",
      "\n",
      "Weights\n",
      "Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=99, weights=array([ 1.98543106, 10.05177014,  4.00437468, -6.03296541]), adam_cache1=array([-1.16258364e-03,  1.20459534e+00, -2.73143570e-01,  4.40065081e-02]), adam_cache2=array([1.27685794e-02, 8.60022506e+02, 7.66903966e+01, 2.13200175e+02]))\n",
      "Mean Squared Error\n",
      "4.065177894837399\n",
      "\n",
      "DNN Gradient Solve\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=0, weights=array([[-0.14378639,  0.21431727, -0.05400054,  0.70574924],\n",
      "       [-0.06733567,  0.90246919,  0.55484501,  0.41608172]]), adam_cache1=array([[0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.]]), adam_cache2=array([[0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=0, weights=array([[-0.78878505,  0.90289039,  0.46411924]]), adam_cache1=array([[0., 0., 0.]]), adam_cache2=array([[0., 0., 0.]]))]\n",
      "Mean Squared Error\n",
      "5236.267840848015\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=1, weights=array([[ 0.35621343,  0.71431726,  0.44599946,  0.20574924],\n",
      "       [ 0.43266397,  1.40246918,  1.05484501, -0.08391827]]), adam_cache1=array([[ -0.26608516, -29.67600082, -11.97713514,  21.59472278],\n",
      "       [ -0.13677109, -15.25377583,  -6.15611031,  11.10054396]]), adam_cache2=array([[7.08013127e-03, 8.80665025e+01, 1.43451766e+01, 4.66332052e+01],\n",
      "       [1.87063318e-03, 2.32677677e+01, 3.78976942e+00, 1.23222076e+01]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=1, weights=array([[-0.28878522,  0.40289039,  0.96411924]]), adam_cache1=array([[ -0.29533326,  10.59448483, -27.05100389]]), adam_cache2=array([[8.72217341e-03, 1.12243109e+01, 7.31756811e+01]]))]\n",
      "Mean Squared Error\n",
      "3991.8826102152275\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=2, weights=array([[ 0.77403164,  1.16310297,  0.88075863, -0.24202394],\n",
      "       [ 0.93287064,  1.88906704,  1.54943421, -0.57117093]]), adam_cache1=array([[ -0.30856726, -38.50088555, -14.73033099,  27.90314797],\n",
      "       [ -0.28884801, -41.94819536, -14.99506263,  30.25364512]]), adam_cache2=array([[7.55040244e-03, 1.01884706e+02, 1.58917999e+01, 5.37571007e+01],\n",
      "       [4.61620244e-03, 1.02880195e+02, 1.27248565e+01, 5.33694327e+01]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=2, weights=array([[0.18826047, 0.58444534, 1.44756566]]), adam_cache1=array([[ -0.43846024, -11.48493019, -77.58102419]]), adam_cache2=array([[1.16946092e-02, 5.53969859e+01, 3.56500313e+02]]))]\n",
      "Mean Squared Error\n",
      "2464.5368380300147\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=3, weights=array([[ 1.09641505,  1.60696391,  1.2732828 , -0.68803953],\n",
      "       [ 1.31912433,  2.37998358,  2.03890401, -1.06073707]]), adam_cache1=array([[ -0.27720262, -48.12761793, -15.79530453,  35.4113965 ],\n",
      "       [ -0.25968948, -71.13402149, -19.78237294,  52.73599217]]), adam_cache2=array([[7.54287783e-03, 1.19945291e+02, 1.65200559e+01, 6.43093843e+01],\n",
      "       [4.61159373e-03, 2.14204065e+02, 1.66645379e+01, 1.18380398e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=3, weights=array([[0.5572676 , 0.92775405, 1.93744433]]), adam_cache1=array([[  -0.39488129,  -45.24962896, -130.18174829]]), adam_cache2=array([[1.16829217e-02, 1.77234685e+02, 7.20462606e+02]]))]\n",
      "Mean Squared Error\n",
      "969.46236369838\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=4, weights=array([[ 1.13955125,  2.04825397,  1.50238322, -1.13782224],\n",
      "       [ 1.2064319 ,  2.87330545,  2.23473693, -1.55537041]]), adam_cache1=array([[-5.03460373e-02, -5.59812057e+01, -1.05363928e+01,\n",
      "         4.26160469e+01],\n",
      "       [ 1.81168655e-01, -9.04729454e+01, -1.01210422e+01,\n",
      "         6.99033826e+01]]), adam_cache2=array([[1.15008625e-02, 1.35868987e+02, 1.78573205e+01, 7.57922753e+01],\n",
      "       [2.18202859e-02, 2.83962416e+02, 2.25508657e+01, 1.68621819e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=4, weights=array([[0.65916508, 1.31723184, 2.42079988]]), adam_cache1=array([[-1.41374247e-01, -6.53536983e+01, -1.53580952e+02]]), adam_cache2=array([[1.62516485e-02, 2.37716373e+02, 8.52364691e+02]]))]\n",
      "Mean Squared Error\n",
      "439.0799041163032\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=5, weights=array([[ 0.95317448,  2.43279142,  1.42082812, -1.54861387],\n",
      "       [ 0.92462227,  3.30475917,  2.06965882, -2.00979026]]), adam_cache1=array([[  0.36221573, -51.99232702,   6.14772395,  41.76012249],\n",
      "       [  0.91111466, -84.38399582,  19.61612291,  69.17251655]]), adam_cache2=array([[2.80972005e-02, 1.35992084e+02, 4.22706459e+01, 7.68763488e+01],\n",
      "       [7.77582718e-02, 2.84553635e+02, 1.05041227e+02, 1.72371297e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=5, weights=array([[0.56145961, 1.56680407, 2.76466517]]), adam_cache1=array([[   0.18193873,  -46.08795016, -118.87051356]]), adam_cache2=array([[2.57943493e-02, 2.53684910e+02, 8.88963645e+02]]))]\n",
      "Mean Squared Error\n",
      "997.4188189100739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=6, weights=array([[ 0.66626954,  2.63156117,  1.18306754, -1.78302354],\n",
      "       [ 0.57123334,  3.50963383,  1.77852027, -2.25594051]]), adam_cache1=array([[  0.73675805, -30.640608  ,  27.19126693,  26.72570997],\n",
      "       [  1.54404913, -47.44511613,  55.87071503,  43.09606749]]), adam_cache2=array([[4.49418008e-02, 1.61946373e+02, 8.91366377e+01, 8.85899581e+01],\n",
      "       [1.30104765e-01, 3.65496818e+02, 2.50984014e+02, 2.08906410e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=6, weights=array([[0.36686089, 1.51736461, 2.82937523]]), adam_cache1=array([[  0.42586469,  14.21803441, -30.1406272 ]]), adam_cache2=array([[3.26392354e-02, 5.63648918e+02, 1.47855681e+03]]))]\n",
      "Mean Squared Error\n",
      "1155.9325999279995\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=7, weights=array([[ 0.36586329,  2.66932997,  0.88375856, -1.84016527],\n",
      "       [ 0.21069814,  3.52330874,  1.43301874, -2.29385288]]), adam_cache1=array([[ 0.81525488, -6.75626304, 39.49438805,  7.66534562],\n",
      "       [ 1.67272393, -3.87916668, 78.29466132,  8.22980057]]), adam_cache2=array([[4.72125102e-02, 2.05132850e+02, 1.11614294e+02, 1.15357345e+02],\n",
      "       [1.37988073e-01, 5.15841724e+02, 3.29194742e+02, 3.02068452e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=7, weights=array([[0.1557092 , 1.31636524, 2.72388816]]), adam_cache1=array([[ 0.48349344, 79.93483978, 63.05236026]]), adam_cache2=array([[3.36109052e-02, 1.01384455e+03, 2.29030210e+03]]))]\n",
      "Mean Squared Error\n",
      "631.5556466875327\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=8, weights=array([[ 0.14068356,  2.62631159,  0.58657973, -1.80313769],\n",
      "       [-0.05767575,  3.43987635,  1.0897943 , -2.2200373 ]]), adam_cache1=array([[  0.63084648,   8.24003585,  40.46593697,  -5.39320179],\n",
      "       [  1.29201476,  26.1397948 ,  80.64755102, -18.02690739]]), adam_cache2=array([[4.82237871e-02, 2.25435884e+02, 1.13924292e+02, 1.30351346e+02],\n",
      "       [1.42405611e-01, 6.03125764e+02, 3.39233584e+02, 3.66453835e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=8, weights=array([[0.00442612, 1.04797305, 2.54049834]]), adam_cache1=array([[  0.35686506, 121.43184983, 121.82862577]]), adam_cache2=array([[3.41900551e-02, 1.25776160e+03, 2.71157198e+03]]))]\n",
      "Mean Squared Error\n",
      "180.24910580091722\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=9, weights=array([[ 0.01951468,  2.56043697,  0.33966462, -1.73317139],\n",
      "       [-0.18163326,  3.32578908,  0.80930741, -2.10259243]]), adam_cache1=array([[  0.35932958,  12.87681047,  34.16235841, -10.45536047],\n",
      "       [  0.65713267,  36.76091787,  67.11107655, -29.8011817 ]]), adam_cache2=array([[5.25199639e-02, 2.28192458e+02, 1.14319765e+02, 1.33358651e+02],\n",
      "       [1.67834493e-01, 6.20039432e+02, 3.41888322e+02, 3.84520779e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=9, weights=array([[-0.0439269 ,  0.76706952,  2.33971908]]), adam_cache1=array([[1.22157878e-01, 1.31311468e+02, 1.37090420e+02]]), adam_cache2=array([[3.81167879e-02, 1.30500423e+03, 2.78418133e+03]]))]\n",
      "Mean Squared Error\n",
      "133.49996976714988\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=10, weights=array([[-0.02435077,  2.50514896,  0.1567467 , -1.66634546],\n",
      "       [-0.18473263,  3.2304892 ,  0.62755832, -1.98995535]]), adam_cache1=array([[ 1.35521447e-01,  1.08996314e+01,  2.57938215e+01,\n",
      "        -1.00719138e+01],\n",
      "       [ 1.81205015e-02,  3.09772320e+01,  4.52942098e+01,\n",
      "        -2.88373876e+01]]), adam_cache2=array([[5.59971520e-02, 2.28011806e+02, 1.16657974e+02, 1.33269129e+02],\n",
      "       [2.00533821e-01, 6.19863588e+02, 3.64364829e+02, 3.84542814e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=10, weights=array([[0.00534246, 0.52285069, 2.1699019 ]]), adam_cache1=array([[ -0.13505315, 115.16523991, 117.01237627]]), adam_cache2=array([[4.40809380e-02, 1.30460829e+03, 2.78545356e+03]]))]\n",
      "Mean Squared Error\n",
      "360.6113187237571\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=11, weights=array([[-0.02316226,  2.47121978,  0.02704612, -1.61632542],\n",
      "       [-0.10395416,  3.18484002,  0.54631042, -1.91341888]]), adam_cache1=array([[-3.73923737e-03,  6.73130336e+00,  1.85387502e+01,\n",
      "        -7.57730942e+00],\n",
      "       [-5.05482858e-01,  1.50967070e+01,  2.13600162e+01,\n",
      "        -1.97757560e+01]]), adam_cache2=array([[5.75214185e-02, 2.28731427e+02, 1.18727523e+02, 1.33357099e+02],\n",
      "       [2.27559905e-01, 6.35583727e+02, 4.01654984e+02, 3.87974908e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=11, weights=array([[0.12895872, 0.3509801 , 2.06319809]]), adam_cache1=array([[-0.36192505, 82.74609748, 75.04258864]]), adam_cache2=array([[4.98149778e-02, 1.34699563e+03, 2.87428662e+03]]))]\n",
      "Mean Squared Error\n",
      "602.7839060479323\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=12, weights=array([[ 1.08537161e-03,  2.45659856e+00, -6.45214785e-02,\n",
      "        -1.58343533e+00],\n",
      "       [ 3.20590592e-02,  3.19935085e+00,  5.51526717e-01,\n",
      "        -1.88653573e+00]]), adam_cache1=array([[-0.07671641,  2.90982477, 13.16889569, -4.99339841],\n",
      "       [-0.88608876, -4.93152619, -1.44389826, -7.05559281]]), adam_cache2=array([[5.80019354e-02, 2.29493906e+02, 1.19845007e+02, 1.33557236e+02],\n",
      "       [2.45921738e-01, 6.69241859e+02, 4.43969592e+02, 3.99127251e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=12, weights=array([[0.30385244, 0.26354113, 2.03084041]]), adam_cache1=array([[-0.53452693, 43.6037722 , 23.53389712]]), adam_cache2=array([[5.41246720e-02, 1.44093022e+03, 3.06505134e+03]]))]\n",
      "Mean Squared Error\n",
      "693.7900548194885\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=13, weights=array([[ 0.03645782,  2.45667547, -0.12827609, -1.56446981],\n",
      "       [ 0.2017277 ,  3.27015404,  0.62717985, -1.90944453]]), adam_cache1=array([[-1.11904411e-01, -1.53034285e-02,  9.18089627e+00,\n",
      "        -2.87737650e+00],\n",
      "       [-1.12773137e+00, -2.47527565e+01, -2.18819560e+01,\n",
      "         6.11838770e+00]]), adam_cache2=array([[5.81276284e-02, 2.29958284e+02, 1.20438645e+02, 1.33685044e+02],\n",
      "       [2.56582420e-01, 7.09840032e+02, 4.85889337e+02, 4.14274277e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=13, weights=array([[0.50976368, 0.2527157 , 2.06797054]]), adam_cache1=array([[ -0.64340431,   5.59730116, -28.00469346]]), adam_cache2=array([[5.67056526e-02, 1.55269525e+03, 3.30390469e+03]]))]\n",
      "Mean Squared Error\n",
      "603.2776070113399\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=14, weights=array([[ 0.07687269,  2.46840847, -0.16891533, -1.55703994],\n",
      "       [ 0.38634896,  3.3856631 ,  0.75899132, -1.97414531]]), adam_cache1=array([[ -0.12745124,  -2.32853998,   5.84421044,  -1.12387044],\n",
      "       [ -1.23384567, -41.23603251, -39.4834368 ,  17.51181262]]), adam_cache2=array([[5.81409889e-02, 2.30264140e+02, 1.20903167e+02, 1.33766207e+02],\n",
      "       [2.61117009e-01, 7.45072860e+02, 5.24566577e+02, 4.28272638e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=14, weights=array([[0.72753206, 0.30052402, 2.16111195]]), adam_cache1=array([[ -0.68449816, -25.34813471, -72.2973073 ]]), adam_cache2=array([[5.77605857e-02, 1.64347167e+03, 3.52237664e+03]]))]\n",
      "Mean Squared Error\n",
      "395.97681845075687\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=15, weights=array([[ 0.11734636,  2.48935842, -0.18792149, -1.55917515],\n",
      "       [ 0.56643225,  3.52954135,  0.93399082, -2.0673449 ]]), adam_cache1=array([[ -0.12695094,  -4.13862131,   2.72540347,   0.32141664],\n",
      "       [ -1.19868187, -51.81783961, -53.75825764,  25.35493237]]), adam_cache2=array([[5.80978415e-02, 2.30451235e+02, 1.21424575e+02, 1.33810103e+02],\n",
      "       [2.61634183e-01, 7.65952697e+02, 5.57250383e+02, 4.37049427e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=15, weights=array([[0.9371967 , 0.38480157, 2.29222453]]), adam_cache1=array([[  -0.65632152,  -45.10599438, -103.29087736]]), adam_cache2=array([[5.78650180e-02, 1.69152453e+03, 3.66495633e+03]]))]\n",
      "Mean Squared Error\n",
      "181.78173728401563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=16, weights=array([[ 0.15046192,  2.51487728, -0.18689232, -1.5670811 ],\n",
      "       [ 0.71953202,  3.6807606 ,  1.13897471, -2.17118561]]), adam_cache1=array([[ -0.10316561,  -5.00821259,  -0.1469685 ,   1.18223791],\n",
      "       [ -1.01288284, -54.29275874, -63.86731577,  28.14595063]]), adam_cache2=array([[5.80520430e-02, 2.30385509e+02, 1.21979063e+02, 1.33756031e+02],\n",
      "       [2.61807236e-01, 7.71049254e+02, 5.80671295e+02, 4.39449550e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=16, weights=array([[1.11759627, 0.48093015, 2.43925746]]), adam_cache1=array([[  -0.56123646,  -51.26563325, -115.85415914]]), adam_cache2=array([[5.78939004e-02, 1.70121840e+03, 3.71369743e+03]]))]\n",
      "Mean Squared Error\n",
      "66.22657680828276\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=17, weights=array([[ 0.16565461,  2.53642107, -0.17041809, -1.57371289],\n",
      "       [ 0.82198556,  3.81350451,  1.35879256, -2.26390508]]), adam_cache1=array([[-4.70343994e-02, -4.19417570e+00, -2.33830310e+00,\n",
      "         9.83729753e-01],\n",
      "       [-6.79268206e-01, -4.72839410e+01, -6.86673958e+01,\n",
      "         2.49299336e+01]]), adam_cache2=array([[5.82038891e-02, 2.30164934e+02, 1.22343741e+02, 1.33622920e+02],\n",
      "       [2.66942982e-01, 7.70527700e+02, 5.92605099e+02, 4.39026214e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=17, weights=array([[1.24909744, 0.56292451, 2.57574511]]), adam_cache1=array([[  -0.40904549,  -43.38588834, -106.68873347]]), adam_cache2=array([[5.87588995e-02, 1.70027518e+03, 3.71056937e+03]]))]\n",
      "Mean Squared Error\n",
      "90.77247704240892\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=18, weights=array([[ 0.15261314,  2.54322751, -0.14713685, -1.57015603],\n",
      "       [ 0.85659814,  3.90147373,  1.57521319, -2.32365613]]), adam_cache1=array([[ 4.02462366e-02, -1.31492521e+00, -3.27666338e+00,\n",
      "        -5.23264099e-01],\n",
      "       [-2.33433092e-01, -3.13080065e+01, -6.71620507e+01,\n",
      "         1.59965221e+01]]), adam_cache2=array([[5.88275846e-02, 2.30539846e+02, 1.22358800e+02, 1.33687718e+02],\n",
      "       [2.80957507e-01, 7.82407889e+02, 5.94886949e+02, 4.42735087e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=18, weights=array([[1.31933163, 0.60773678, 2.67388822]]), adam_cache1=array([[ -0.22049293, -23.66205885, -76.42044479]]), adam_cache2=array([[6.08801343e-02, 1.72224547e+03, 3.74527251e+03]]))]\n",
      "Mean Squared Error\n",
      "191.31091603352309\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=19, weights=array([[ 0.10718031,  2.52750676, -0.12786505, -1.54995689],\n",
      "       [ 0.82089254,  3.92839963,  1.76811206, -2.33667211]]), adam_cache1=array([[  0.14011145,   3.01902257,  -2.68604614,  -2.94941864],\n",
      "       [  0.24718439,  -9.694753  , -59.28670553,   3.49676478]]), adam_cache2=array([[5.98480668e-02, 2.32075370e+02, 1.22243356e+02, 1.34168317e+02],\n",
      "       [3.01586516e-01, 8.15785588e+02, 5.94426423e+02, 4.54173581e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=19, weights=array([[1.32763362, 0.60276603, 2.71402138]]), adam_cache1=array([[-2.64299650e-02,  2.64207267e+00, -3.15151347e+01]]), adam_cache2=array([[6.37781244e-02, 1.77782565e+03, 3.88038233e+03]]))]\n",
      "Mean Squared Error\n",
      "248.24333875509325\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=20, weights=array([[ 0.03432644,  2.4891504 , -0.12139918, -1.51309027],\n",
      "       [ 0.72694106,  3.8955608 ,  1.92035424, -2.30354588]]), adam_cache1=array([[  0.22413127,   7.32304782,  -0.89271816,  -5.3417923 ],\n",
      "       [  0.66419959,  12.00597324, -46.49045393,  -8.94820775]]), adam_cache2=array([[6.07492257e-02, 2.33964751e+02, 1.22353591e+02, 1.34756315e+02],\n",
      "       [3.20797791e-01, 8.57948279e+02, 5.98548363e+02, 4.68349026e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=20, weights=array([[1.28410563, 0.54998922, 2.69312494]]), adam_cache1=array([[ 0.13999802, 28.28149676, 16.65861879]]), adam_cache2=array([[6.63968986e-02, 1.84314764e+03, 4.07920216e+03]]))]\n",
      "Mean Squared Error\n",
      "206.41802616554605\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=21, weights=array([[-0.05448344,  2.43545633, -0.13108862, -1.46573236],\n",
      "       [ 0.59500918,  3.81863252,  2.02341666, -2.23762215]]), adam_cache1=array([[  0.27133398,  10.16792176,   1.32573306,  -6.79751181],\n",
      "       [  0.93922548,  28.31340657, -31.41296911, -17.79219219]]), adam_cache2=array([[6.11731130e-02, 2.35010407e+02, 1.22684578e+02, 1.35017529e+02],\n",
      "       [3.32135519e-01, 8.87743444e+02, 6.08825050e+02, 4.77365109e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=21, weights=array([[1.20529502, 0.46305793, 2.62333114]]), adam_cache1=array([[ 0.25379666, 46.63693049, 56.1696235 ]]), adam_cache2=array([[6.79637458e-02, 1.88617891e+03, 4.24467639e+03]]))]\n",
      "Mean Squared Error\n",
      "115.1439292448048\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=22, weights=array([[-0.14663877,  2.37672695, -0.15482523, -1.41639741],\n",
      "       [ 0.44790755,  3.72039249,  2.0785882 , -2.15903693]]), adam_cache1=array([[  0.27866187,  11.00452927,   3.21656782,  -7.00375809],\n",
      "       [  1.04143778,  35.97202853, -16.80582754, -21.02628915]]), adam_cache2=array([[6.12306979e-02, 2.35118906e+02, 1.22971311e+02, 1.34961010e+02],\n",
      "       [3.35650272e-01, 8.97859632e+02, 6.21362784e+02, 4.79401078e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=22, weights=array([[1.11017268, 0.36177026, 2.52501116]]), adam_cache1=array([[ 0.30416105, 53.93178505, 78.98023804]]), adam_cache2=array([[6.84694982e-02, 1.89859342e+03, 4.32124443e+03]]))]\n",
      "Mean Squared Error\n",
      "56.10393887983071\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=23, weights=array([[-0.23282509,  2.32152799, -0.18768925, -1.3718189 ],\n",
      "       [ 0.30701009,  3.62483967,  2.09364692, -2.08877901]]), adam_cache1=array([[  0.25761348,  10.2238147 ,   4.4061048 ,  -6.25545557],\n",
      "       [  0.98635093,  34.59360288,  -4.57455988, -18.58142416]]), adam_cache2=array([[6.11741154e-02, 2.34894010e+02, 1.23076710e+02, 1.34826279e+02],\n",
      "       [3.35555280e-01, 8.97454070e+02, 6.31873117e+02, 4.78933390e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=23, weights=array([[1.01714873, 0.26718963, 2.4209637 ]]), adam_cache1=array([[ 0.29410613, 49.78112779, 82.74619263]]), adam_cache2=array([[6.84424865e-02, 1.89684921e+03, 4.33052802e+03]]))]\n",
      "Mean Squared Error\n",
      "65.09472255853802\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=24, weights=array([[-0.30854468,  2.27435007, -0.2248015 , -1.33512249],\n",
      "       [ 0.1891433 ,  3.55229731,  2.07866226, -2.04368859]]), adam_cache1=array([[  0.22367352,   8.63580516,   4.9188565 ,  -5.08928113],\n",
      "       [  0.81602726,  25.99176435,   4.52495691, -11.81438643]]), adam_cache2=array([[6.11196303e-02, 2.34691110e+02, 1.23044524e+02, 1.34720681e+02],\n",
      "       [3.35733650e-01, 8.99201124e+02, 6.38709765e+02, 4.80864182e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=24, weights=array([[0.9416671 , 0.19729302, 2.33192186]]), adam_cache1=array([[ 0.23597426, 36.42287167, 69.9944469 ]]), adam_cache2=array([[6.84565351e-02, 1.90197504e+03, 4.32820196e+03]]))]\n",
      "Mean Squared Error\n",
      "116.75145235063766\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=25, weights=array([[-0.37284752,  2.23622172, -0.26284032, -1.30659848],\n",
      "       [ 0.10502693,  3.51572586,  2.04312955, -2.03337774]]), adam_cache1=array([[ 0.18770552,  6.89696064,  4.98199249, -3.90922677],\n",
      "       [ 0.57750656, 13.02400007, 10.63732303, -2.68677637]]), adam_cache2=array([[6.10770084e-02, 2.34533027e+02, 1.22952284e+02, 1.34631001e+02],\n",
      "       [3.37860241e-01, 9.09052684e+02, 6.42380796e+02, 4.86697482e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=25, weights=array([[0.89456962, 0.16350622, 2.27280293]]), adam_cache1=array([[ 0.14594613, 17.50215348, 46.06859013]]), adam_cache2=array([[6.88293825e-02, 1.92341611e+03, 4.35252410e+03]]))]\n",
      "Mean Squared Error\n",
      "160.16110600589423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=26, weights=array([[-0.42605345,  2.20675394, -0.30015195, -1.28554294],\n",
      "       [ 0.05961392,  3.5196025 ,  1.99465137, -2.05983121]]), adam_cache1=array([[ 0.15346605,  5.26695108,  4.82793563, -2.85127252],\n",
      "       [ 0.31001933, -1.37670865, 14.36300661,  6.87004021]]), adam_cache2=array([[6.10398602e-02, 2.34386913e+02, 1.22841175e+02, 1.34540863e+02],\n",
      "       [3.41921324e-01, 9.25300201e+02, 6.44032266e+02, 4.94837737e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=26, weights=array([[0.88160862, 0.1693132 , 2.25063681]]), adam_cache1=array([[ 0.03991885, -2.9988356 , 17.17959795]]), adam_cache2=array([[6.95965462e-02, 1.95665185e+03, 4.40713378e+03]]))]\n",
      "Mean Squared Error\n",
      "160.19040746281476\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=27, weights=array([[-0.46811645,  2.18563188, -0.33618052, -1.27144417],\n",
      "       [ 0.05338879,  3.5610779 ,  1.93869793, -2.11873028]]), adam_cache1=array([[  0.1198826 ,   3.73016598,   4.60533679,  -1.88631198],\n",
      "       [  0.04232282, -14.69171822,  16.39139198,  15.23491371]]), adam_cache2=array([[6.10120786e-02, 2.34254554e+02, 1.22725104e+02, 1.34452540e+02],\n",
      "       [3.47181835e-01, 9.42472361e+02, 6.44588639e+02, 5.02536547e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=27, weights=array([[0.90391191, 0.21104678, 2.26463788]]), adam_cache1=array([[ -0.06838574, -21.48111634, -10.80303692]]), adam_cache2=array([[7.06150637e-02, 1.98997217e+03, 4.47170996e+03]]))]\n",
      "Mean Squared Error\n",
      "115.78240084479641\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=28, weights=array([[-0.49824359,  2.17263374, -0.37087935, -1.26382438],\n",
      "       [ 0.08352241,  3.63181076,  1.87895752, -2.20098767]]), adam_cache1=array([[  0.08485349,   2.26803516,   4.38133075,  -1.00720745],\n",
      "       [ -0.20407288, -24.9291805 ,  17.29585907,  21.13207219]]), adam_cache2=array([[6.10041546e-02, 2.34138917e+02, 1.22607974e+02, 1.34365762e+02],\n",
      "       [3.52698965e-01, 9.55234417e+02, 6.44591043e+02, 5.07540615e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=28, weights=array([[0.95881213, 0.27970939, 2.30755821]]), adam_cache1=array([[ -0.16759934, -35.12908737, -32.90879741]]), adam_cache2=array([[7.16691550e-02, 2.01293382e+03, 4.52099761e+03]]))]\n",
      "Mean Squared Error\n",
      "54.26561539689194\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=29, weights=array([[-0.51561604,  2.166544  , -0.40432145, -1.26115038],\n",
      "       [ 0.14444476,  3.71948402,  1.81794981, -2.29389099]]), adam_cache1=array([[  0.04836319,   1.0498355 ,   4.17115667,  -0.34918046],\n",
      "       [ -0.41050804, -30.62941575,  17.45183748,  23.62477866]]), adam_cache2=array([[6.10215781e-02, 2.34003065e+02, 1.22490562e+02, 1.34262456e+02],\n",
      "       [3.57492016e-01, 9.60991959e+02, 6.44301988e+02, 5.09154519e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=29, weights=array([[1.04058706, 0.36280921, 2.3673193 ]]), adam_cache1=array([[ -0.24822089, -42.11098198, -45.38603114]]), adam_cache2=array([[7.25458013e-02, 2.02193497e+03, 4.54133995e+03]]))]\n",
      "Mean Squared Error\n",
      "11.868962307921247\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=30, weights=array([[-0.52069691,  2.16405497, -0.43641704, -1.25983683],\n",
      "       [ 0.22796981,  3.80919033,  1.75772946, -2.38231405]]), adam_cache1=array([[ 1.39821320e-02,  4.23883876e-01,  3.95445107e+00,\n",
      "        -1.69438281e-01],\n",
      "       [-5.58726949e-01, -3.09761925e+01,  1.70187796e+01,\n",
      "         2.22132090e+01]]), adam_cache2=array([[6.10478457e-02, 2.33796202e+02, 1.22372088e+02, 1.34130291e+02],\n",
      "       [3.60716826e-01, 9.61193585e+02, 6.43829853e+02, 5.08735787e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=30, weights=array([[1.14100002, 0.44611161, 2.42880052]]), adam_cache1=array([[ -0.3023703 , -41.71379942, -46.13714573]]), adam_cache2=array([[7.30969053e-02, 2.02136763e+03, 4.53959672e+03]]))]\n",
      "Mean Squared Error\n",
      "10.207980094565716\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=31, weights=array([[-0.5163868 ,  2.15978878, -0.46662523, -1.25440062],\n",
      "       [ 0.32340897,  3.8855631 ,  1.70055629, -2.45098057]]), adam_cache1=array([[-1.17225699e-02,  7.17724789e-01,  3.67668851e+00,\n",
      "        -6.92779275e-01],\n",
      "       [-6.32125141e-01, -2.60559864e+01,  1.59617437e+01,\n",
      "         1.70548497e+01]]), adam_cache2=array([[6.10458784e-02, 2.33573711e+02, 1.22251101e+02, 1.34025351e+02],\n",
      "       [3.62027206e-01, 9.60564574e+02, 6.43227605e+02, 5.09089671e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=31, weights=array([[1.24982146, 0.51581342, 2.47644877]]), adam_cache1=array([[ -0.3243103 , -34.48700884, -35.33680182]]), adam_cache2=array([[7.32960526e-02, 2.02027982e+03, 4.53888456e+03]]))]\n",
      "Mean Squared Error\n",
      "40.671623908270064\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=32, weights=array([[-0.50821409,  2.14792906, -0.49371833, -1.23921192],\n",
      "       [ 0.41826485,  3.93616593,  1.64935036, -2.48817266]]), adam_cache1=array([[ -0.02196154,   1.97183968,   3.25766989,  -1.91339823],\n",
      "       [ -0.62089593, -17.09121273,  14.12297711,   9.16006984]]), adam_cache2=array([[6.09978541e-02, 2.33515935e+02, 1.22129113e+02, 1.34057709e+02],\n",
      "       [3.61935405e-01, 9.63647920e+02, 6.42590263e+02, 5.12411318e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=32, weights=array([[1.35568069, 0.5613587 , 2.49806391]]), adam_cache1=array([[ -0.31175111, -22.30437211, -15.88052501]]), adam_cache2=array([[7.32622456e-02, 2.02588770e+03, 4.55969859e+03]]))]\n",
      "Mean Squared Error\n",
      "71.20526662886104\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=33, weights=array([[-0.50310802,  2.12474959, -0.51581679, -1.21106117],\n",
      "       [ 0.49997618,  3.9550432 ,  1.60770364, -2.489399  ]]), adam_cache1=array([[-0.01355649,  3.81097953,  2.6252921 , -3.50785427],\n",
      "       [-0.52849672, -6.32600858, 11.35009808,  0.30024141]]), adam_cache2=array([[6.09407113e-02, 2.33697081e+02, 1.22016385e+02, 1.34242558e+02],\n",
      "       [3.61665337e-01, 9.70885536e+02, 6.42132791e+02, 5.18209337e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=33, weights=array([[1.44752301, 0.5780149 , 2.48848314]]), adam_cache1=array([[-0.26725451, -8.08739878,  6.98885133]]), adam_cache2=array([[7.32067295e-02, 2.03822952e+03, 4.60042836e+03]]))]\n",
      "Mean Squared Error\n",
      "74.87101529655953\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=34, weights=array([[-0.50719419,  2.0903703 , -0.53083321, -1.17080457],\n",
      "       [ 0.55845983,  3.94420829,  1.57935961, -2.45857406]]), adam_cache1=array([[ 0.01072397,  5.59055321,  1.76293797, -4.96257233],\n",
      "       [-0.37428467,  3.60357338,  7.63653657, -7.50015369]]), adam_cache2=array([[6.09323253e-02, 2.33930234e+02, 1.21930348e+02, 1.34434300e+02],\n",
      "       [3.62331105e-01, 9.78558036e+02, 6.42155551e+02, 5.23728994e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=34, weights=array([[1.51649515, 0.567732  , 2.45094264]]), adam_cache1=array([[-0.19854986,  4.9514084 , 27.18629655]]), adam_cache2=array([[7.33097481e-02, 2.05114875e+03, 4.63949360e+03]]))]\n",
      "Mean Squared Error\n",
      "52.62961129542433\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=35, weights=array([[-0.52387681,  2.04855355, -0.53715041, -1.12294868],\n",
      "       [ 0.58822687,  3.91221443,  1.56738329, -2.40613345]]), adam_cache1=array([[  0.04330556,   6.72374057,   0.73309768,  -5.83370008],\n",
      "       [ -0.18882129,  10.54367749,   3.19192817, -12.65064837]]), adam_cache2=array([[6.09846521e-02, 2.33982672e+02, 1.21881272e+02, 1.34486840e+02],\n",
      "       [3.64160207e-01, 9.82909151e+02, 6.42868338e+02, 5.26686867e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=35, weights=array([[1.55752066, 0.53782397, 2.39548251]]), adam_cache1=array([[-0.11700706, 14.26444988, 39.78916026]]), adam_cache2=array([[7.36169771e-02, 2.05871764e+03, 4.65832892e+03]]))]\n",
      "Mean Squared Error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.76742462802575\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=36, weights=array([[-0.5532824 ,  2.0048976 , -0.53415031, -1.07366735],\n",
      "       [ 0.58890029,  3.87099601,  1.57343007, -2.3455921 ]]), adam_cache1=array([[ 7.55217107e-02,  6.93847920e+00, -3.44223233e-01,\n",
      "        -5.93819743e+00],\n",
      "       [-4.23756253e-03,  1.34353215e+01, -1.59519753e+00,\n",
      "        -1.44467678e+01]]), adam_cache2=array([[6.10572336e-02, 2.33827386e+02, 1.21860194e+02, 1.34399669e+02],\n",
      "       [3.66541749e-01, 9.83483343e+02, 6.44221712e+02, 5.27097265e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=36, weights=array([[1.56983312, 0.49837515, 2.33565629]]), adam_cache1=array([[-3.48218527e-02,  1.86097173e+01,  4.24392259e+01]]), adam_cache2=array([[7.40401665e-02, 2.05999019e+03, 4.65806493e+03]]))]\n",
      "Mean Squared Error\n",
      "16.64239023911288\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=37, weights=array([[-0.59303576,  1.96472012, -0.52234357, -1.02863001],\n",
      "       [ 0.56419375,  3.83250682,  1.59738503, -2.29008867]]), adam_cache1=array([[  0.1010055 ,   6.31167716,  -1.33956537,  -5.36396498],\n",
      "       [  0.15419249,  12.4004777 ,  -6.25763589, -13.09120149]]), adam_cache2=array([[6.11053138e-02, 2.33594008e+02, 1.21844376e+02, 1.34265308e+02],\n",
      "       [3.68671806e-01, 9.82509389e+02, 6.45902618e+02, 5.26570962e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=37, weights=array([[1.55641575, 0.45952342, 2.28491515]]), adam_cache1=array([[ 0.03762767, 18.11660213, 35.58024841]]), adam_cache2=array([[7.44417757e-02, 2.05811730e+03, 4.65409072e+03]]))]\n",
      "Mean Squared Error\n",
      "23.653226357806375\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=38, weights=array([[-0.63953119,  1.93168776, -0.50315105, -0.99168705],\n",
      "       [ 0.52032689,  3.80608679,  1.63735932, -2.24977727]]), adam_cache1=array([[  0.11684719,   5.13012137,  -2.15339615,  -4.34990206],\n",
      "       [  0.27128037,   8.41776632, -10.3404159 ,  -9.40412806]]), adam_cache2=array([[6.11115085e-02, 2.33390707e+02, 1.21812361e+02, 1.34153859e+02],\n",
      "       [3.70058948e-01, 9.82279100e+02, 6.47473754e+02, 5.26609857e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=38, weights=array([[1.5228885 , 0.42934119, 2.25354191]]), adam_cache1=array([[ 0.09316654, 13.91475644, 21.77187359]]), adam_cache2=array([[7.47190023e-02, 2.05663048e+03, 4.65994359e+03]]))]\n",
      "Mean Squared Error\n",
      "35.24412220274963\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=39, weights=array([[-0.68893534,  1.90744938, -0.47856998, -0.96460258],\n",
      "       [ 0.46470373,  3.79697516,  1.68986331, -2.23044707]]), adam_cache1=array([[  0.12278305,   3.72239736,  -2.72747308,  -3.15368765],\n",
      "       [  0.34051699,   2.87345989, -13.44561474,  -4.46521804]]), adam_cache2=array([[6.10814454e-02, 2.33237367e+02, 1.21752867e+02, 1.34077651e+02],\n",
      "       [3.70617504e-01, 9.83508199e+02, 6.48539611e+02, 5.27682045e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=39, weights=array([[1.47639424, 0.41261947, 2.24675504]]), adam_cache1=array([[0.1279033 , 7.62632226, 4.66766474]]), adam_cache2=array([[7.48383537e-02, 2.05697187e+03, 4.67756525e+03]]))]\n",
      "Mean Squared Error\n",
      "38.36995756587893\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=40, weights=array([[-0.73765803,  1.89189607, -0.45093667, -0.94739396],\n",
      "       [ 0.40509346,  3.80593785,  1.75000765, -2.23327661]]), adam_cache1=array([[  0.11974414,   2.36238235,  -3.03228246,  -1.98189477],\n",
      "       [  0.36099074,  -2.79901233, -15.24161258,   0.64764218]]), adam_cache2=array([[6.10289006e-02, 2.33101700e+02, 1.21664471e+02, 1.34016920e+02],\n",
      "       [3.70544189e-01, 9.85424649e+02, 6.48877383e+02, 5.29331834e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=40, weights=array([[1.42471054, 0.41054265, 2.2641919 ]]), adam_cache1=array([[  0.14065107,   0.93739067, -11.89093379]]), adam_cache2=array([[7.48287348e-02, 2.05842700e+03, 4.69878239e+03]]))]\n",
      "Mean Squared Error\n",
      "29.5343305185887\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=41, weights=array([[-0.782429  ,  1.88355754, -0.42285188, -0.93875985],\n",
      "       [ 0.34913424,  3.82968373,  1.8117274 , -2.25531741]]), adam_cache1=array([[  0.10881773,   1.25276007,  -3.04793534,  -0.98363896],\n",
      "       [  0.33514491,  -7.34255682, -15.47188194,   4.99833137]]), adam_cache2=array([[6.09679816e-02, 2.32944878e+02, 1.21552975e+02, 1.33946914e+02],\n",
      "       [3.70184158e-01, 9.86765788e+02, 6.48536308e+02, 5.30752125e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=41, weights=array([[1.37553166, 0.42104471, 2.3004856 ]]), adam_cache1=array([[  0.13236019,  -4.69137258, -24.52679475]]), adam_cache2=array([[7.47572402e-02, 2.05943223e+03, 4.71319654e+03]]))]\n",
      "Mean Squared Error\n",
      "15.730987658434874\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=42, weights=array([[-0.8202772 ,  1.87992481, -0.39716942, -0.93638025],\n",
      "       [ 0.30384604,  3.86179213,  1.86817254, -2.29037926]]), adam_cache1=array([[  0.09099313,   0.53986723,  -2.7568577 ,  -0.26818105],\n",
      "       [  0.26832064,  -9.82535785, -13.99551196,   7.87302323]]), adam_cache2=array([[6.09118339e-02, 2.32746463e+02, 1.21431441e+02, 1.33851047e+02],\n",
      "       [3.69924928e-01, 9.86813967e+02, 6.47888274e+02, 5.31360115e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=42, weights=array([[1.33576733, 0.43962357, 2.34653652]]), adam_cache1=array([[  0.10586848,  -8.21212974, -30.80650317]]), adam_cache2=array([[7.47000543e-02, 2.05896472e+03, 4.71610880e+03]]))]\n",
      "Mean Squared Error\n",
      "7.700636095404922\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=43, weights=array([[-0.84867798,  1.87780988, -0.37684838, -0.93719983],\n",
      "       [ 0.27492105,  3.8941074 ,  1.91246587, -2.33027345]]), adam_cache1=array([[  0.06755755,   0.31092836,  -2.15798976,   0.09137772],\n",
      "       [  0.16964942,  -9.78281981, -10.86695425,   8.86446648]]), adam_cache2=array([[6.08714749e-02, 2.32516777e+02, 1.21320454e+02, 1.33728268e+02],\n",
      "       [3.70071089e-01, 9.85915513e+02, 6.47539332e+02, 5.31145149e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=43, weights=array([[1.31077103, 0.46048927, 2.39133778]]), adam_cache1=array([[  0.06587294,  -9.12448004, -29.64956416]]), adam_cache2=array([[7.47118414e-02, 2.05720628e+03, 4.71176276e+03]]))]\n",
      "Mean Squared Error\n",
      "10.807520828059111\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=44, weights=array([[-0.86596383,  1.87394946, -0.36451292, -0.93792124],\n",
      "       [ 0.26580768,  3.91863564,  1.93890017, -2.36651164]]), adam_cache1=array([[ 0.04069584,  0.56153495, -1.29629741,  0.07957975],\n",
      "       [ 0.05295568, -7.34751973, -6.4222412 ,  7.96674195]]), adam_cache2=array([[6.08510283e-02, 2.32292196e+02, 1.21240852e+02, 1.33594540e+02],\n",
      "       [3.70695601e-01, 9.85141888e+02, 6.48019421e+02, 5.30614016e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=44, weights=array([[1.30357732, 0.47796329, 2.42436964]]), adam_cache1=array([[ 1.87772643e-02, -7.56038606e+00, -2.16346590e+01]]), adam_cache2=array([[7.48012224e-02, 2.05519154e+03, 4.70960120e+03]]))]\n",
      "Mean Squared Error\n",
      "20.468085328863086\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=45, weights=array([[-0.87184313,  1.86586234, -0.36177789, -0.93577018],\n",
      "       [ 0.27691182,  3.92968405,  1.944306  , -2.39233086]]), adam_cache1=array([[ 0.01370276,  1.16415942, -0.28450341, -0.23481352],\n",
      "       [-0.06395634, -3.27683979, -1.30163406,  5.6181292 ]]), adam_cache2=array([[6.08427260e-02, 2.32103302e+02, 1.21197432e+02, 1.33470336e+02],\n",
      "       [3.71570729e-01, 9.85269587e+02, 6.49376993e+02, 5.30324253e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=45, weights=array([[1.31438218, 0.48791627, 2.43824405]]), adam_cache1=array([[-0.02794589, -4.26194776, -9.00268738]]), adam_cache2=array([[7.49275325e-02, 2.05378273e+03, 4.71585056e+03]]))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error\n",
      "26.55798509491582\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=46, weights=array([[-0.86768786,  1.85263927, -0.36866233, -0.92927432],\n",
      "       [ 0.30537196,  3.92551371,  1.92899567, -2.40443135]]), adam_cache1=array([[-0.00958856,  1.88415457,  0.7090228 , -0.70185318],\n",
      "       [-0.16247214,  1.22521272,  3.65501579,  2.60733958]]), adam_cache2=array([[6.08299365e-02, 2.31941157e+02, 1.21169372e+02, 1.33360927e+02],\n",
      "       [3.72299799e-01, 9.86026853e+02, 6.51057113e+02, 5.30393678e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=46, weights=array([[1.34051738, 0.48882   , 2.43069388]]), adam_cache1=array([[-0.06697795, -0.38310837,  4.85721502]]), adam_cache2=array([[7.50275517e-02, 2.05292102e+03, 4.72792992e+03]]))]\n",
      "Mean Squared Error\n",
      "23.12205281940091\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=47, weights=array([[-0.85632177,  1.83521823, -0.38346871, -0.91862981],\n",
      "       [ 0.34565279,  3.9087283 ,  1.89676106, -2.40362752]]), adam_cache1=array([[-0.02596792,  2.45740523,  1.509876  , -1.13852208],\n",
      "       [-0.22782047,  4.88484608,  7.62814552, -0.1715483 ]]), adam_cache2=array([[6.07991679e-02, 2.31767230e+02, 1.21124198e+02, 1.33253256e+02],\n",
      "       [3.72593282e-01, 9.86471295e+02, 6.52288428e+02, 5.30497394e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=47, weights=array([[1.37700938, 0.48198517, 2.40498593]]), adam_cache1=array([[-0.09263409,  2.8686844 , 16.39555606]]), adam_cache2=array([[7.50572019e-02, 2.05190074e+03, 4.73765979e+03]]))]\n",
      "Mean Squared Error\n",
      "13.471748877669732\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=48, weights=array([[-0.84134916,  1.81596471, -0.40323903, -0.90543218],\n",
      "       [ 0.39078814,  3.88522946,  1.85389926, -2.39410409]]), adam_cache1=array([[-0.03386809,  2.68882229,  1.99621138, -1.39753346],\n",
      "       [-0.25279718,  6.7720104 , 10.04922406, -2.01270979]]), adam_cache2=array([[6.07493874e-02, 2.31558230e+02, 1.21043692e+02, 1.33133905e+02],\n",
      "       [3.72448779e-01, 9.86049195e+02, 6.52649857e+02, 5.30312231e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=48, weights=array([[1.41758637, 0.47088956, 2.36862987]]), adam_cache1=array([[-0.10199562,  4.61081475, 22.97062404]]), adam_cache2=array([[7.50168336e-02, 2.05026053e+03, 4.73967014e+03]]))]\n",
      "Mean Squared Error\n",
      "5.801756028311202\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=49, weights=array([[-0.82634319,  1.79780025, -0.42454281, -0.89194767],\n",
      "       [ 0.43381742,  3.86227081,  1.80770985, -2.38170752]]), adam_cache1=array([[-0.03360872,  2.51169277,  2.12990761, -1.41382683],\n",
      "       [-0.23862596,  6.55108698, 10.72480943, -2.5942118 ]]), adam_cache2=array([[6.06896161e-02, 2.31327514e+02, 1.20933758e+02, 1.33003207e+02],\n",
      "       [3.72088670e-01, 9.85083965e+02, 6.52279618e+02, 5.29843192e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=49, weights=array([[1.45585058, 0.45991948, 2.33103838]]), adam_cache1=array([[-0.0952333 ,  4.51365687, 23.51872053]]), adam_cache2=array([[7.49429982e-02, 2.04822351e+03, 4.73573996e+03]]))]\n",
      "Mean Squared Error\n",
      "5.081421574094163\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=50, weights=array([[-0.81422923,  1.7833157 , -0.44422145, -0.88031785],\n",
      "       [ 0.4689609 ,  3.84639595,  1.76502229, -2.37203967]]), adam_cache1=array([[-0.02686778,  1.98339735,  1.94828721, -1.20750408],\n",
      "       [-0.19300993,  4.48615307,  9.81528429, -2.00351186]]), adam_cache2=array([[6.06300689e-02, 2.31103866e+02, 1.20812923e+02, 1.32870625e+02],\n",
      "       [3.71763903e-01, 9.84297641e+02, 6.51629993e+02, 5.29324323e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=50, weights=array([[1.48632284, 0.45302838, 2.30099474]]), adam_cache1=array([[-0.07510815,  2.80787388, 18.61488106]]), adam_cache2=array([[7.48792951e-02, 2.04633264e+03, 4.73165548e+03]]))]\n",
      "Mean Squared Error\n",
      "9.57243469968491\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=51, weights=array([[-0.80699507,  1.77417197, -0.45985488, -0.87199422],\n",
      "       [ 0.4923027 ,  3.84187936,  1.73113459, -2.36897636]]), adam_cache1=array([[-0.01589181,  1.2401415 ,  1.53297811, -0.85595796],\n",
      "       [-0.12700243,  1.26462106,  7.71792636, -0.62881765]]), adam_cache2=array([[6.05763099e-02, 2.30902456e+02, 1.20696971e+02, 1.32743081e+02],\n",
      "       [3.71610289e-01, 9.84082250e+02, 6.51102871e+02, 5.28932907e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=51, weights=array([[1.50516715, 0.45271942, 2.28462013]]), adam_cache1=array([[-0.04601642,  0.12470178, 10.05309678]]), adam_cache2=array([[7.48509894e-02, 2.04486346e+03, 4.73141322e+03]]))]\n",
      "Mean Squared Error\n",
      "13.846140842375796\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=52, weights=array([[-0.80564731,  1.77083765, -0.46994231, -0.86747183],\n",
      "       [ 0.50201305,  3.84994627,  1.709277  , -2.37388775]]), adam_cache1=array([[-2.93306210e-03,  4.47994538e-01,  9.79868065e-01,\n",
      "        -4.60680436e-01],\n",
      "       [-5.23616217e-02, -2.23864272e+00,  4.93255816e+00,\n",
      "         9.98884998e-01]]), adam_cache2=array([[6.05286603e-02, 2.30716194e+02, 1.20592259e+02, 1.32619928e+02],\n",
      "       [3.71622342e-01, 9.84238447e+02, 6.50857217e+02, 5.28648841e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=52, weights=array([[1.51054413, 0.45953863, 2.28424847]]), adam_cache1=array([[-0.01301312, -2.72685266,  0.22620815]]), adam_cache2=array([[7.48568038e-02, 2.04362463e+03, 4.73446383e+03]]))]\n",
      "Mean Squared Error\n",
      "14.030157671227167\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=53, weights=array([[-0.81026091,  1.77257066, -0.47391358, -0.86625209],\n",
      "       [ 0.49828041,  3.86874425,  1.700516  , -2.38554861]]), adam_cache1=array([[ 0.00994813, -0.23069631,  0.38220561, -0.12309785],\n",
      "       [ 0.01995236, -5.17064214,  1.95958832,  2.34997851]]), adam_cache2=array([[6.04839772e-02, 2.30525659e+02, 1.20496635e+02, 1.32495806e+02],\n",
      "       [3.71700663e-01, 9.84250156e+02, 6.50821258e+02, 5.28330727e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=53, weights=array([[1.50264776, 0.47207764, 2.29828798]]), adam_cache1=array([[ 0.01894434, -4.9681436 , -8.4721902 ]]), adam_cache2=array([[7.48759269e-02, 2.04221302e+03, 4.73725628e+03]]))]\n",
      "Mean Squared Error\n",
      "10.55958388055466\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=54, weights=array([[-0.82003164,  1.77757533, -0.47207056, -0.86697548],\n",
      "       [ 0.48309826,  3.89391363,  1.70395607, -2.40061352]]), adam_cache1=array([[ 0.0208773 , -0.66013051, -0.17576846,  0.07233678],\n",
      "       [ 0.08045149, -6.86133267, -0.76275851,  3.00834716]]), adam_cache2=array([[6.04377113e-02, 2.30315610e+02, 1.20403152e+02, 1.32366664e+02],\n",
      "       [3.71719517e-01, 9.83753324e+02, 6.50808700e+02, 5.27882206e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=54, weights=array([[1.4835056 , 0.48739793, 2.32191356]]), adam_cache1=array([[  0.04552745,  -6.01476443, -14.13240075]]), adam_cache2=array([[7.48821481e-02, 2.04040902e+03, 4.73675368e+03]]))]\n",
      "Mean Squared Error\n",
      "6.913360677942835\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=55, weights=array([[-0.83335015,  1.78335342, -0.46547391, -0.8677338 ],\n",
      "       [ 0.45994165,  3.91963069,  1.71714576, -2.41454613]]), adam_cache1=array([[ 0.02820254, -0.75525687, -0.62348736,  0.07514229],\n",
      "       [ 0.12163841, -6.94742525, -2.89915084,  2.75706767]]), adam_cache2=array([[6.03861340e-02, 2.30087891e+02, 1.20304399e+02, 1.32234308e+02],\n",
      "       [3.71590177e-01, 9.82829204e+02, 6.50647481e+02, 5.27354570e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=55, weights=array([[1.45659196, 0.50177496, 2.348375  ]]), adam_cache1=array([[  0.06345339,  -5.59338078, -15.6870232 ]]), adam_cache2=array([[7.48577950e-02, 2.03837186e+03, 4.73289775e+03]]))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error\n",
      "6.0579297989330945\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=56, weights=array([[-0.84798731,  1.78728002, -0.45573136, -0.86657715],\n",
      "       [ 0.43328097,  3.94001007,  1.73665341, -2.42288156]]), adam_cache1=array([[ 0.0307192 , -0.5086754 , -0.91266157, -0.11359302],\n",
      "       [ 0.13881216, -5.4565451 , -4.2505153 ,  1.63486818]]), adam_cache2=array([[6.03285962e-02, 2.29860729e+02, 1.20196452e+02, 1.32105357e+02],\n",
      "       [3.71304656e-01, 9.81909758e+02, 6.50266214e+02, 5.26898870e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=56, weights=array([[1.42626962, 0.51164186, 2.3707078 ]]), adam_cache1=array([[  0.0708615 ,  -3.8046287 , -13.12154662]]), adam_cache2=array([[7.48018530e-02, 2.03648463e+03, 4.72826421e+03]]))]\n",
      "Mean Squared Error\n",
      "7.821997152085809\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=57, weights=array([[-0.86146025,  1.78731926, -0.44466456, -0.86212699],\n",
      "       [ 0.40788723,  3.95062324,  1.75877098, -2.42256954]]), adam_cache1=array([[ 0.02802732, -0.00503896, -1.02762069, -0.43322262],\n",
      "       [ 0.13105435, -2.81731379, -4.77716644, -0.06067317]]), adam_cache2=array([[6.02682820e-02, 2.29651368e+02, 1.20080508e+02, 1.31984207e+02],\n",
      "       [3.70937101e-01, 9.81366155e+02, 6.49706521e+02, 5.26606690e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=57, weights=array([[1.39708592, 0.51454015, 2.38352885]]), adam_cache1=array([[ 0.0676018 , -1.10789138, -7.46824189]]), adam_cache2=array([[7.47285153e-02, 2.03498466e+03, 4.72542050e+03]]))]\n",
      "Mean Squared Error\n",
      "9.492989040688148\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=58, weights=array([[-0.87153883,  1.78263244, -0.43391708, -0.85406833],\n",
      "       [ 0.38799401,  3.94969207,  1.78022747, -2.41297696]]), adam_cache1=array([[ 0.02078484,  0.59668253, -0.98932509, -0.77775765],\n",
      "       [ 0.1017809 ,  0.24513763, -4.59426228, -1.84972455]]), adam_cache2=array([[6.02099849e-02, 2.29457863e+02, 1.19960843e+02, 1.31867266e+02],\n",
      "       [3.70592304e-01, 9.81158029e+02, 6.49065506e+02, 5.26402329e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=58, weights=array([[1.37301159, 0.5098033 , 2.38442249]]), adam_cache1=array([[ 0.05528435,  1.79534804, -0.51624002]]), adam_cache2=array([[7.46568751e-02, 2.03372945e+03, 4.72454551e+03]]))]\n",
      "Mean Squared Error\n",
      "8.860772830008923\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=59, weights=array([[-0.87672698,  1.77379416, -0.42464052, -0.84327636],\n",
      "       [ 0.37652388,  3.9384985 ,  1.79872253, -2.39614163]]), adam_cache1=array([[ 0.01060862,  1.11567668, -0.84663202, -1.03270189],\n",
      "       [ 0.05819288,  2.9227222 , -3.92635196, -3.21934923]]), adam_cache2=array([[6.01563322e-02, 2.29261890e+02, 1.19841074e+02, 1.31746469e+02],\n",
      "       [3.70333334e-01, 9.80907005e+02, 6.48420787e+02, 5.26117604e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=59, weights=array([[1.35678242, 0.49872357, 2.37442794]]), adam_cache1=array([[0.03695446, 4.16421148, 5.72673346]]), adam_cache2=array([[7.45986060e-02, 2.03234515e+03, 4.72365424e+03]]))]\n",
      "Mean Squared Error\n",
      "6.413290745053772\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=60, weights=array([[-0.87652645,  1.76250783, -0.41737962, -0.83150298],\n",
      "       [ 0.37462972,  3.92084502,  1.81311905, -2.37615801]]), adam_cache1=array([[-4.06622240e-04,  1.41275594e+00, -6.57093989e-01,\n",
      "        -1.11714309e+00],\n",
      "       [ 9.53131045e-03,  4.57151553e+00, -3.03058623e+00,\n",
      "        -3.78949140e+00]]), adam_cache2=array([[6.01060848e-02, 2.29049327e+02, 1.19722333e+02, 1.31618247e+02],\n",
      "       [3.70146547e-01, 9.80302871e+02, 6.47797681e+02, 5.25671066e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=60, weights=array([[1.3495141 , 0.48413371, 2.35746398]]), adam_cache1=array([[0.01641402, 5.43767149, 9.64031771]]), adam_cache2=array([[7.45523827e-02, 2.03059838e+03, 4.72094324e+03]]))]\n",
      "Mean Squared Error\n",
      "4.478822788071578\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=61, weights=array([[-0.87140083,  1.7509771 , -0.41217249, -0.82078611],\n",
      "       [ 0.38167991,  3.90181084,  1.82327189, -2.3579528 ]]), adam_cache1=array([[-0.01030791,  1.43137144, -0.46732362, -1.00845153],\n",
      "       [-0.0351908 ,  4.88828377, -2.1195736 , -3.42358857]]), adam_cache2=array([[6.00558630e-02, 2.28822834e+02, 1.19604149e+02, 1.31486629e+02],\n",
      "       [3.69967973e-01, 9.79382464e+02, 6.47186844e+02, 5.25145412e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=61, weights=array([[1.35067611, 0.46956991, 2.33895879]]), adam_cache1=array([[-2.60290672e-03,  5.38292346e+00,  1.04291977e+01]]), adam_cache2=array([[7.45080212e-02, 2.02859170e+03, 4.71652957e+03]]))]\n",
      "Mean Squared Error\n",
      "4.630669774460186\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=62, weights=array([[-0.86251461,  1.74123162, -0.4087755 , -0.81285585],\n",
      "       [ 0.39562126,  3.886286  ,  1.82962212, -2.34591551]]), adam_cache1=array([[-0.01772543,  1.19985777, -0.30237545, -0.74013089],\n",
      "       [-0.06903112,  3.95442615, -1.31489914, -2.24529276]]), adam_cache2=array([[6.00029445e-02, 2.28594793e+02, 1.19485943e+02, 1.31357947e+02],\n",
      "       [3.69737577e-01, 9.78422886e+02, 6.46574788e+02, 5.24690146e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=62, weights=array([[1.35838444, 0.45831809, 2.32420132]]), adam_cache1=array([[-0.01712779,  4.12478385,  8.24907185]]), adam_cache2=array([[7.44553734e-02, 2.02661492e+03, 4.71194236e+03]]))]\n",
      "Mean Squared Error\n",
      "6.26934145068686\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=63, weights=array([[-0.85139877,  1.73464316, -0.4068919 , -0.80873695],\n",
      "       [ 0.41353585,  3.87774475,  1.83275189, -2.3428097 ]]), adam_cache1=array([[-0.02199452,  0.80462958, -0.16631064, -0.38132487],\n",
      "       [-0.0879965 ,  2.15824331, -0.6428404 , -0.5747587 ]]), adam_cache2=array([[5.99465917e-02, 2.28373774e+02, 1.19367577e+02, 1.31234700e+02],\n",
      "       [3.69434758e-01, 9.77640671e+02, 6.45957435e+02, 5.24374549e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=63, weights=array([[1.36988838, 0.45262885, 2.31692387]]), adam_cache1=array([[-0.0253569 ,  2.06890849,  4.03556479]]), adam_cache2=array([[7.43908021e-02, 2.02485838e+03, 4.70837868e+03]]))]\n",
      "Mean Squared Error\n",
      "7.529617665951585\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=64, weights=array([[-0.83965998,  1.73170665, -0.40632404, -0.80859306],\n",
      "       [ 0.43222001,  3.87753843,  1.83306141, -2.34920066]]), adam_cache1=array([[-0.02304235,  0.35578288, -0.04974017, -0.01321525],\n",
      "       [-0.09104743,  0.05172873, -0.06306935,  1.17360641]]), adam_cache2=array([[5.98876996e-02, 2.28158971e+02, 1.19249208e+02, 1.31114354e+02],\n",
      "       [3.69079367e-01, 9.77020501e+02, 6.45338050e+02, 5.24136085e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=64, weights=array([[1.38211917, 0.4532738 , 2.31845879]]), adam_cache1=array([[-0.02674459, -0.23269519, -0.84456274]]), adam_cache2=array([[7.43179506e-02, 2.02327231e+03, 4.70567427e+03]]))]\n",
      "Mean Squared Error\n",
      "7.194531238444449\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=65, weights=array([[-0.828767  ,  1.73204241, -0.40702493, -0.81176027],\n",
      "       [ 0.44869474,  3.88478422,  1.83064918, -2.36344133]]), adam_cache1=array([[-0.02121412, -0.04036247,  0.06091021,  0.28861906],\n",
      "       [-0.07965036, -1.80269548,  0.48767502,  2.59513105]]), adam_cache2=array([[5.98278346e-02, 2.27943813e+02, 1.19131076e+02, 1.30992270e+02],\n",
      "       [3.68710813e-01, 9.76385454e+02, 6.44722353e+02, 5.23848765e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=65, weights=array([[1.39220713, 0.45949286, 2.32759668]]), adam_cache1=array([[-0.02188571, -2.22640836, -4.98939929]]), adam_cache2=array([[7.42441098e-02, 2.02165586e+03, 4.70275729e+03]]))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error\n",
      "5.686863805735459\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=66, weights=array([[-0.81988911,  1.73455909, -0.40905758, -0.81691543],\n",
      "       [ 0.46062353,  3.89678945,  1.82540017, -2.38214748]]), adam_cache1=array([[-0.01715576, -0.30018916,  0.17527708,  0.46614288],\n",
      "       [-0.05722672, -2.96393307,  1.05298874,  3.38282942]]), adam_cache2=array([[5.97683819e-02, 2.27722831e+02, 1.19013395e+02, 1.30865538e+02],\n",
      "       [3.68363007e-01, 9.75589032e+02, 6.44115341e+02, 5.23434582e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=66, weights=array([[1.39791418, 0.4692962 , 2.34109751]]), adam_cache1=array([[-0.01228589, -3.48255541, -7.31510943]]), adam_cache2=array([[7.41753584e-02, 2.01985288e+03, 4.69885239e+03]]))]\n",
      "Mean Squared Error\n",
      "4.416684889369899\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=67, weights=array([[-0.81375746,  1.73774876, -0.41248247, -0.82236439],\n",
      "       [ 0.46662788,  3.90988315,  1.81722945, -2.40105151]]), adam_cache1=array([[-0.01175843, -0.37755567,  0.29307454,  0.4889425 ],\n",
      "       [-0.02858683, -3.20799111,  1.62662933,  3.39251279]]), adam_cache2=array([[5.97099691e-02, 2.27496261e+02, 1.18896213e+02, 1.30735154e+02],\n",
      "       [3.68047164e-01, 9.74642652e+02, 6.43517321e+02, 5.22923255e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=67, weights=array([[1.39794556, 0.48003453, 2.35469548]]), adam_cache1=array([[-6.70513983e-05, -3.78557602e+00, -7.31144638e+00]]), adam_cache2=array([[7.41132616e-02, 2.01787544e+03, 4.69420652e+03]]))]\n",
      "Mean Squared Error\n",
      "4.320520530357894\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=68, weights=array([[-0.81055649,  1.74008955, -0.41720924, -0.8264374 ],\n",
      "       [ 0.46646771,  3.92046845,  1.80638306, -2.41607374]]), adam_cache1=array([[-6.09216055e-03, -2.74984332e-01,  4.01431086e-01,\n",
      "         3.62719742e-01],\n",
      "       [ 7.56896254e-04, -2.57386786e+00,  2.14308507e+00,\n",
      "         2.67557772e+00]]), adam_cache2=array([[5.96522755e-02, 2.27269185e+02, 1.18779212e+02, 1.30605017e+02],\n",
      "       [3.67749262e-01, 9.73677827e+02, 6.42919924e+02, 5.22414597e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=68, weights=array([[1.39210412, 0.48910867, 2.364353  ]]), adam_cache1=array([[ 0.01238719, -3.174765  , -5.15364406]]), adam_cache2=array([[7.40546424e-02, 2.01586296e+03, 4.68971585e+03]]))]\n",
      "Mean Squared Error\n",
      "5.056173055548108\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=69, weights=array([[-0.80989084,  1.7404758 , -0.42287191, -0.82791927],\n",
      "       [ 0.4610332 ,  3.92603009,  1.79366844, -2.42435861]]), adam_cache1=array([[-1.25745828e-03, -4.50375158e-02,  4.77334895e-01,\n",
      "         1.30986480e-01],\n",
      "       [ 2.54919223e-02, -1.34233602e+00,  2.49357094e+00,\n",
      "         1.46473833e+00]]), adam_cache2=array([[5.95944087e-02, 2.27046015e+02, 1.18661780e+02, 1.30478232e+02],\n",
      "       [3.67443070e-01, 9.72799045e+02, 6.42308903e+02, 5.21981160e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=69, weights=array([[1.38125506, 0.49464198, 2.36743153]]), adam_cache1=array([[ 0.02283694, -1.92156342, -1.63075355]]), adam_cache2=array([[7.39942498e-02, 2.01393466e+03, 4.68593065e+03]]))]\n",
      "Mean Squared Error\n",
      "5.5658867139071475\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=70, weights=array([[-0.81087412,  1.73853297, -0.42879514, -0.82637165],\n",
      "       [ 0.45211628,  3.92578893,  1.78049774, -2.42496044]]), adam_cache1=array([[ 1.84384271e-03,  2.24875568e-01,  4.95629600e-01,\n",
      "        -1.35796154e-01],\n",
      "       [ 4.15214819e-02,  5.77827395e-02,  2.56405239e+00,\n",
      "         1.05632663e-01]]), adam_cache2=array([[5.95356997e-02, 2.26826013e+02, 1.18543554e+02, 1.30354190e+02],\n",
      "       [3.67110144e-01, 9.71986492e+02, 6.41676824e+02, 5.21606226e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=70, weights=array([[1.36709579, 0.49592104, 2.36341694]]), adam_cache1=array([[ 0.0295874 , -0.44093526,  2.11125998]]), adam_cache2=array([[7.39284172e-02, 2.01208674e+03, 4.68252560e+03]]))]\n",
      "Mean Squared Error\n",
      "5.277879180673673\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=71, weights=array([[-0.81234015,  1.73468359, -0.43408831, -0.82220871],\n",
      "       [ 0.44199595,  3.92078175,  1.76869479, -2.41894002]]), adam_cache1=array([[ 0.00272916,  0.44232209,  0.43969525, -0.36263404],\n",
      "       [ 0.04678375,  1.19110166,  2.28110696, -1.04917792]]), adam_cache2=array([[5.94762784e-02, 2.26604944e+02, 1.18425014e+02, 1.30229615e+02],\n",
      "       [3.66751897e-01, 9.71144260e+02, 6.41035218e+02, 5.21215550e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=71, weights=array([[1.35177256, 0.49346669, 2.35395567]]), adam_cache1=array([[0.03178785, 0.83999085, 4.94002512]]), adam_cache2=array([[7.38571505e-02, 2.01022763e+03, 4.67876717e+03]]))]\n",
      "Mean Squared Error\n",
      "4.6005139222328335\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=72, weights=array([[-0.81311649,  1.72993553, -0.43784229, -0.81649174],\n",
      "       [ 0.43294289,  3.91336111,  1.76011661, -2.40885802]]), adam_cache1=array([[ 1.43487818e-03,  5.41682909e-01,  3.09603942e-01,\n",
      "        -4.94444703e-01],\n",
      "       [ 4.15502519e-02,  1.75261095e+00,  1.64601389e+00,\n",
      "        -1.74451095e+00]]), adam_cache2=array([[5.94169064e-02, 2.26380401e+02, 1.18307331e+02, 1.30102211e+02],\n",
      "       [3.66385176e-01, 9.70219440e+02, 6.40410746e+02, 5.20758375e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=72, weights=array([[1.33743149, 0.48872991, 2.3422119 ]]), adam_cache1=array([[0.02953725, 1.60955417, 6.08804153]]), adam_cache2=array([[7.37833795e-02, 2.00829026e+03, 4.67435802e+03]]))]\n",
      "Mean Squared Error\n",
      "4.290058968130589\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=73, weights=array([[-0.81227183,  1.72549315, -0.43935431, -0.81054084],\n",
      "       [ 0.42677527,  3.90633258,  1.75621552, -2.39788643]]), adam_cache1=array([[-1.55012321e-03,  5.03223721e-01,  1.23820893e-01,\n",
      "        -5.11035878e-01],\n",
      "       [ 2.81071852e-02,  1.64825763e+00,  7.43291669e-01,\n",
      "        -1.88502802e+00]]), adam_cache2=array([[5.93582969e-02, 2.26154045e+02, 1.18191421e+02, 1.29972545e+02],\n",
      "       [3.66027418e-01, 9.69249723e+02, 6.39824817e+02, 5.20247537e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=73, weights=array([[1.32580811, 0.48354546, 2.33180435]]), adam_cache1=array([[0.02377061, 1.7492113 , 5.35716682]]), adam_cache2=array([[7.37103873e-02, 2.00629100e+03, 4.66968516e+03]]))]\n",
      "Mean Squared Error\n",
      "4.566334330572054\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=74, weights=array([[-0.80927249,  1.72235521, -0.43830241, -0.80552757],\n",
      "       [ 0.42456357,  3.90205065,  1.75767976, -2.38886786]]), adam_cache1=array([[-0.00546601,  0.35297708, -0.08554081, -0.42750899],\n",
      "       [ 0.01000913,  0.99715191, -0.27705862, -1.53865934]]), adam_cache2=array([[5.93005959e-02, 2.25928889e+02, 1.18077109e+02, 1.29842677e+02],\n",
      "       [3.65684761e-01, 9.68304120e+02, 6.39274488e+02, 5.19729782e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=74, weights=array([[1.31793922, 0.47954477, 2.32569843]]), adam_cache1=array([[0.01598032, 1.34038904, 3.12109509]]), adam_cache2=array([[7.36396072e-02, 2.00429018e+03, 4.66530459e+03]]))]\n",
      "Mean Squared Error\n",
      "4.966114188709506\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=75, weights=array([[-0.80403204,  1.72103275, -0.43482516, -0.80218175],\n",
      "       [ 0.42651758,  3.90175325,  1.76424643, -2.38360653]]), adam_cache1=array([[-0.00948452,  0.1477337 , -0.28082484, -0.28334933],\n",
      "       [-0.00878231,  0.0687824 , -1.23404439, -0.89146295]]), adam_cache2=array([[5.92433793e-02, 2.25705849e+02, 1.17963187e+02, 1.29713863e+02],\n",
      "       [3.65350726e-01, 9.67404483e+02, 6.38732176e+02, 5.19234389e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=75, weights=array([[1.31403806, 0.47770903, 2.32539032]]), adam_cache1=array([[0.00786814, 0.61080687, 0.15641825]]), adam_cache2=array([[7.35702111e-02, 2.00232136e+03, 4.66134290e+03]]))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error\n",
      "4.949193833981243\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=76, weights=array([[-0.79687425,  1.72144589, -0.42950109, -0.80066991],\n",
      "       [ 0.43203805,  3.9052922 ,  1.77472165, -2.38254623]]), adam_cache1=array([[-0.01286655, -0.0458387 , -0.42704991, -0.12716204],\n",
      "       [-0.02464359, -0.81292903, -1.95525275, -0.17843618]]), adam_cache2=array([[5.91860112e-02, 2.25483340e+02, 1.17848262e+02, 1.29585784e+02],\n",
      "       [3.65013397e-01, 9.66513612e+02, 6.38164780e+02, 5.18754078e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=76, weights=array([[1.31353218, 0.47816897, 2.33058326]]), adam_cache1=array([[ 1.01335835e-03, -1.51992696e-01, -2.61854784e+00]]), adam_cache2=array([[7.35003229e-02, 2.00036828e+03, 4.65744294e+03]]))]\n",
      "Mean Squared Error\n",
      "4.489832121476954\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=77, weights=array([[-0.78843744,  1.72299628, -0.42324167, -0.80064052],\n",
      "       [ 0.43989612,  3.91128698,  1.78719834, -2.38485997]]), adam_cache1=array([[-0.01506371, -0.17085879, -0.49869779, -0.00245567],\n",
      "       [-0.03484327, -1.36780747, -2.31321233,  0.38676456]]), adam_cache2=array([[5.91280389e-02, 2.25259536e+02, 1.17731722e+02, 1.29457452e+02],\n",
      "       [3.64664421e-01, 9.65587570e+02, 6.37557250e+02, 5.18265284e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=77, weights=array([[1.31523779, 0.48027457, 2.33939611]]), adam_cache1=array([[-3.39367239e-03, -6.91153526e-01, -4.41417116e+00]]), adam_cache2=array([[7.34286764e-02, 1.99839864e+03, 4.65320882e+03]]))]\n",
      "Mean Squared Error\n",
      "4.043689827896936\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=78, weights=array([[-0.77953869,  1.72477923, -0.41711608, -0.80140053],\n",
      "       [ 0.44850506,  3.91762809,  1.79943274, -2.38888041]]), adam_cache1=array([[-0.01578285, -0.19518172, -0.48478851,  0.06307278],\n",
      "       [-0.03791898, -1.43720856, -2.25320432,  0.66759315]]), adam_cache2=array([[5.90694062e-02, 2.25034448e+02, 1.17614119e+02, 1.29328421e+02],\n",
      "       [3.64304060e-01, 9.64626233e+02, 6.36922628e+02, 5.17757227e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=78, weights=array([[1.31763158, 0.48288998, 2.34900997]]), adam_cache1=array([[-4.73126397e-03, -8.52782294e-01, -4.78337696e+00]]), adam_cache2=array([[7.33555290e-02, 1.99640557e+03, 4.64862132e+03]]))]\n",
      "Mean Squared Error\n",
      "4.011843373396169\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=79, weights=array([[-0.77101133,  1.72587812, -0.41213374, -0.80217119],\n",
      "       [ 0.45624705,  3.92217945,  1.80931   , -2.39273156]]), adam_cache1=array([[-0.01502474, -0.11950662, -0.39171803,  0.06353611],\n",
      "       [-0.03387626, -1.02478583, -1.8071402 ,  0.63527885]]), adam_cache2=array([[5.90104040e-02, 2.24809729e+02, 1.17496704e+02, 1.29199097e+02],\n",
      "       [3.63939762e-01, 9.63668827e+02, 6.36290578e+02, 5.17239588e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=79, weights=array([[1.31917203, 0.48481633, 2.35655804]]), adam_cache1=array([[-3.02466217e-03, -6.23980238e-01, -3.73086586e+00]]), adam_cache2=array([[7.32823256e-02, 1.99441122e+03, 4.64400567e+03]]))]\n",
      "Mean Squared Error\n",
      "4.324125184648122\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=80, weights=array([[-0.76353738,  1.72565842, -0.4090293 , -0.80235106],\n",
      "       [ 0.46180233,  3.92346953,  1.81529358, -2.39497042]]), adam_cache1=array([[-0.01308312,  0.02373779, -0.24249098,  0.01473243],\n",
      "       [-0.02415013, -0.28859152, -1.08766547,  0.36691932]]), adam_cache2=array([[5.89514129e-02, 2.24586643e+02, 1.17380419e+02, 1.29070078e+02],\n",
      "       [3.63579840e-01, 9.62745318e+02, 6.35683314e+02, 5.16726544e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=80, weights=array([[1.31861168, 0.48521027, 2.35999588]]), adam_cache1=array([[ 1.09310081e-03, -1.26774762e-01, -1.68827065e+00]]), adam_cache2=array([[7.32104989e-02, 1.99243572e+03, 4.63964039e+03]]))]\n",
      "Mean Squared Error\n",
      "4.572904330986816\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=81, weights=array([[-0.75751129,  1.72396488, -0.40811103, -0.80170163],\n",
      "       [ 0.46441065,  3.92114731,  1.81673437, -2.39503293]]), adam_cache1=array([[-0.01048093,  0.18180552, -0.071267  , -0.05285214],\n",
      "       [-0.01126633,  0.51616316, -0.26022859,  0.01017859]]), adam_cache2=array([[5.88926289e-02, 2.24364630e+02, 1.17265198e+02, 1.28941445e+02],\n",
      "       [3.63227220e-01, 9.61842774e+02, 6.35099279e+02, 5.16220061e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=81, weights=array([[1.31523615, 0.48386116, 2.35868012]]), adam_cache1=array([[0.00654266, 0.43137809, 0.64203203]]), adam_cache2=array([[7.31403785e-02, 1.99047304e+03, 4.63546795e+03]]))]\n",
      "Mean Squared Error\n",
      "4.4910744262044915\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=82, weights=array([[-0.75297322,  1.72114991, -0.4092239 , -0.80039027],\n",
      "       [ 0.46399862,  3.91604834,  1.81394499, -2.39333715]]), adam_cache1=array([[-0.00784278,  0.30027459,  0.08582292, -0.10604376],\n",
      "       [ 0.00176843,  1.12619067,  0.5006268 , -0.27438353]]), adam_cache2=array([[5.88339891e-02, 2.24142133e+02, 1.17150182e+02, 1.28812845e+02],\n",
      "       [3.62878173e-01, 9.60924708e+02, 6.34518178e+02, 5.15711880e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=82, weights=array([[1.30897689, 0.48123461, 2.35347068]]), adam_cache1=array([[0.01205537, 0.83451075, 2.52593619]]), adam_cache2=array([[7.30710413e-02, 1.98850248e+03, 4.63121199e+03]]))]\n",
      "Mean Squared Error\n",
      "4.2075177612295445\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=83, weights=array([[-0.74963084,  1.71792699, -0.41183751, -0.79888133],\n",
      "       [ 0.46114163,  3.90986014,  1.8080233 , -2.39102556]]), adam_cache1=array([[-0.00574011,  0.34163518,  0.20029209, -0.12125524],\n",
      "       [ 0.01218556,  1.35819261,  1.05616069, -0.37167627]]), adam_cache2=array([[5.87753289e-02, 2.23918501e+02, 1.17034546e+02, 1.28684099e+02],\n",
      "       [3.62526518e-01, 9.59975660e+02, 6.33920334e+02, 5.15197724e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=83, weights=array([[1.30037529, 0.4782761 , 2.34634304]]), adam_cache1=array([[0.01646308, 0.93408509, 3.43438536]]), adam_cache2=array([[7.30011211e-02, 1.98651733e+03, 4.62671558e+03]]))]\n",
      "Mean Squared Error\n",
      "4.032387376683874\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=84, weights=array([[-0.74695654,  1.71511434, -0.41521971, -0.79772952],\n",
      "       [ 0.45688137,  3.90452808,  1.80049886, -2.38946813]]), adam_cache1=array([[-0.00456428,  0.29629452,  0.25758531, -0.09198313],\n",
      "       [ 0.01805813,  1.16302211,  1.333704  , -0.2488622 ]]), adam_cache2=array([[5.87165898e-02, 2.23694595e+02, 1.16918109e+02, 1.28555445e+02],\n",
      "       [3.62169020e-01, 9.59016036e+02, 6.33301095e+02, 5.14683260e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=84, weights=array([[1.2904164 , 0.47605852, 2.33967431]]), adam_cache1=array([[0.01894282, 0.69580771, 3.19332441]]), adam_cache2=array([[7.29298224e-02, 1.98453291e+03, 4.62208991e+03]]))]\n",
      "Mean Squared Error\n",
      "4.088865989053326\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=85, weights=array([[-0.74432431,  1.71336896, -0.41863094, -0.79736142],\n",
      "       [ 0.45246012,  3.90162328,  1.79292744, -2.38972523]]), adam_cache1=array([[-0.0044649 ,  0.18273553,  0.2582014 , -0.02921555],\n",
      "       [ 0.01862548,  0.62970853,  1.33379758,  0.04083084]]), adam_cache2=array([[5.86578860e-02, 2.23471604e+02, 1.16801261e+02, 1.28427176e+02],\n",
      "       [3.61807414e-01, 9.58074410e+02, 6.32669575e+02, 5.14175589e+02]])),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=85, weights=array([[1.28028221, 0.47540734, 2.33546619]]), adam_cache1=array([[0.01915799, 0.20306715, 2.002706  ]]), adam_cache2=array([[7.28573376e-02, 1.98256628e+03, 4.61754374e+03]]))]\n",
      "Mean Squared Error\n",
      "4.223265904635736\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=86, weights=array([[-0.74114816,  1.71299629, -0.42148129, -0.7979203 ],\n",
      "       [ 0.4490433 ,  3.90187943,  1.78655301, -2.39215816]]), adam_cache1=array([[-0.00535482,  0.03878116,  0.21443735,  0.04408874],\n",
      "       [ 0.01430674, -0.05519202,  1.11611676,  0.38403436]]), adam_cache2=array([[5.85994067e-02, 2.23249712e+02, 1.16684492e+02, 1.28299244e+02],\n",
      "       [3.61446210e-01, 9.57155016e+02, 6.32037616e+02, 5.13673474e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=86, weights=array([[1.27108718, 0.47662984, 2.33475842]]), adam_cache1=array([[ 0.01727705, -0.37891821,  0.33480107]]), adam_cache2=array([[7.27844804e-02, 1.98061526e+03, 4.61314159e+03]]))]\n",
      "Mean Squared Error\n",
      "4.2352457113332775\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=87, weights=array([[-0.73699555,  1.71388105, -0.42341946, -0.79921497],\n",
      "       [ 0.44748776,  3.90502362,  1.78211182, -2.39630132]]), adam_cache1=array([[-0.00695906, -0.09151685,  0.14493728,  0.10152022],\n",
      "       [ 0.00647421, -0.67342646,  0.77295361,  0.65007069]]), adam_cache2=array([[5.85412651e-02, 2.23028061e+02, 1.16568038e+02, 1.28171327e+02],\n",
      "       [3.61088862e-01, 9.56236767e+02, 6.31410940e+02, 5.13169069e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=87, weights=array([[1.26365222, 0.47942645, 2.33739455]]), adam_cache1=array([[ 0.01388612, -0.86162031, -1.23953337]]), adam_cache2=array([[7.27119725e-02, 1.97866175e+03, 4.60876587e+03]]))]\n",
      "Mean Squared Error\n",
      "4.111917527990202\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=88, weights=array([[-0.73166213,  1.71554598, -0.42434837, -0.80077956],\n",
      "       [ 0.4481922 ,  3.90992525,  1.77979866, -2.40101823]]), adam_cache1=array([[-0.00888488, -0.17119412,  0.06905245,  0.12195735],\n",
      "       [-0.0029145 , -1.0436111 ,  0.40019868,  0.73569807]]), adam_cache2=array([[5.84834112e-02, 2.22805822e+02, 1.16451847e+02, 1.28043250e+02],\n",
      "       [3.60735414e-01, 9.55299674e+02, 6.30788259e+02, 5.12658169e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=88, weights=array([[1.25835759, 0.48299929, 2.34217825]]), adam_cache1=array([[ 0.00982996, -1.094239  , -2.23600609]]), adam_cache2=array([[7.26399721e-02, 1.97669325e+03, 4.60428264e+03]]))]\n",
      "Mean Squared Error\n",
      "4.006909029800791\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=89, weights=array([[-0.72519985,  1.7173155 , -0.42437866, -0.80202389],\n",
      "       [ 0.45104847,  3.91499772,  1.77937131, -2.40488096]]), adam_cache1=array([[-0.01070222, -0.18087867,  0.00223775,  0.09642362],\n",
      "       [-0.0117481 , -1.07363794,  0.0735012 ,  0.59893133]]), adam_cache2=array([[5.84256599e-02, 2.22583088e+02, 1.16335754e+02, 1.27915224e+02],\n",
      "       [3.60383005e-01, 9.54346180e+02, 6.30165689e+02, 5.12145910e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=89, weights=array([[1.25509703, 0.48631524, 2.34734236]]), adam_cache1=array([[ 0.00601799, -1.00959125, -2.39963612]]), adam_cache2=array([[7.25681325e-02, 1.97471662e+03, 4.59969335e+03]]))]\n",
      "Mean Squared Error\n",
      "4.0322306094489\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=90, weights=array([[-0.71789335,  1.71853575, -0.42374183, -0.80242977],\n",
      "       [ 0.45549839,  3.9187208 ,  1.78033962, -2.40664904]]), adam_cache1=array([[-0.01203003, -0.12400712, -0.0467873 ,  0.03126848],\n",
      "       [-0.01819665, -0.7834475 , -0.16557475,  0.27255588]]), adam_cache2=array([[5.83678093e-02, 2.22360655e+02, 1.16219657e+02, 1.27787617e+02],\n",
      "       [3.60028434e-01, 9.53395176e+02, 6.29540893e+02, 5.11640866e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=90, weights=array([[1.25333597, 0.48843722, 2.3511671 ]]), adam_cache1=array([[ 0.00323148, -0.64231568, -1.76693515]]), adam_cache2=array([[7.24960416e-02, 1.97274899e+03, 4.59510908e+03]]))]\n",
      "Mean Squared Error\n",
      "4.129636050832533\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=91, weights=array([[-0.71018624,  1.71878167, -0.42269318, -0.80172883],\n",
      "       [ 0.46068333,  3.92011566,  1.78217029, -2.40568896]]), adam_cache1=array([[-0.01261663, -0.02484831, -0.07660158, -0.05368958],\n",
      "       [-0.02108032, -0.29183311, -0.31123456, -0.14715076]]), adam_cache2=array([[5.83097618e-02, 2.22139047e+02, 1.16103556e+02, 1.27660499e+02],\n",
      "       [3.59670618e-01, 9.52458860e+02, 6.28913984e+02, 5.11144627e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=91, weights=array([[1.25225753, 0.48881648, 2.35254207]]), adam_cache1=array([[ 0.00196751, -0.11413921, -0.63155237]]), adam_cache2=array([[7.24236341e-02, 1.97079777e+03, 4.59060588e+03]]))]\n",
      "Mean Squared Error\n",
      "4.160443807302694\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=92, weights=array([[-0.70257257,  1.71797926, -0.42143612, -0.79999636],\n",
      "       [ 0.4656525 ,  3.91901678,  1.78444136, -2.4021886 ]]), adam_cache1=array([[-0.0123927 ,  0.08061408, -0.09130155, -0.13194566],\n",
      "       [-0.02008801,  0.22860152, -0.38390788, -0.53344763]]), adam_cache2=array([[5.82515597e-02, 2.21917969e+02, 1.15987502e+02, 1.27533538e+02],\n",
      "       [3.59311072e-01, 9.51530534e+02, 6.28286147e+02, 5.10649563e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=92, weights=array([[1.25096045, 0.48744937, 2.35128672]]), adam_cache1=array([[0.00235293, 0.40909853, 0.57333205]]), adam_cache2=array([[7.23512444e-02, 1.96885317e+03, 4.58614563e+03]]))]\n",
      "Mean Squared Error\n",
      "4.0798665353518215\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=93, weights=array([[-0.69548195,  1.7164    , -0.42009104, -0.79762431],\n",
      "       [ 0.4695786 ,  3.91605627,  1.78690388, -2.39709197]]), adam_cache1=array([[-0.01147629,  0.15776616, -0.0971442 , -0.17963817],\n",
      "       [-0.01578194,  0.61241376, -0.41392345, -0.77234432]]), adam_cache2=array([[5.81933186e-02, 2.21696777e+02, 1.15871537e+02, 1.27406375e+02],\n",
      "       [3.58952288e-01, 9.50595542e+02, 6.27658329e+02, 5.10147454e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=93, weights=array([[1.24866108, 0.48484955, 2.34813165]]), adam_cache1=array([[0.00414759, 0.77359756, 1.43284828]]), adam_cache2=array([[7.22793052e-02, 1.96690075e+03, 4.58164355e+03]]))]\n",
      "Mean Squared Error\n",
      "3.978137865382172\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=94, weights=array([[-0.6891899 ,  1.71453675, -0.41871119, -0.79518652],\n",
      "       [ 0.47192742,  3.91238332,  1.78944782, -2.39178475]]), adam_cache1=array([[-0.01012694,  0.18509777, -0.09909926, -0.18358733],\n",
      "       [-0.009389  ,  0.75554952, -0.42522508, -0.79976887]]), adam_cache2=array([[5.81351293e-02, 2.21475266e+02, 1.15755679e+02, 1.27279017e+02],\n",
      "       [3.58595654e-01, 9.49649123e+02, 6.27030948e+02, 5.09638402e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=94, weights=array([[1.2448523 , 0.48185375, 2.34438939]]), adam_cache1=array([[0.00683197, 0.88644921, 1.6900308 ]]), adam_cache2=array([[7.22079863e-02, 1.96493747e+03, 4.57707794e+03]]))]\n",
      "Mean Squared Error\n",
      "3.957945478404991\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=95, weights=array([[-0.68377431,  1.71291703, -0.41732837, -0.79325194],\n",
      "       [ 0.4725449 ,  3.90923728,  1.7920073 , -2.38765204]]), adam_cache1=array([[-0.00866816,  0.16001521, -0.09876356, -0.14488543],\n",
      "       [-0.00245464,  0.64358617, -0.42545798, -0.61933817]]), adam_cache2=array([[5.80770141e-02, 2.21253795e+02, 1.15639933e+02, 1.27151779e+02],\n",
      "       [3.58240653e-01, 9.48699607e+02, 6.26404100e+02, 5.09129773e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=95, weights=array([[1.23938676, 0.47933815, 2.34145411]]), adam_cache1=array([[0.00974969, 0.74024619, 1.31827014]]), adam_cache2=array([[7.21370750e-02, 1.96297286e+03, 4.57250497e+03]]))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error\n",
      "4.01616522554949\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=96, weights=array([[-0.679121  ,  1.71192653, -0.41600091, -0.79221496],\n",
      "       [ 0.47165248,  3.9075333 ,  1.79446   , -2.38566438]]), adam_cache1=array([[-0.00740731,  0.09731927, -0.094291  , -0.07723793],\n",
      "       [ 0.0035282 ,  0.34667835, -0.40547788, -0.29624813]]), adam_cache2=array([[5.80189526e-02, 2.21032759e+02, 1.15524296e+02, 1.27024910e+02],\n",
      "       [3.57885704e-01, 9.47756315e+02, 6.25777747e+02, 5.08627463e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=96, weights=array([[1.23247475, 0.47794662, 2.3403175 ]]), adam_cache1=array([[0.01226263, 0.407236  , 0.50767598]]), adam_cache2=array([[7.20661545e-02, 1.96101660e+03, 4.56797854e+03]]))]\n",
      "Mean Squared Error\n",
      "4.0659963719708765\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=97, weights=array([[-0.67496799,  1.71169986, -0.41483989, -0.79219452],\n",
      "       [ 0.46976537,  3.90759699,  1.79657158, -2.38612202]]), adam_cache1=array([[-0.0065751 ,  0.02214953, -0.08202223, -0.00151391],\n",
      "       [ 0.00742036, -0.01288791, -0.34719387,  0.06784068]]), adam_cache2=array([[5.79609345e-02, 2.20812155e+02, 1.15408772e+02, 1.26898348e+02],\n",
      "       [3.57529620e-01, 9.46819115e+02, 6.25152001e+02, 5.08130022e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=97, weights=array([[1.22460786, 0.47791736, 2.34126324]]), adam_cache1=array([[ 0.01388116,  0.00851621, -0.42013794]]), adam_cache2=array([[7.19948976e-02, 1.95906840e+03, 4.56348748e+03]]))]\n",
      "Mean Squared Error\n",
      "4.0495893760559465\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=98, weights=array([[-0.67097236,  1.71210454, -0.41400134, -0.79302406],\n",
      "       [ 0.46756124,  3.90911947,  1.79801259, -2.38862235]]), adam_cache1=array([[-0.00629201, -0.03933337, -0.05892263,  0.06112236],\n",
      "       [ 0.00862046, -0.30642394, -0.2356651 ,  0.36865772]]), adam_cache2=array([[5.79029876e-02, 2.20591694e+02, 1.15293386e+02, 1.26771840e+02],\n",
      "       [3.57172468e-01, 9.45880988e+02, 6.24527439e+02, 5.07631354e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=98, weights=array([[1.2164334 , 0.4790533 , 2.34382613]]), adam_cache1=array([[ 0.01434653, -0.32886469, -1.13244095]]), adam_cache2=array([[7.19232463e-02, 1.95712065e+03, 4.55898090e+03]]))]\n",
      "Mean Squared Error\n",
      "3.9963833943698424\n",
      "\n",
      "Weights\n",
      "[Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=99, weights=array([[-0.66678496,  1.71281258, -0.4136461 , -0.79432558],\n",
      "       [ 0.46572993,  3.91132711,  1.79844473, -2.39224189]]), adam_cache1=array([[-0.00655895, -0.06845265, -0.02482922,  0.09538998],\n",
      "       [ 0.00712427, -0.44196304, -0.07029749,  0.5308466 ]]), adam_cache2=array([[5.78451649e-02, 2.20371211e+02, 1.15178172e+02, 1.26645231e+02],\n",
      "       [3.56815336e-01, 9.44937868e+02, 6.23904922e+02, 5.07127685e+02]])),\n",
      " Weights(adam_gradient=AdamGradient(learning_rate=0.5, decay1=0.9, decay2=0.999), time=99, weights=array([[1.20860954, 0.48083236, 2.34700838]]), adam_cache1=array([[ 0.01365827, -0.51231751, -1.39864935]]), adam_cache2=array([[7.18513787e-02, 1.95516821e+03, 4.55443631e+03]]))]\n",
      "Mean Squared Error\n",
      "3.970570625619249\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "'''An interface for different kinds of function approximations\n",
    "(tabular, linear, DNN... etc), with several implementations.'''\n",
    "import sys\n",
    "sys.path.append(\"../../RL-book\")\n",
    "\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass, replace, field\n",
    "import itertools\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "from scipy.interpolate import splrep, BSpline\n",
    "from typing import (Callable, Dict, Generic, Iterator, Iterable, List,\n",
    "                    Mapping, Optional, Sequence, Tuple, TypeVar)\n",
    "\n",
    "import rl.iterate as iterate\n",
    "\n",
    "X = TypeVar('X')\n",
    "SMALL_NUM = 1e-6\n",
    "\n",
    "\n",
    "class FunctionApprox(ABC, Generic[X]):\n",
    "    '''Interface for function approximations.\n",
    "    An object of this class approximates some function X   in a way\n",
    "    that can be evaluated at specific points in X and updated with\n",
    "    additional (X, ) points.\n",
    "    '''\n",
    "\n",
    "    @abstractmethod\n",
    "    def representational_gradient(self, x_value: X) -> FunctionApprox[X]:\n",
    "        '''Computes the gradient of the self FunctionApprox with respect\n",
    "        to the parameters in the internal representation of the\n",
    "        FunctionApprox, i.e., computes Gradient with respect to internal\n",
    "        parameters of expected value of y for the input x, where the\n",
    "        expectation is with respect tp the FunctionApprox's model of\n",
    "        the probability distribution of y|x. The gradient is output\n",
    "        in the form of a FunctionApprox whose internal parameters are\n",
    "        equal to the gradient values.\n",
    "        '''\n",
    "\n",
    "    @abstractmethod\n",
    "    def evaluate(self, x_values_seq: Iterable[X]) -> np.ndarray:\n",
    "        '''Computes expected value of y for each x in\n",
    "        x_values_seq (with the probability distribution\n",
    "        function of y|x estimated as FunctionApprox)\n",
    "        '''\n",
    "\n",
    "    def __call__(self, x_value: X) -> float:\n",
    "        return self.evaluate([x_value]).item()\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(\n",
    "        self,\n",
    "        xy_vals_seq: Iterable[Tuple[X, float]]\n",
    "    ) -> FunctionApprox[X]:\n",
    "\n",
    "        '''Update the internal parameters of the FunctionApprox\n",
    "        based on incremental data provided in the form of (x,y)\n",
    "        pairs as a xy_vals_seq data structure\n",
    "        '''\n",
    "\n",
    "    @abstractmethod\n",
    "    def solve(\n",
    "        self,\n",
    "        xy_vals_seq: Iterable[Tuple[X, float]],\n",
    "        error_tolerance: Optional[float] = None\n",
    "    ) -> FunctionApprox[X]:\n",
    "        '''Assuming the entire data set of (x,y) pairs is available\n",
    "        in the form of the given input xy_vals_seq data structure,\n",
    "        solve for the internal parameters of the FunctionApprox\n",
    "        such that the internal parameters are fitted to xy_vals_seq.\n",
    "        Since this is a best-fit, the internal parameters are fitted\n",
    "        to within the input error_tolerance (where applicable, since\n",
    "        some methods involve a direct solve for the fit that don't\n",
    "        require an error_tolerance)\n",
    "        '''\n",
    "\n",
    "    @abstractmethod\n",
    "    def within(self, other: FunctionApprox[X], tolerance: float) -> bool:\n",
    "        '''Is this function approximation within a given tolerance of\n",
    "        another function approximation of the same type?\n",
    "        '''\n",
    "\n",
    "    def argmax(self, xs: Iterable[X]) -> X:\n",
    "        '''Return the input X that maximizes the function being approximated.\n",
    "        Arguments:\n",
    "          xs -- list of inputs to evaluate and maximize, cannot be empty\n",
    "        Returns the X that maximizes the function this approximates.\n",
    "        '''\n",
    "        return list(xs)[np.argmax(self.evaluate(xs))]\n",
    "\n",
    "    def rmse(\n",
    "        self,\n",
    "        xy_vals_seq: Iterable[Tuple[X, float]]\n",
    "    ) -> float:\n",
    "        '''The Root-Mean-Squared-Error between FunctionApprox's\n",
    "        predictions (from evaluate) and the associated (supervisory)\n",
    "        y values\n",
    "        '''\n",
    "        x_seq, y_seq = zip(*xy_vals_seq)\n",
    "        errors: np.ndarray = self.evaluate(x_seq) - np.array(y_seq)\n",
    "        return np.sqrt(np.mean(errors * errors))\n",
    "\n",
    "    def iterate_updates(\n",
    "        self,\n",
    "        xy_seq_stream: Iterator[Iterable[Tuple[X, float]]]\n",
    "    ) -> Iterator[FunctionApprox[X]]:\n",
    "        '''Given a stream (Iterator) of data sets of (x,y) pairs,\n",
    "        perform a series of incremental updates to the internal\n",
    "        parameters (using update method), with each internal\n",
    "        parameter update done for each data set of (x,y) pairs in the\n",
    "        input stream of xy_seq_stream\n",
    "        '''\n",
    "        return iterate.accumulate(\n",
    "            xy_seq_stream,\n",
    "            lambda fa, xy: fa.update(xy),\n",
    "            initial=self\n",
    "        )\n",
    "\n",
    "    def representational_gradient_stream(\n",
    "        self,\n",
    "        x_values_seq: Iterable[X]\n",
    "    ) -> Iterator[FunctionApprox[X]]:\n",
    "        for x_val in x_values_seq:\n",
    "            yield self.representational_gradient(x_val)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Dynamic(FunctionApprox[X]):\n",
    "    '''A FunctionApprox that works exactly the same as exact dynamic\n",
    "    programming. Each update for a value in X replaces the previous\n",
    "    value at X altogether.\n",
    "\n",
    "    Fields:\n",
    "    values_map -- mapping from X to its approximated value\n",
    "    '''\n",
    "\n",
    "    values_map: Mapping[X, float]\n",
    "\n",
    "    def representational_gradient(self, x_value: X) -> Dynamic[X]:\n",
    "        return Dynamic({x_value: 1.0})\n",
    "\n",
    "    def evaluate(self, x_values_seq: Iterable[X]) -> np.ndarray:\n",
    "        '''Evaluate the function approximation by looking up the value in the\n",
    "        mapping for each state.\n",
    "\n",
    "        Will raise an error if an X value has not been seen before and\n",
    "        was not initialized.\n",
    "\n",
    "        '''\n",
    "        return np.array([self.values_map[x] for x in x_values_seq])\n",
    "\n",
    "    def update(self, xy_vals_seq: Iterable[Tuple[X, float]]) -> Dynamic[X]:\n",
    "        '''Update each X value by replacing its saved Y with a new one. Pairs\n",
    "        later in the list take precedence over pairs earlier in the\n",
    "        list.\n",
    "\n",
    "        '''\n",
    "        new_map = dict(self.values_map)\n",
    "        for x, y in xy_vals_seq:\n",
    "            new_map[x] = y\n",
    "\n",
    "        return replace(self, values_map=new_map)\n",
    "\n",
    "    def solve(\n",
    "        self,\n",
    "        xy_vals_seq: Iterable[Tuple[X, float]],\n",
    "        error_tolerance: Optional[float] = None\n",
    "    ) -> Dynamic[X]:\n",
    "        return replace(self, value_map=dict(xy_vals_seq))\n",
    "\n",
    "    def within(self, other: FunctionApprox[X], tolerance: float) -> bool:\n",
    "        '''This approximation is within a tolerance of another if the value\n",
    "        for each X in both approximations is within the given\n",
    "        tolerance.\n",
    "\n",
    "        Raises an error if the other approximation is missing states\n",
    "        that this approximation has.\n",
    "\n",
    "        '''\n",
    "        if not isinstance(other, Dynamic):\n",
    "            return False\n",
    "\n",
    "        return all(abs(self.values_map[s] - other.values_map[s]) <= tolerance\n",
    "                   for s in self.values_map)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Tabular(FunctionApprox[X]):\n",
    "    '''Approximates a function with a discrete domain (`X'), without any\n",
    "    interpolation. The value for each `X' is maintained as a weighted\n",
    "    mean of observations by recency (managed by\n",
    "    `count_to_weight_func').\n",
    "\n",
    "    In practice, this means you can use this to approximate a function\n",
    "    with a learning rate (n) specified by count_to_weight_func.\n",
    "\n",
    "    If `count_to_weight_func' always returns 1, this behaves the same\n",
    "    way as `Dynamic'.\n",
    "\n",
    "    Fields:\n",
    "    values_map -- mapping from X to its approximated value\n",
    "    counts_map -- how many times a given X has been updated\n",
    "    count_to_weight_func -- function for how much to weigh an update\n",
    "      to X based on the number of times that X has been updated\n",
    "\n",
    "    '''\n",
    "\n",
    "    values_map: Mapping[X, float] = field(default_factory=lambda: {})\n",
    "    counts_map: Mapping[X, int] = field(default_factory=lambda: {})\n",
    "    count_to_weight_func: Callable[[int], float] = \\\n",
    "        field(default_factory=lambda: lambda n: 1.0 / n)\n",
    "\n",
    "    def representational_gradient(self, x_value: X) -> Tabular[X]:\n",
    "        return Tabular({x_value: 1.0})\n",
    "\n",
    "    def evaluate(self, x_values_seq: Iterable[X]) -> np.ndarray:\n",
    "        '''Evaluate the approximation at each given X.\n",
    "\n",
    "        If an X has not been seen before, will return 0.0.\n",
    "        '''\n",
    "        return np.array([self.values_map.get(x, 0.) for x in x_values_seq])\n",
    "\n",
    "    def update(self, xy_vals_seq: Iterable[Tuple[X, float]]) -> Tabular[X]:\n",
    "        '''Update the approximation with the given points.\n",
    "\n",
    "        Each X keeps a count n of how many times it was updated, and\n",
    "        each subsequent update is discounted by\n",
    "        count_to_weight_func(n), which defines our learning rate.\n",
    "\n",
    "        '''\n",
    "        values_map: Dict[X, float] = dict(self.values_map)\n",
    "        counts_map: Dict[X, int] = dict(self.counts_map)\n",
    "\n",
    "        for x, y in xy_vals_seq:\n",
    "            counts_map[x] = counts_map.get(x, 0) + 1\n",
    "            weight: float = self.count_to_weight_func(counts_map.get(x, 0))\n",
    "            values_map[x] = weight * y + (1 - weight) * values_map.get(x, 0.)\n",
    "\n",
    "        return replace(\n",
    "            self,\n",
    "            values_map=values_map,\n",
    "            counts_map=counts_map\n",
    "        )\n",
    "\n",
    "    def solve(\n",
    "        self,\n",
    "        xy_vals_seq: Iterable[Tuple[X, float]],\n",
    "        error_tolerance: Optional[float] = None\n",
    "    ) -> Tabular[X]:\n",
    "        values_map: Dict[X, float] = {}\n",
    "        counts_map: Dict[X, int] = {}\n",
    "        for x, y in xy_vals_seq:\n",
    "            counts_map[x] = counts_map.get(x, 0) + 1\n",
    "            weight: float = self.count_to_weight_func(counts_map.get(x, 0))\n",
    "            values_map[x] = weight * y + (1 - weight) * values_map.get(x, 0.)\n",
    "        return replace(\n",
    "            self,\n",
    "            values_map=values_map,\n",
    "            counts_map=counts_map\n",
    "        )\n",
    "\n",
    "    def within(self, other: FunctionApprox[X], tolerance: float) -> bool:\n",
    "        if isinstance(other, Tabular):\n",
    "            return\\\n",
    "                all(abs(self.values_map[s] - other.values_map[s]) <= tolerance\n",
    "                    for s in self.values_map)\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class BSplineApprox(FunctionApprox[X]):\n",
    "    feature_function: Callable[[X], float]\n",
    "    degree: int\n",
    "    knots: np.ndarray = field(default_factory=lambda: np.array([]))\n",
    "    coeffs: np.ndarray = field(default_factory=lambda: np.array([]))\n",
    "\n",
    "    def get_feature_values(self, x_values_seq: Iterable[X]) -> Sequence[float]:\n",
    "        return [self.feature_function(x) for x in x_values_seq]\n",
    "\n",
    "    def representational_gradient(self, x_value: X) -> BSplineApprox[X]:\n",
    "        feature_val: float = self.feature_function(x_value)\n",
    "        eps: float = 1e-6\n",
    "        one_hots: np.array = np.eye(len(self.coeffs))\n",
    "        return replace(\n",
    "            self,\n",
    "            coeffs=np.array([(\n",
    "                BSpline(\n",
    "                    self.knots,\n",
    "                    c + one_hots[i] * eps,\n",
    "                    self.degree\n",
    "                )(feature_val) -\n",
    "                BSpline(\n",
    "                    self.knots,\n",
    "                    c - one_hots[i] * eps,\n",
    "                    self.degree\n",
    "                )(feature_val)\n",
    "            ) / (2 * eps) for i, c in enumerate(self.coeffs)]))\n",
    "\n",
    "    def evaluate(self, x_values_seq: Iterable[X]) -> np.ndarray:\n",
    "        spline_func: Callable[[Sequence[float]], np.ndarray] = \\\n",
    "            BSpline(self.knots, self.coeffs, self.degree)\n",
    "        return spline_func(self.get_feature_values(x_values_seq))\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        xy_vals_seq: Iterable[Tuple[X, float]]\n",
    "    ) -> BSplineApprox[X]:\n",
    "        x_vals, y_vals = zip(*xy_vals_seq)\n",
    "        feature_vals: Sequence[float] = self.get_feature_values(x_vals)\n",
    "        sorted_pairs: Sequence[Tuple[float, float]] = \\\n",
    "            sorted(zip(feature_vals, y_vals), key=itemgetter(0))\n",
    "        new_knots, new_coeffs, _ = splrep(\n",
    "            [f for f, _ in sorted_pairs],\n",
    "            [y for _, y in sorted_pairs],\n",
    "            k=self.degree\n",
    "        )\n",
    "        return replace(\n",
    "            self,\n",
    "            knots=new_knots,\n",
    "            coeffs=new_coeffs\n",
    "        )\n",
    "\n",
    "    def solve(\n",
    "        self,\n",
    "        xy_vals_seq: Iterable[Tuple[X, float]],\n",
    "        error_tolerance: Optional[float] = None\n",
    "    ) -> BSplineApprox[X]:\n",
    "        return self.update(xy_vals_seq)\n",
    "\n",
    "    def within(self, other: FunctionApprox[X], tolerance: float) -> bool:\n",
    "        if isinstance(other, BSplineApprox):\n",
    "            return \\\n",
    "                np.all(np.abs(self.knots - other.knots) <= tolerance).item() \\\n",
    "                and \\\n",
    "                np.all(np.abs(self.coeffs - other.coeffs) <= tolerance).item()\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class AdamGradient:\n",
    "    learning_rate: float\n",
    "    decay1: float\n",
    "    decay2: float\n",
    "\n",
    "    @staticmethod\n",
    "    def default_settings() -> AdamGradient:\n",
    "        return AdamGradient(\n",
    "            learning_rate=0.001,\n",
    "            decay1=0.9,\n",
    "            decay2=0.999\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Weights:\n",
    "    adam_gradient: AdamGradient\n",
    "    time: int\n",
    "    weights: np.ndarray\n",
    "    adam_cache1: np.ndarray\n",
    "    adam_cache2: np.ndarray\n",
    "\n",
    "    @staticmethod\n",
    "    def create(\n",
    "        weights: np.ndarray,\n",
    "        adam_gradient: AdamGradient = AdamGradient.default_settings(),\n",
    "        adam_cache1: Optional[np.ndarray] = None,\n",
    "        adam_cache2: Optional[np.ndarray] = None\n",
    "    ) -> Weights:\n",
    "        return Weights(\n",
    "            adam_gradient=adam_gradient,\n",
    "            time=0,\n",
    "            weights=weights,\n",
    "            adam_cache1=np.zeros_like(\n",
    "                weights\n",
    "            ) if adam_cache1 is None else adam_cache1,\n",
    "            adam_cache2=np.zeros_like(\n",
    "                weights\n",
    "            ) if adam_cache2 is None else adam_cache2\n",
    "        )\n",
    "\n",
    "    def update(self, gradient: np.ndarray) -> Weights:\n",
    "        time: int = self.time + 1\n",
    "        new_adam_cache1: np.ndarray = self.adam_gradient.decay1 * \\\n",
    "            self.adam_cache1 + (1 - self.adam_gradient.decay1) * gradient\n",
    "        new_adam_cache2: np.ndarray = self.adam_gradient.decay2 * \\\n",
    "            self.adam_cache2 + (1 - self.adam_gradient.decay2) * gradient ** 2\n",
    "        corrected_m: np.ndarray = new_adam_cache1 / \\\n",
    "            (1 - self.adam_gradient.decay1 ** time)\n",
    "        corrected_v: np.ndarray = new_adam_cache2 / \\\n",
    "            (1 - self.adam_gradient.decay2 ** time)\n",
    "\n",
    "        new_weights: np.ndarray = self.weights - \\\n",
    "            self.adam_gradient.learning_rate * corrected_m / \\\n",
    "            (np.sqrt(corrected_v) + SMALL_NUM)\n",
    "\n",
    "        return replace(\n",
    "            self,\n",
    "            time=time,\n",
    "            weights=new_weights,\n",
    "            adam_cache1=new_adam_cache1,\n",
    "            adam_cache2=new_adam_cache2,\n",
    "        )\n",
    "\n",
    "    def within(self, other: Weights, tolerance: float) -> bool:\n",
    "        return np.all(np.abs(self.weights - other.weights) <= tolerance).item()\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class LinearFunctionApprox(FunctionApprox[X]):\n",
    "\n",
    "    feature_functions: Sequence[Callable[[X], float]]\n",
    "    regularization_coeff: float\n",
    "    weights: Weights\n",
    "    direct_solve: bool\n",
    "\n",
    "    @staticmethod\n",
    "    def create(\n",
    "        feature_functions: Sequence[Callable[[X], float]],\n",
    "        adam_gradient: AdamGradient = AdamGradient.default_settings(),\n",
    "        regularization_coeff: float = 0.,\n",
    "        weights: Optional[Weights] = None,\n",
    "        direct_solve: bool = True\n",
    "    ) -> LinearFunctionApprox[X]:\n",
    "        return LinearFunctionApprox(\n",
    "            feature_functions=feature_functions,\n",
    "            regularization_coeff=regularization_coeff,\n",
    "            weights=Weights.create(\n",
    "                adam_gradient=adam_gradient,\n",
    "                weights=np.zeros(len(feature_functions))\n",
    "            ) if weights is None else weights,\n",
    "            direct_solve=direct_solve\n",
    "        )\n",
    "\n",
    "    def get_feature_values(self, x_values_seq: Iterable[X]) -> np.ndarray:\n",
    "        return np.array(\n",
    "            [[f(x) for f in self.feature_functions] for x in x_values_seq]\n",
    "        )\n",
    "\n",
    "    def representational_gradient(self, x_value: X) -> LinearFunctionApprox[X]:\n",
    "        return replace(\n",
    "            self,\n",
    "            weights=replace(\n",
    "                self.weights,\n",
    "                weights=np.array([f(x_value) for f in self.feature_functions])\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def evaluate(self, x_values_seq: Iterable[X]) -> np.ndarray:\n",
    "        return np.dot(\n",
    "            self.get_feature_values(x_values_seq),\n",
    "            self.weights.weights\n",
    "        )\n",
    "\n",
    "    def within(self, other: FunctionApprox[X], tolerance: float) -> bool:\n",
    "        if isinstance(other, LinearFunctionApprox):\n",
    "            return self.weights.within(other.weights, tolerance)\n",
    "\n",
    "        return False\n",
    "\n",
    "    def regularized_loss_gradient(\n",
    "        self,\n",
    "        xy_vals_seq: Iterable[Tuple[X, float]]\n",
    "    ) -> np.ndarray:\n",
    "        x_vals, y_vals = zip(*xy_vals_seq)\n",
    "        feature_vals: np.ndarray = self.get_feature_values(x_vals)\n",
    "        diff: np.ndarray = np.dot(feature_vals, self.weights.weights) \\\n",
    "            - np.array(y_vals)\n",
    "        return np.dot(feature_vals.T, diff) / len(diff) \\\n",
    "            + self.regularization_coeff * self.weights.weights\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        xy_vals_seq: Iterable[Tuple[X, float]]\n",
    "    ) -> LinearFunctionApprox[X]:\n",
    "        gradient: np.ndarray = self.regularized_loss_gradient(xy_vals_seq)\n",
    "        new_weights: np.ndarray = self.weights.update(gradient)\n",
    "        return replace(self, weights=new_weights)\n",
    "\n",
    "    def solve(\n",
    "        self,\n",
    "        xy_vals_seq: Iterable[Tuple[X, float]],\n",
    "        error_tolerance: Optional[float] = None\n",
    "    ) -> LinearFunctionApprox[X]:\n",
    "        if self.direct_solve:\n",
    "            x_vals, y_vals = zip(*xy_vals_seq)\n",
    "            feature_vals: np.ndarray = self.get_feature_values(x_vals)\n",
    "            feature_vals_T: np.ndarray = feature_vals.T\n",
    "            left: np.ndarray = np.dot(feature_vals_T, feature_vals) \\\n",
    "                + feature_vals.shape[0] * self.regularization_coeff * \\\n",
    "                np.eye(len(self.weights.weights))\n",
    "            right: np.ndarray = np.dot(feature_vals_T, y_vals)\n",
    "            ret = replace(\n",
    "                self,\n",
    "                weights=Weights.create(\n",
    "                    adam_gradient=self.weights.adam_gradient,\n",
    "                    weights=np.dot(np.linalg.inv(left), right)\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            tol: float = 1e-6 if error_tolerance is None else error_tolerance\n",
    "\n",
    "            def done(\n",
    "                a: LinearFunctionApprox[X],\n",
    "                b: LinearFunctionApprox[X],\n",
    "                tol: float = tol\n",
    "            ) -> bool:\n",
    "                return a.within(b, tol)\n",
    "\n",
    "            ret = iterate.converged(\n",
    "                self.iterate_updates(itertools.repeat(xy_vals_seq)),\n",
    "                done=done\n",
    "            )\n",
    "\n",
    "        return ret\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DNNSpec:\n",
    "    neurons: Sequence[int]\n",
    "    bias: bool\n",
    "    hidden_activation: Callable[[np.ndarray], np.ndarray]\n",
    "    hidden_activation_deriv: Callable[[np.ndarray], np.ndarray]\n",
    "    output_activation: Callable[[np.ndarray], np.ndarray]\n",
    "    output_activation_deriv: Callable[[np.ndarray], np.ndarray]\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DNNApprox(FunctionApprox[X]):\n",
    "\n",
    "    feature_functions: Sequence[Callable[[X], float]]\n",
    "    dnn_spec: DNNSpec\n",
    "    regularization_coeff: float\n",
    "    weights: Sequence[Weights]\n",
    "\n",
    "    @staticmethod\n",
    "    def create(\n",
    "        feature_functions: Sequence[Callable[[X], float]],\n",
    "        dnn_spec: DNNSpec,\n",
    "        adam_gradient: AdamGradient = AdamGradient.default_settings(),\n",
    "        regularization_coeff: float = 0.,\n",
    "        weights: Optional[Sequence[Weights]] = None\n",
    "    ) -> DNNApprox[X]:\n",
    "        if weights is None:\n",
    "            inputs: Sequence[int] = [len(feature_functions)] + \\\n",
    "                [n + (1 if dnn_spec.bias else 0)\n",
    "                 for i, n in enumerate(dnn_spec.neurons)]\n",
    "            outputs: Sequence[int] = list(dnn_spec.neurons) + [1]\n",
    "            wts = [Weights.create(\n",
    "                weights=np.random.randn(output, inp) / np.sqrt(inp),\n",
    "                adam_gradient=adam_gradient\n",
    "            ) for inp, output in zip(inputs, outputs)]\n",
    "        else:\n",
    "            wts = weights\n",
    "\n",
    "        return DNNApprox(\n",
    "            feature_functions=feature_functions,\n",
    "            dnn_spec=dnn_spec,\n",
    "            regularization_coeff=regularization_coeff,\n",
    "            weights=wts\n",
    "        )\n",
    "\n",
    "    def get_feature_values(self, x_values_seq: Iterable[X]) -> np.ndarray:\n",
    "        return np.array(\n",
    "            [[f(x) for f in self.feature_functions] for x in x_values_seq]\n",
    "        )\n",
    "\n",
    "    def forward_propagation(\n",
    "        self,\n",
    "        x_values_seq: Iterable[X]\n",
    "    ) -> Sequence[np.ndarray]:\n",
    "        \"\"\"\n",
    "        :param x_values_seq: a n-length-sequence of input points\n",
    "        :return: list of length (L+2) where the first (L+1) values\n",
    "                 each represent the 2-D input arrays (of size n x |I_l|),\n",
    "                 for each of the (L+1) layers (L of which are hidden layers),\n",
    "                 and the last value represents the output of the DNN (as a\n",
    "                 1-D array of length n)\n",
    "        \"\"\"\n",
    "        inp: np.ndarray = self.get_feature_values(x_values_seq)\n",
    "        ret: List[np.ndarray] = [inp]\n",
    "        for w in self.weights[:-1]:\n",
    "            out: np.ndarray = self.dnn_spec.hidden_activation(\n",
    "                np.dot(inp, w.weights.T)\n",
    "            )\n",
    "            if self.dnn_spec.bias:\n",
    "                inp = np.insert(out, 0, 1., axis=1)\n",
    "            else:\n",
    "                inp = out\n",
    "            ret.append(inp)\n",
    "        ret.append(\n",
    "            self.dnn_spec.output_activation(\n",
    "                np.dot(inp, self.weights[-1].weights.T)\n",
    "            )[:, 0]\n",
    "        )\n",
    "        return ret\n",
    "\n",
    "    def evaluate(self, x_values_seq: Iterable[X]) -> np.ndarray:\n",
    "        return self.forward_propagation(x_values_seq)[-1]\n",
    "\n",
    "    def within(self, other: FunctionApprox[X], tolerance: float) -> bool:\n",
    "        if isinstance(other, DNNApprox):\n",
    "            return all(w1.within(w2, tolerance)\n",
    "                       for w1, w2 in zip(self.weights, other.weights))\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def backward_propagation(\n",
    "        self,\n",
    "        fwd_prop: Sequence[np.ndarray],\n",
    "        objective_derivative_output: Callable[[np.ndarray], np.ndarray]\n",
    "    ) -> Sequence[np.ndarray]:\n",
    "        \"\"\"\n",
    "        :param\n",
    "        fwd_prop represents the result of forward propagation, a sequence\n",
    "        of (L+1) 2-D np.ndarrays, followed by a 1-D np.ndarray for the output\n",
    "        of the DNN.\n",
    "        objective_derivative_output represents the derivative of the objective\n",
    "        function with respect to the output of the DNN\n",
    "\n",
    "        :return: list (of length L+1) of |O_l| x |I_l| 2-D arrays,\n",
    "                 i.e., same as the type of self.weights.weights\n",
    "        This function computes the gradient (with respect to weights) of\n",
    "        cross-entropy loss where the output layer activation function\n",
    "        is the canonical link function of the conditional distribution of y|x\n",
    "        \"\"\"\n",
    "        layer_inputs: Sequence[np.ndarray] = fwd_prop[:-1]\n",
    "        deriv: np.ndarray = objective_derivative_output(fwd_prop[-1]) * \\\n",
    "            self.dnn_spec.output_activation_deriv(fwd_prop[-1])\n",
    "        deriv = deriv.reshape(1, -1)\n",
    "        back_prop: List[np.ndarray] = [np.dot(deriv, layer_inputs[-1]) /\n",
    "                                       deriv.shape[1]]\n",
    "        # L is the number of hidden layers, n is the number of points\n",
    "        # layer l deriv represents dObj/dS_l where S_l = I_l . weights_l\n",
    "        # (S_l is the result of applying layer l without the activation func)\n",
    "        for i in reversed(range(len(self.weights) - 1)):\n",
    "            # deriv_l is a 2-D array of dimension |O_l| x n\n",
    "            # The recursive formulation of deriv is as follows:\n",
    "            # deriv_{l-1} = (weights_l^T inner deriv_l) haddamard g'(S_{l-1}),\n",
    "            # which is ((|I_l| x |O_l|) inner (|O_l| x n)) haddamard\n",
    "            # (|I_l| x n), which is (|I_l| x n) = (|O_{l-1}| x n)\n",
    "            # Note: g'(S_{l-1}) is expressed as hidden layer activation\n",
    "            # derivative as a function of O_{l-1} (=I_l).\n",
    "            deriv = np.dot(self.weights[i + 1].weights.T, deriv) * \\\n",
    "                self.dnn_spec.hidden_activation_deriv(layer_inputs[i + 1].T)\n",
    "            # If self.dnn_spec.bias is True, then I_l = O_{l-1} + 1, in which\n",
    "            # case # the first row of the calculated deriv is removed to yield\n",
    "            # a 2-D array of dimension |O_{l-1}| x n.\n",
    "            if self.dnn_spec.bias:\n",
    "                deriv = deriv[1:]\n",
    "            # layer l gradient is deriv_l inner layer_inputs_l, which is\n",
    "            # of dimension (|O_l| x n) inner (n x (|I_l|) = |O_l| x |I_l|\n",
    "            back_prop.append(np.dot(deriv, layer_inputs[i]) / deriv.shape[1])\n",
    "        return back_prop[::-1]\n",
    "\n",
    "    def representational_gradient(self, x_value: X) -> DNNApprox[X]:\n",
    "        return replace(\n",
    "            self,\n",
    "            weights=replace(\n",
    "                self.weights,\n",
    "                weights=self.backward_propagation(\n",
    "                    fwd_prop=self.forward_propagation([x_value]),\n",
    "                    objective_derivative_output=lambda arr: np.ones_like(arr)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def regularized_loss_gradient(\n",
    "        self,\n",
    "        xy_vals_seq: Iterable[Tuple[X, float]]\n",
    "    ) -> Sequence[np.ndarray]:\n",
    "        \"\"\"\n",
    "        :param pairs: list of pairs of n (x, y) points\n",
    "        :return: list (of length L+1) of |O_l| x |I_l| 2-D array,\n",
    "                 i.e., same as the type of self.weights.weights\n",
    "        This function computes the regularized gradient (with respect to\n",
    "        weights) of cross-entropy loss where the output layer activation\n",
    "        function is the canonical link function of the conditional\n",
    "        distribution of y|x\n",
    "        \"\"\"\n",
    "        x_vals, y_vals = zip(*xy_vals_seq)\n",
    "        fwd_prop: Sequence[np.ndarray] = self.forward_propagation(x_vals)\n",
    "\n",
    "        def obj_deriv_output(out: np.ndarray) -> np.ndarray:\n",
    "            return (out - np.array(y_vals)) / \\\n",
    "                self.dnn_spec.output_activation_deriv(out)\n",
    "\n",
    "        return [x + self.regularization_coeff * self.weights[i].weights\n",
    "                for i, x in enumerate(self.backward_propagation(\n",
    "                    fwd_prop=fwd_prop,\n",
    "                    objective_derivative_output=obj_deriv_output\n",
    "                ))]\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        xy_vals_seq: Iterable[Tuple[X, float]]\n",
    "    ) -> DNNApprox[X]:\n",
    "        return replace(\n",
    "            self,\n",
    "            weights=[w.update(g) for w, g in zip(\n",
    "                self.weights,\n",
    "                self.regularized_loss_gradient(xy_vals_seq)\n",
    "            )]\n",
    "        )\n",
    "\n",
    "    def solve(\n",
    "        self,\n",
    "        xy_vals_seq: Iterable[Tuple[X, float]],\n",
    "        error_tolerance: Optional[float] = None\n",
    "    ) -> DNNApprox[X]:\n",
    "        tol: float = 1e-6 if error_tolerance is None else error_tolerance\n",
    "\n",
    "        def done(\n",
    "            a: DNNApprox[X],\n",
    "            b: DNNApprox[X],\n",
    "            tol: float = tol\n",
    "        ) -> bool:\n",
    "            return a.within(b, tol)\n",
    "\n",
    "        return iterate.converged(\n",
    "            self.iterate_updates(itertools.repeat(xy_vals_seq)),\n",
    "            done=done\n",
    "        )\n",
    "\n",
    "\n",
    "def learning_rate_schedule(\n",
    "    initial_learning_rate: float,\n",
    "    half_life: float,\n",
    "    exponent: float\n",
    ") -> Callable[[int], float]:\n",
    "    def lr_func(n: int) -> float:\n",
    "        return initial_learning_rate * (1 + (n - 1) / half_life) ** -exponent\n",
    "    return lr_func\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    from scipy.stats import norm\n",
    "    from pprint import pprint\n",
    "\n",
    "    alpha = 2.0\n",
    "    beta_1 = 10.0\n",
    "    beta_2 = 4.0\n",
    "    beta_3 = -6.0\n",
    "    beta = (beta_1, beta_2, beta_3) # Beta is the weights?\n",
    "\n",
    "    x_pts = np.arange(-10.0, 10.5, 0.5)\n",
    "    y_pts = np.arange(-10.0, 10.5, 0.5)\n",
    "    z_pts = np.arange(-10.0, 10.5, 0.5)\n",
    "    pts: Sequence[Tuple[float, float, float]] = \\\n",
    "        [(x, y, z) for x in x_pts for y in y_pts for z in z_pts] #Create random points as data\n",
    "\n",
    "    def superv_func(pt): #This is what fitted function is\n",
    "        return alpha + np.dot(beta, pt)\n",
    "\n",
    "    n = norm(loc=0., scale=2.)\n",
    "    \n",
    "    xy_vals_seq: Sequence[Tuple[Tuple[float, float, float], float]] = \\\n",
    "        [(x, superv_func(x) + n.rvs(size=1)[0]) for x in pts]\n",
    "\n",
    "    ag = AdamGradient(\n",
    "        learning_rate=0.5,\n",
    "        decay1=0.9,\n",
    "        decay2=0.999\n",
    "    )\n",
    "    ffs = [\n",
    "        lambda _: 1.,\n",
    "        lambda x: x[0],\n",
    "        lambda x: x[1],\n",
    "        lambda x: x[2]\n",
    "    ]\n",
    "\n",
    "    lfa = LinearFunctionApprox.create(\n",
    "         feature_functions=ffs,\n",
    "         adam_gradient=ag,\n",
    "         regularization_coeff=0.001,\n",
    "         direct_solve=True\n",
    "    )\n",
    "\n",
    "    lfa_ds = lfa.solve(xy_vals_seq)\n",
    "    print(\"Direct Solve\")\n",
    "    pprint(lfa_ds.weights)\n",
    "    errors: np.ndarray = lfa_ds.evaluate(pts) - \\\n",
    "        np.array([y for _, y in xy_vals_seq])\n",
    "    print(\"Mean Squared Error\")\n",
    "    pprint(np.mean(errors * errors))\n",
    "    print()\n",
    "\n",
    "    print(\"Linear Gradient Solve\")\n",
    "    for _ in range(100):\n",
    "        print(\"Weights\")\n",
    "        pprint(lfa.weights)\n",
    "        errors: np.ndarray = lfa.evaluate(pts) - \\\n",
    "            np.array([y for _, y in xy_vals_seq])\n",
    "        print(\"Mean Squared Error\")\n",
    "        pprint(np.mean(errors * errors))\n",
    "        lfa = lfa.update(xy_vals_seq)\n",
    "        print()\n",
    "\n",
    "    ds = DNNSpec(\n",
    "        neurons=[2],\n",
    "        bias=True,\n",
    "        hidden_activation=lambda x: x,\n",
    "        hidden_activation_deriv=lambda x: np.ones_like(x),\n",
    "        output_activation=lambda x: x,\n",
    "        output_activation_deriv=lambda x: np.ones_like(x)\n",
    "    )\n",
    "\n",
    "    dnna = DNNApprox.create(\n",
    "        feature_functions=ffs,\n",
    "        dnn_spec=ds,\n",
    "        adam_gradient=ag,\n",
    "        regularization_coeff=0.01\n",
    "    )\n",
    "    print(\"DNN Gradient Solve\")\n",
    "    for _ in range(100):\n",
    "        print(\"Weights\")\n",
    "        pprint(dnna.weights)\n",
    "        errors: np.ndarray = dnna.evaluate(pts) - \\\n",
    "            np.array([y for _, y in xy_vals_seq])\n",
    "        print(\"Mean Squared Error\")\n",
    "        pprint(np.mean(errors * errors))\n",
    "        dnna = dnna.update(xy_vals_seq)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
