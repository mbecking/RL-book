{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container{width:90%!important;}"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Import Modules:\n",
    "import sys\n",
    "sys.path.append(\"../../RL-book\")\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict\n",
    "from typing import Iterable, Iterator, Tuple, TypeVar\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess, MarkovDecisionProcess\n",
    "from rl.markov_decision_process import FinitePolicy, StateActionMapping\n",
    "from rl.markov_process import FiniteMarkovProcess, FiniteMarkovRewardProcess\n",
    "from rl.distribution import Categorical, Constant\n",
    "from scipy.stats import poisson\n",
    "import numpy as np\n",
    "from more_itertools import distinct_permutations\n",
    "import matplotlib.pyplot as plt\n",
    "from rl.dynamic_programming import value_iteration, policy_iteration, policy_iteration_result\n",
    "import rl.markov_process as mp\n",
    "from matplotlib.pyplot import cm\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container{width:90%!important;}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Vampire Problem for Testing Purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Class:\n",
    "@dataclass(frozen=True)\n",
    "class Village_State:\n",
    "    num_villagers_alive: int\n",
    "    Vampire_is_alive : bool\n",
    "        \n",
    "VampireMapping = StateActionMapping[Village_State, int]\n",
    "\n",
    "class Vampire_MDP_Finite(FiniteMarkovDecisionProcess[Village_State, int]):\n",
    "    def __init__(self,num_villagers_alive:int):\n",
    "        self.num_villagers_alive = num_villagers_alive\n",
    "        super().__init__(self.get_action_transition_reward_map())\n",
    "    \n",
    "    def get_action_transition_reward_map(self) -> VampireMapping:\n",
    "        d: Dict[Village_State, Dict[str, Categorical[Tuple[Village_State,float]]]] = {}\n",
    "\n",
    "        for villagers in range(1,self.num_villagers_alive+1):\n",
    "            START_STATE = Village_State(num_villagers_alive = villagers,Vampire_is_alive=True)\n",
    "            action_dict = {}\n",
    "            \n",
    "            for poison_amount in range(villagers+1):\n",
    "                action_str = \"Poison \" + str(poison_amount) + \" villigers\"\n",
    "                END_STATE_Murdered = Village_State(num_villagers_alive = villagers-poison_amount,Vampire_is_alive=False)\n",
    "                END_STATE_Survived = Village_State(num_villagers_alive = villagers-poison_amount-1,Vampire_is_alive=True)\n",
    "                \n",
    "                prob_dict = {}\n",
    "                \n",
    "                vampire_murdered_prob = poison_amount/villagers\n",
    "                vampire_survive_prob = 1 - poison_amount/villagers\n",
    "                \n",
    "                Reward_vampire_survives = 0\n",
    "                Reward_vampire_is_murdered = villagers - poison_amount\n",
    "                \n",
    "                prob_dict[(END_STATE_Murdered,Reward_vampire_is_murdered)] = vampire_murdered_prob\n",
    "                prob_dict[(END_STATE_Survived,Reward_vampire_survives)] = vampire_survive_prob\n",
    "                \n",
    "                action_dict[action_str] =Categorical(prob_dict)\n",
    "                \n",
    "                d[END_STATE_Murdered] = None\n",
    "            d[Village_State(num_villagers_alive = 0,Vampire_is_alive=True)]=None\n",
    "            d[Village_State(num_villagers_alive = 0,Vampire_is_alive=False)]=None\n",
    "            d[START_STATE] = action_dict\n",
    "        return d\n",
    "    \n",
    "Vampire_MDP = Vampire_MDP_Finite(num_villagers_alive=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a deterministic policy, starting distribution, and starting distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_policy_and_start_distribution(num_villagers):\n",
    "    policy = {}\n",
    "    start_distribution = {}\n",
    "    approx_0 = {}\n",
    "\n",
    "    for villagers in range(1,num_villagers+1):\n",
    "        START_STATE = Village_State(num_villagers_alive = villagers,Vampire_is_alive=True)\n",
    "        approx_0[START_STATE] = 0\n",
    "        action_dict = {}\n",
    "        start_distribution[START_STATE] = 1/num_villagers\n",
    "\n",
    "        for poison_amount in range(villagers+1):\n",
    "            action_str = \"Poison \" + str(poison_amount) + \" villigers\"\n",
    "\n",
    "            if poison_amount ==1:\n",
    "                action_dict[action_str] = 1\n",
    "\n",
    "            END_STATE_Murdered = Village_State(num_villagers_alive = villagers-poison_amount,Vampire_is_alive=False)\n",
    "            policy[END_STATE_Murdered] = None\n",
    "            approx_0[END_STATE_Murdered] = 0\n",
    "\n",
    "        policy[START_STATE] = Categorical(action_dict)\n",
    "    policy[Village_State(num_villagers_alive = 0,Vampire_is_alive=True)]=None\n",
    "    policy[Village_State(num_villagers_alive = 0,Vampire_is_alive=False)]=None\n",
    "    \n",
    "    approx_0[Village_State(num_villagers_alive = 0,Vampire_is_alive=True)]=0\n",
    "    approx_0[Village_State(num_villagers_alive = 0,Vampire_is_alive=False)]=0\n",
    "    \n",
    "    policy = FinitePolicy(policy)\n",
    "    start_distribution = Categorical(start_distribution)\n",
    "    return policy,start_distribution,approx_0\n",
    "\n",
    "policy, start_distribution, approx_0 = generate_policy_and_start_distribution(10)\n",
    "num_visits= approx_0.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate traces for Prediction algorithm to use to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vampire_MRP = Vampire_MDP.apply_finite_policy(policy)\n",
    "\n",
    "traces = []\n",
    "for i, trace in enumerate(Vampire_MRP.reward_traces(start_distribution)):\n",
    "\n",
    "    if i >= 10000:\n",
    "        break\n",
    "    \n",
    "    temp_list=[]\n",
    "    for j,x in enumerate(trace):\n",
    "        temp_list.append(x)\n",
    "    \n",
    "    traces.append(temp_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: n-Step Bootstrapping\n",
    "\n",
    "Implement the n-Step Bootstrapping Prediction algorithm from scratch in Python code. First do\n",
    "it for the Tabular case, then do it for the case of Function Approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function & Number of Rewards per Non-Terminal States\n",
      "\n",
      "Village_State(num_villagers_alive=1, Vampire_is_alive=True) 0.0  Number of Visits to State: 30691\n",
      "Village_State(num_villagers_alive=2, Vampire_is_alive=True) 0.49745002637904034  Number of Visits to State: 34118\n",
      "Village_State(num_villagers_alive=3, Vampire_is_alive=True) 0.666924564796903  Number of Visits to State: 31020\n",
      "Village_State(num_villagers_alive=4, Vampire_is_alive=True) 1.1248873838889135  Number of Visits to State: 32189\n",
      "Village_State(num_villagers_alive=5, Vampire_is_alive=True) 1.3321889066910755  Number of Visits to State: 26214\n",
      "Village_State(num_villagers_alive=6, Vampire_is_alive=True) 1.7739337568058064  Number of Visits to State: 26448\n",
      "Village_State(num_villagers_alive=7, Vampire_is_alive=True) 2.0018003706645304  Number of Visits to State: 18885\n",
      "Village_State(num_villagers_alive=8, Vampire_is_alive=True) 2.426143240540976  Number of Visits to State: 19003\n",
      "Village_State(num_villagers_alive=9, Vampire_is_alive=True) 2.6390116512655726  Number of Visits to State: 9956\n",
      "Village_State(num_villagers_alive=10, Vampire_is_alive=True) 3.065265801815974  Number of Visits to State: 9883\n",
      "Village_State(num_villagers_alive=0, Vampire_is_alive=True) 0  Number of Visits to State: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "S = TypeVar('S')\n",
    "A = TypeVar('A')\n",
    "value_function = TypeVar('V')\n",
    "\n",
    "policy,start_distribution,approx_0 = generate_policy_and_start_distribution(10)\n",
    "num_visits= approx_0.copy()\n",
    "\n",
    "#Create Function:\n",
    "def n_step_bootstrapping_prediction_tabular(\n",
    "    traces: Iterable[Iterable[mp.TransitionStep[S]]],\n",
    "    approx_0: Dict[S,value_function],\n",
    "    gamma: float,\n",
    "    n_step : int,\n",
    "    num_visits: Dict[S,int]) -> Dict[S,value_function]:\n",
    "    \n",
    "    \"\"\"The main difference between this prediction and the one from the code base is that we are\n",
    "        now representing the approximate value function as a dictionary. Instead of updating the function using the\n",
    "        iterate updates method we will now write code to do that. The function will return a new approx_0\n",
    "        that should be a better value function.\"\"\"    \n",
    "    bootstrapping_reward = num_visits.copy()\n",
    "    \n",
    "    \n",
    "    total_accumulated_reward = {}\n",
    "    \n",
    "    #Loop through all of the individual trial runs\n",
    "    for trace in traces:\n",
    "        COLOR = iter(cm.rainbow(np.linspace(0,1,len(approx_0))))\n",
    "        \n",
    "        #Loop through all of the individual state transitions\n",
    "        for n,transition in enumerate(trace): #Transition has S.A.R.S. values\n",
    "            \n",
    "            #Since we are in some state rn update that state with the visit number\n",
    "            num_visits[transition.state] = num_visits[transition.state] + 1\n",
    "            \n",
    "            \n",
    "            gamma_power = 0\n",
    "            reward_component = 0\n",
    "\n",
    "            for i,transition_2 in enumerate(trace): #(All of the remaining steps and their rewards)\n",
    "                if i >= n and i <= n + n_step:\n",
    "                    \n",
    "\n",
    "                    reward_component += gamma**gamma_power * transition_2.reward\n",
    "                    \n",
    "                    if i == n+n_step or i == len(trace)-1:\n",
    "                        \n",
    "                        value_component = gamma**gamma_power * approx_0[transition_2.next_state]\n",
    "                        bootstrapped_return = reward_component + value_component\n",
    "                        \n",
    "                        bootstrapping_reward[transition.state] += bootstrapped_return\n",
    "                        \n",
    "                    gamma_power += 1\n",
    "                    \n",
    "            approx_0[transition.state] +=  1/num_visits[transition.state] * (bootstrapped_return - approx_0[transition.state])\n",
    "                    \n",
    "        for x in approx_0:\n",
    "            color = next(COLOR)\n",
    "            if x.Vampire_is_alive:\n",
    "                plt.plot(num_visits[x],approx_0[x],marker='.',color=color)\n",
    "                \n",
    "    #print(bootstrapping_reward)\n",
    "    return approx_0,num_visits\n",
    "\n",
    "approx_0,num_visits = n_step_bootstrapping_prediction_tabular(traces=traces,approx_0=approx_0,gamma=1,n_step=3,num_visits=num_visits)\n",
    "\n",
    "print (\"Value Function & Number of Rewards per Non-Terminal States\")\n",
    "print ()\n",
    "for x in approx_0:\n",
    "    if x.Vampire_is_alive:\n",
    "        print (x,approx_0[x],\" Number of Visits to State:\",num_visits[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Village_State(num_villagers_alive=1, Vampire_is_alive=False) is a Terminal State\n",
       "Village_State(num_villagers_alive=0, Vampire_is_alive=False) is a Terminal State\n",
       "Village_State(num_villagers_alive=0, Vampire_is_alive=True) is a Terminal State\n",
       "For State Village_State(num_villagers_alive=1, Vampire_is_alive=True):\n",
       "  Do Action Poison 0 villigers with Probability 1.000\n",
       "Village_State(num_villagers_alive=2, Vampire_is_alive=False) is a Terminal State\n",
       "For State Village_State(num_villagers_alive=2, Vampire_is_alive=True):\n",
       "  Do Action Poison 2 villigers with Probability 1.000\n",
       "Village_State(num_villagers_alive=3, Vampire_is_alive=False) is a Terminal State\n",
       "For State Village_State(num_villagers_alive=3, Vampire_is_alive=True):\n",
       "  Do Action Poison 1 villigers with Probability 1.000\n",
       "Village_State(num_villagers_alive=4, Vampire_is_alive=False) is a Terminal State\n",
       "For State Village_State(num_villagers_alive=4, Vampire_is_alive=True):\n",
       "  Do Action Poison 3 villigers with Probability 1.000\n",
       "Village_State(num_villagers_alive=5, Vampire_is_alive=False) is a Terminal State\n",
       "For State Village_State(num_villagers_alive=5, Vampire_is_alive=True):\n",
       "  Do Action Poison 2 villigers with Probability 1.000\n",
       "Village_State(num_villagers_alive=6, Vampire_is_alive=False) is a Terminal State\n",
       "For State Village_State(num_villagers_alive=6, Vampire_is_alive=True):\n",
       "  Do Action Poison 0 villigers with Probability 1.000\n",
       "Village_State(num_villagers_alive=7, Vampire_is_alive=False) is a Terminal State\n",
       "For State Village_State(num_villagers_alive=7, Vampire_is_alive=True):\n",
       "  Do Action Poison 4 villigers with Probability 1.000\n",
       "Village_State(num_villagers_alive=8, Vampire_is_alive=False) is a Terminal State\n",
       "For State Village_State(num_villagers_alive=8, Vampire_is_alive=True):\n",
       "  Do Action Poison 0 villigers with Probability 1.000\n",
       "Village_State(num_villagers_alive=9, Vampire_is_alive=False) is a Terminal State\n",
       "For State Village_State(num_villagers_alive=9, Vampire_is_alive=True):\n",
       "  Do Action Poison 6 villigers with Probability 1.000\n",
       "Village_State(num_villagers_alive=10, Vampire_is_alive=False) is a Terminal State\n",
       "For State Village_State(num_villagers_alive=10, Vampire_is_alive=True):\n",
       "  Do Action Poison 2 villigers with Probability 1.000"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def random_policy(FiniteMDP):\n",
    "    policy_dict = {}\n",
    "    for state in FiniteMDP.states():\n",
    "        action_dict = {}\n",
    "        \n",
    "        actions = list(FiniteMDP.actions(state))\n",
    "        n = len(actions)\n",
    "        if n > 0:\n",
    "            uniform_actions = Categorical({action:1/n for action in actions})\n",
    "\n",
    "            random_action = uniform_actions.sample()\n",
    "\n",
    "        \n",
    "            policy_dict[state] = Constant(random_action)\n",
    "        else:\n",
    "            policy_dict[state] = None\n",
    "    return FinitePolicy(policy_dict)\n",
    "\n",
    "random_policy(Vampire_MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: TD$(\\lambda)$\n",
    "\n",
    "Implement the TD(位) Prediction algorithm from scratch in Python code. First do it for the Tabular case, then do it for the case of Function Approximation.\n",
    "\n",
    "I want to first go through the rough idea of TD($\\lambda$) since it is not as straight forward as the rest of the prediction algorithms. First of all there are actually 2 methods for doing this. The first one is the forward method which is effctively averging over all of the different n's in the n - bootstrapping method. It is not an exact average. we use a geometric decay for how the differet components calculate the lambda return. However to update all of the values we need to get to the end of the episode. Thus this approach is not used in real life. **Introducing Backwards TD($lambda$):** This method accomplishes a similiar goal as the forward method, but we can continuously update it. For this we introduce the elgibility trace. For a tabular method we introduce this as a dictionary. for each state we have this eligibility value. As we run through our traces every time we visit a state we increase the eligibility value by 1. Separately, everytime we change from state we multiply the eligbibility value of all of the states times the lambda value. thus the eligibility value decays over time:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can represent the eligibility trace as a function that starts at 0 and increases with time.:\n",
    "\n",
    "\\begin{equation}\n",
    "E_t(s) = \\gamma \\lambda E_{t-1}(s) + 1(S_t = s)\n",
    "\\end{equation}\n",
    "\n",
    "We can then update "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function & Number of Rewards per Non-Terminal States\n",
      "\n",
      "Village_State(num_villagers_alive=1, Vampire_is_alive=True) 0.0  Number of Visits to State: 3108\n",
      "Village_State(num_villagers_alive=2, Vampire_is_alive=True) 0.49794238683127556  Number of Visits to State: 3402\n",
      "Village_State(num_villagers_alive=3, Vampire_is_alive=True) 0.6664546899841004  Number of Visits to State: 3145\n",
      "Village_State(num_villagers_alive=4, Vampire_is_alive=True) 1.1202056188832106  Number of Visits to State: 3171\n",
      "Village_State(num_villagers_alive=5, Vampire_is_alive=True) 1.2738994235059986  Number of Visits to State: 2571\n",
      "Village_State(num_villagers_alive=6, Vampire_is_alive=True) 1.8091604662471554  Number of Visits to State: 2598\n",
      "Village_State(num_villagers_alive=7, Vampire_is_alive=True) 1.9720800568938877  Number of Visits to State: 1850\n",
      "Village_State(num_villagers_alive=8, Vampire_is_alive=True) 2.448498085840534  Number of Visits to State: 1888\n",
      "Village_State(num_villagers_alive=9, Vampire_is_alive=True) 2.5404497592955737  Number of Visits to State: 959\n",
      "Village_State(num_villagers_alive=10, Vampire_is_alive=True) 3.1763869086901737  Number of Visits to State: 998\n",
      "Village_State(num_villagers_alive=0, Vampire_is_alive=True) 0.0  Number of Visits to State: 0\n"
     ]
    }
   ],
   "source": [
    "S = TypeVar('S')\n",
    "A = TypeVar('A')\n",
    "value_function = TypeVar('V')\n",
    "\n",
    "policy,start_distribution,approx_0 = generate_policy_and_start_distribution(10)\n",
    "num_visits= approx_0.copy()\n",
    "\n",
    "#Create Function:\n",
    "def TD_Lambda_prediction_tabular(\n",
    "    traces: Iterable[Iterable[mp.TransitionStep[S]]],\n",
    "    approx_0: Dict[S,value_function],\n",
    "    lambda_value : float,\n",
    "    gamma: float,\n",
    "    num_visits: Dict[S,int]) -> Dict[S,value_function]:\n",
    "    \n",
    "    \"\"\"The main difference between this prediction and the one from the code base is that we are\n",
    "        now representing the approximate value function as a dictionary. Instead of updating the function using the\n",
    "        iterate updates method we will now write code to do that. The function will return a new approx_0\n",
    "        that should be a better value function.\"\"\"  \n",
    "    \n",
    "    eligibility_trace = num_visits.copy()\n",
    "    \n",
    "    \n",
    "    total_accumulated_reward = {}\n",
    "    \n",
    "    #Loop through all of the individual trial runs\n",
    "    for trace in traces:\n",
    "        COLOR = iter(cm.rainbow(np.linspace(0,1,len(approx_0))))\n",
    "        \n",
    "        for key in eligibility_trace.keys():\n",
    "            eligibility_trace[key] = 0\n",
    "        \n",
    "        #Loop through all of the individual state transitions\n",
    "        for n,transition in enumerate(trace):\n",
    "        \n",
    "            #Update eligibility trace by 1 for state we visited    \n",
    "            eligibility_trace[transition.state] = eligibility_trace[transition.state] + 1\n",
    "                \n",
    "            #Since we are in some state rn update that state with the visit number\n",
    "            num_visits[transition.state] = num_visits[transition.state] + 1\n",
    "            \n",
    "            #TD error value (delta)\n",
    "            TD_error =  (transition.reward +gamma*approx_0[transition.next_state] - approx_0[transition.state])\n",
    "            \n",
    "            alpha =  1/(num_visits[transition.state])\n",
    "            \n",
    "            #approx_0[transition.state] += alpha * TD_error * eligibility_trace[transition.state]\n",
    "            \n",
    "            for key in approx_0.keys():\n",
    "                approx_0[key] += alpha * TD_error * eligibility_trace[key]\n",
    "                eligibility_trace[key] = eligibility_trace[key] * lambda_value * gamma\n",
    "\n",
    "\n",
    "                    \n",
    "#         for x in approx_0:\n",
    "#             color = next(COLOR)\n",
    "#             if x.Vampire_is_alive:\n",
    "#                 plt.plot(num_visits[x],approx_0[x],marker='.',color=color)\n",
    "                \n",
    "    return approx_0,num_visits\n",
    "\n",
    "approx_0,num_visits = TD_Lambda_prediction_tabular(traces=traces,approx_0=approx_0,lambda_value=.9,gamma=1,num_visits=num_visits)\n",
    "\n",
    "print (\"Value Function & Number of Rewards per Non-Terminal States\")\n",
    "print ()\n",
    "for x in approx_0:\n",
    "    if x.Vampire_is_alive:\n",
    "        print (x,approx_0[x],\" Number of Visits to State:\",num_visits[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: MC & TD equivalence proof\n",
    "\n",
    "Prove that the MC Error can be written as the sum of discounted TD errors\n",
    "\n",
    "\\begin{equation}\n",
    "G_t - V(S_t) = \\sum_{u=t}^{T-1} \\gamma^{u-t} \\cdot (R_{u+1} +\\gamma \\cdot V(S_{u+1}) - V(S_u))\n",
    "\\end{equation}\n",
    "\n",
    "First, let's define some terms:\n",
    "\n",
    "Return: $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + ...$\n",
    "\n",
    "Value Function: (Expected Return): $V(S_T) = $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Testing out algorithm performance\n",
    "\n",
    "Test your above implementation of TD(位) Prediction algorithm by comparing the Value Function\n",
    "of an MRP you have previously developed (or worked with) as obtained by Policy Evaluation (DP)\n",
    "algorithm, as obtained by MC, as obtained by TD, and as obtained by your TD(位) implementation.\n",
    "Plot graphs of convergence for different values of 位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
