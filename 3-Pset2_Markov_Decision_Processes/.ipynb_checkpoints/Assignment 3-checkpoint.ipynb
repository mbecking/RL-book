{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../RL-book\")\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "from rl.markov_decision_process import FinitePolicy, StateActionMapping\n",
    "from rl.markov_process import FiniteMarkovProcess, FiniteMarkovRewardProcess\n",
    "from rl.distribution import Categorical, Constant\n",
    "from scipy.stats import poisson\n",
    "import numpy as np\n",
    "from more_itertools import distinct_permutations\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1:\n",
    "\n",
    "For a Deterministic Policy $\\pi_D$ ∶ $\\mathcal{S} \\to \\mathcal{A}$, write with precise mathematical notation the 4 MDP\n",
    "Bellman Policy Equations, i.e., $V^{\\pi_D}$ in terms of $Q^{\\pi_D}$, $Q^{\\pi_D}$ in terms of $V^{\\pi_D}$ , $V^{\\pi_D}$ in terms of $V^{\\pi_D}$, $Q^{\\pi_D}$ in terms of $Q^{\\pi_D}$. Note that in the book, we have written the 4 MDP Policy Equations in\n",
    "terms of the notation for a stochastic policy π ∶ $\\mathcal{S}$ × $\\mathcal{A}$ → [0, 1].\n",
    "\n",
    "**Answer:** Okay, so first off we have a deterministic policy. This means that for every state there is only one action and thus only one reward. Now we can modify the equations in the book by summing over the policies (they are all 1) and the actions (also 1). What is kinda weird about this question is that once you have a determanistic policy, there really is not need for an action value function since you can only take one action and that action leads to the next state. What I don't get is that the action value function tells you the benifit of taking a specific action while in a specific state and the state value function tells you the benifit of being in that state. SO does that mean that the action value function is the benifit of going to a new state?\n",
    "\n",
    "We know in a stocastic policy we have:\n",
    "\n",
    "\\begin{equation}\n",
    "V^{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(s,a) \\cdot Q^{\\pi}(s,a)\n",
    "\\end{equation}\n",
    "\n",
    "However in a determanistic policy we know there is one action per state so this just becomes:\n",
    "\n",
    "\\begin{equation}\n",
    "V^{\\pi}(s) =  Q^{\\pi}(s,a)\n",
    "\\end{equation}\n",
    "\n",
    "Additionally we also know:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{P}^{\\pi}(s,s') = \\mathcal{P}^{\\pi}(s,a,s')\n",
    "\\end{equation}\n",
    "\n",
    "So I guess they are just interchangable now so that's the answer...\n",
    "\n",
    "**State-Value Function:**\n",
    "\n",
    "\\begin{equation}\n",
    "V^{\\pi}(s) = \\mathcal{R}^{\\pi}(s) + \\gamma \\cdot \\sum_{s' \\in \\mathcal{N}} \\mathcal{P}^{\\pi}(s,s') \\cdot V^{\\pi}(s')\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "V^{\\pi}(s) = \\mathcal{R}^{\\pi}(s) + \\gamma \\cdot \\sum_{s' \\in \\mathcal{N}} \\mathcal{P}^{\\pi}(s,s') \\cdot Q^{\\pi}(s',a')\n",
    "\\end{equation}\n",
    "\n",
    "**Action-Value Function:**\n",
    "\n",
    "\\begin{equation}\n",
    "Q^{\\pi}(s,a) = \\mathcal{R}^{\\pi}(s) + \\gamma \\cdot \\sum_{s' \\in \\mathcal{N}} \\mathcal{P}^{\\pi}(s,s') \\cdot Q^{\\pi}(s',a')\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "Q^{\\pi}(s,a) = \\mathcal{R}^{\\pi}(s) + \\gamma \\cdot \\sum_{s' \\in \\mathcal{N}} \\mathcal{P}^{\\pi}(s,s') \\cdot V^{\\pi}(s')\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2:\n",
    "\n",
    "For this problem we are going to be looking at a MPD with infinite states. We always start in state s=1 and each state has a continuous set of actions a $\\in$ [0,1]. The probability of taking an action and moving to the next state is equal to the action value and the probability of taking an action and staying in the same state is 1 - the action value. The reward for transitioning to the next state is 1-a and the reward for staying in the same state is 1 + a.\n",
    "\n",
    "Alright so breaking this down if you have a low \"a\" you are not likely to move states, but your reward is 1 + a so you only really get a reward of 1.\n",
    "\n",
    "If you have a large \"a\" then you are very likely to transition to a new state, but you reward is now pretty small. \n",
    "\n",
    "Based on this information I would guess that the optimal action would be an a of 0 as you are guarenteed a reward of 1 each time, however if the probability is .1 then 90% of the time you get a reward of 1.1 and 10% of the time you get a reward of .9. thus if we summed this over 10 with perfect probabilities we would get a total of 1.1*9 + .9 = 10.8 which is larger than 10. Thus the answer is somewhere in the lower a's. Now let's find it:\n",
    "\n",
    "Let's start with the **Bellman Optimality Equation:**\n",
    "\n",
    "\\begin{equation}\n",
    "v_*(s) = max(\\mathcal{R}_s^a) + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^{a} v_*(s')\n",
    "\\end{equation}\n",
    "\n",
    "Actually, let's just logic our way through this. For every single step there are infinite actions, but there are finite environmental situations. So we should be able to observe all of the states by writing the expectation value for the Reward and then maximizing it.\n",
    "\n",
    "So we care about $R$\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle R\\rangle = \\mathcal{P}_{s \\to s} R_{s \\to s} + \\mathcal{P}_{s \\to s'} R_{s \\to s'}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{P}_{s \\to s} = 1 -a\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{P}_{s \\to s'} = a\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "R_{s \\to s} = 1 - a\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "R_{s \\to s'} = 1 + a\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle R\\rangle = (1-a)(a) + (1+a)(1-a)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle R\\rangle = -2a^2 + a + 1\n",
    "\\end{equation}\n",
    "\n",
    "We now have an expectation value for the reward so we can use calculus to find the maximum\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{d \\langle R\\rangle}{da} = -4a + 1 = 0 \n",
    "\\end{equation}\n",
    "\n",
    "a = 1/4\n",
    "\n",
    "Thus the optimal policy is a = .25 for all states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3:\n",
    "\n",
    "**State Space:**\n",
    "\n",
    "Let n+1 = the number of lilypads on a pond, numbered 0 to n. \n",
    "\n",
    "$\\mathcal{S} = \\{0,...,n\\}$\n",
    "\n",
    "$\\mathcal{S}_0 = \\{1,...,n-1\\}$\n",
    "\n",
    "$\\mathcal{T} = \\{0,n\\} $\n",
    "\n",
    "$\\mathcal{N} = \\{1,n-1\\}$\n",
    "\n",
    "$\\mathcal{A} = \\{A,B\\}$\n",
    "\n",
    "$\\mathcal{T} = \\{A,B\\}$\n",
    "\n",
    "$\\mathcal{P}(s,s') = \\{A,B\\}$\n",
    "\n",
    "1. Create a **State** class to define the state of the Frog. \n",
    "2. Create a **Finite Markov Decision** class used to define the action transition reward map\n",
    "3. Create a **Finite Policy** class to tell us the probabilities of when to take certain actions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Frog_State class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Frog_State:\n",
    "    '''This class is the lilypad that the frog is on'''\n",
    "    \n",
    "    position: int\n",
    "        \n",
    "FrogMapping = StateActionMapping[Frog_State, int]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design Action Transition Reward Map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Frog_State(position=1): {0: {(Frog_State(position=2), 1): 0.8,\n",
       "   (Frog_State(position=2), -1): 0.2},\n",
       "  1: {(Frog_State(position=0), -1): 0.2,\n",
       "   (Frog_State(position=2), 1): 0.2,\n",
       "   (Frog_State(position=3), 2): 0.2,\n",
       "   (Frog_State(position=4), 3): 0.2,\n",
       "   (Frog_State(position=5), 4): 0.2}},\n",
       " Frog_State(position=2): {0: {(Frog_State(position=3), 1): 0.6,\n",
       "   (Frog_State(position=3), -1): 0.4},\n",
       "  1: {(Frog_State(position=0), -2): 0.2,\n",
       "   (Frog_State(position=1), -1): 0.2,\n",
       "   (Frog_State(position=3), 1): 0.2,\n",
       "   (Frog_State(position=4), 2): 0.2,\n",
       "   (Frog_State(position=5), 3): 0.2}},\n",
       " Frog_State(position=3): {0: {(Frog_State(position=4), 1): 0.4,\n",
       "   (Frog_State(position=4), -1): 0.6},\n",
       "  1: {(Frog_State(position=0), -3): 0.2,\n",
       "   (Frog_State(position=1), -2): 0.2,\n",
       "   (Frog_State(position=2), -1): 0.2,\n",
       "   (Frog_State(position=4), 1): 0.2,\n",
       "   (Frog_State(position=5), 2): 0.2}},\n",
       " Frog_State(position=4): {0: {(Frog_State(position=5), 1): 0.2,\n",
       "   (Frog_State(position=5), -1): 0.8},\n",
       "  1: {(Frog_State(position=0), -4): 0.2,\n",
       "   (Frog_State(position=1), -3): 0.2,\n",
       "   (Frog_State(position=2), -2): 0.2,\n",
       "   (Frog_State(position=3), -1): 0.2,\n",
       "   (Frog_State(position=5), 1): 0.2}},\n",
       " Frog_State(position=0): None,\n",
       " Frog_State(position=5): None}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_action_transition_reward_map() -> FrogMapping:\n",
    "    d: Dict[Frog_State, Dict[int, Categorical[Tuple[Frog_State,float]]]] = {}\n",
    "    #{Start_State:{Action:(Next_State:Reward)}} = Probability\n",
    "    #{State:{Action 1:{(Next_State,Reward):Probability},{Action 2:{(Next_State,Reward):Probability}}}}\n",
    "    \n",
    "    NUM_LILIY_PADS = 5\n",
    "    #For each of the Lily Pads that are non terminal...\n",
    "    for i in range(1,NUM_LILIY_PADS):\n",
    "        action_dict = {}\n",
    "        for action in range(0,2):\n",
    "\n",
    "            if action ==0:                                              #A Croak\n",
    "                prob_dict = {}\n",
    "                upstep_prob = (NUM_LILIY_PADS-i)/NUM_LILIY_PADS\n",
    "                downstep_prob = i/NUM_LILIY_PADS\n",
    "                \n",
    "                up_reward = +1\n",
    "                down_reward = -1\n",
    "                \n",
    "                #Once an action is taken declare possible movements with probabilities.\n",
    "                prob_dict[(Frog_State(position=i+1),up_reward)] = upstep_prob\n",
    "                prob_dict[(Frog_State(position=i+1),down_reward)] = downstep_prob\n",
    "                \n",
    "                #Pair all outcomes to one action:\n",
    "                action_dict[action] =prob_dict\n",
    "                \n",
    "                \n",
    "                \n",
    "            elif action ==1:                                            #B Croak\n",
    "                prob_dict = {}\n",
    "                move_prob = 1/NUM_LILIY_PADS\n",
    "                for j in range(0,NUM_LILIY_PADS+1):\n",
    "                    if i != j:\n",
    "                        reward = j-i \n",
    "                        \n",
    "                        #Once an action is taken loop through all the possible movements is can now undergo\n",
    "                        prob_dict[(Frog_State(position=j),reward)] = move_prob\n",
    "                \n",
    "                #link movements to action\n",
    "                action_dict[action] =prob_dict\n",
    "                \n",
    "        #Pair all actions to an initial state\n",
    "        d[Frog_State(position=i)] = action_dict\n",
    "    \n",
    "    #Terminal States\n",
    "    d[Frog_State(position=0)] = None\n",
    "    d[Frog_State(position=NUM_LILIY_PADS)] = None\n",
    "    \n",
    "    return d\n",
    "        \n",
    "get_action_transition_reward_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Markov Decision Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Frog_MDP_Finite(FiniteMarkovDecisionProcess[Frog_State, int]):\n",
    "    def __init__(self,num_lily_pads:int):\n",
    "        self.num_lily_pads = num_lily_pads\n",
    "        super().__init__(self.get_action_transition_reward_map())\n",
    "    \n",
    "    def get_action_transition_reward_map(self) -> FrogMapping:\n",
    "        d: Dict[InventoryState, Dict[int, Categorical[Tuple[InventoryState,float]]]] = {}\n",
    "        #{State:{Action 1:{(Next_State,Reward):Probability},{Action 2:{(Next_State,Reward):Probability}}}}\n",
    "\n",
    "        NUM_LILIY_PADS = self.num_lily_pads\n",
    "        #For each of the Lily Pads that are non terminal...\n",
    "        for i in range(1,NUM_LILIY_PADS):\n",
    "            action_dict = {}\n",
    "            for action in range(0,2):\n",
    "\n",
    "                if action ==0:                                              #A Croak\n",
    "                    prob_dict = {}\n",
    "                    upstep_prob = (NUM_LILIY_PADS-i)/NUM_LILIY_PADS\n",
    "                    downstep_prob = i/NUM_LILIY_PADS\n",
    "                    \n",
    "                    #Choose Reward\n",
    "                    if i + 1 == NUM_LILIY_PADS:\n",
    "                        up_reward = 1\n",
    "                        down_reward =0\n",
    "                    else:\n",
    "                        up_reward = 0\n",
    "                        down_reward = 0\n",
    "\n",
    "                    #Once an action is taken declare possible movements with probabilities.\n",
    "                    prob_dict[(Frog_State(position=i+1),up_reward)] = upstep_prob\n",
    "                    prob_dict[(Frog_State(position=i-1),down_reward)] = downstep_prob\n",
    "\n",
    "                    #Pair all outcomes to one action:\n",
    "                    action_dict[action] = Categorical(prob_dict)\n",
    "\n",
    "\n",
    "\n",
    "                elif action ==1:                                            #B Croak\n",
    "                    prob_dict = {}\n",
    "                    move_prob = 1/NUM_LILIY_PADS\n",
    "                    for j in range(0,NUM_LILIY_PADS+1):\n",
    "                        if i != j:\n",
    "                            if j == NUM_LILIY_PADS:\n",
    "                                reward = 1\n",
    "                            else:\n",
    "                                reward = 0\n",
    "\n",
    "                            #Once an action is taken loop through all the possible movements is can now undergo\n",
    "                            prob_dict[(Frog_State(position=j),reward)] = move_prob\n",
    "\n",
    "                    #link movements to action\n",
    "                    action_dict[action] =Categorical(prob_dict)\n",
    "\n",
    "            #Pair all actions to an initial state\n",
    "            d[Frog_State(position=i)] = action_dict\n",
    "\n",
    "        #Terminal States\n",
    "        d[Frog_State(position=0)] = None\n",
    "        d[Frog_State(position=NUM_LILIY_PADS)] = None\n",
    "\n",
    "        return d\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP Transition Map\n",
      "------------------\n",
      "From State Frog_State(position=1):\n",
      "  With Action 0:\n",
      "    To [State Frog_State(position=2) and Reward 0.000] with Probability 0.900\n",
      "    To [State Frog_State(position=0) and Reward 0.000] with Probability 0.100\n",
      "  With Action 1:\n",
      "    To [State Frog_State(position=0) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=2) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=3) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=4) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=5) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=6) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=7) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=8) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=9) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=10) and Reward 1.000] with Probability 0.100\n",
      "From State Frog_State(position=2):\n",
      "  With Action 0:\n",
      "    To [State Frog_State(position=3) and Reward 0.000] with Probability 0.800\n",
      "    To [State Frog_State(position=1) and Reward 0.000] with Probability 0.200\n",
      "  With Action 1:\n",
      "    To [State Frog_State(position=0) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=1) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=3) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=4) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=5) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=6) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=7) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=8) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=9) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=10) and Reward 1.000] with Probability 0.100\n",
      "From State Frog_State(position=3):\n",
      "  With Action 0:\n",
      "    To [State Frog_State(position=4) and Reward 0.000] with Probability 0.700\n",
      "    To [State Frog_State(position=2) and Reward 0.000] with Probability 0.300\n",
      "  With Action 1:\n",
      "    To [State Frog_State(position=0) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=1) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=2) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=4) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=5) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=6) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=7) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=8) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=9) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=10) and Reward 1.000] with Probability 0.100\n",
      "From State Frog_State(position=4):\n",
      "  With Action 0:\n",
      "    To [State Frog_State(position=5) and Reward 0.000] with Probability 0.600\n",
      "    To [State Frog_State(position=3) and Reward 0.000] with Probability 0.400\n",
      "  With Action 1:\n",
      "    To [State Frog_State(position=0) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=1) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=2) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=3) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=5) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=6) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=7) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=8) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=9) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=10) and Reward 1.000] with Probability 0.100\n",
      "From State Frog_State(position=5):\n",
      "  With Action 0:\n",
      "    To [State Frog_State(position=6) and Reward 0.000] with Probability 0.500\n",
      "    To [State Frog_State(position=4) and Reward 0.000] with Probability 0.500\n",
      "  With Action 1:\n",
      "    To [State Frog_State(position=0) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=1) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=2) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=3) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=4) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=6) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=7) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=8) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=9) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=10) and Reward 1.000] with Probability 0.100\n",
      "From State Frog_State(position=6):\n",
      "  With Action 0:\n",
      "    To [State Frog_State(position=7) and Reward 0.000] with Probability 0.400\n",
      "    To [State Frog_State(position=5) and Reward 0.000] with Probability 0.600\n",
      "  With Action 1:\n",
      "    To [State Frog_State(position=0) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=1) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=2) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=3) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=4) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=5) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=7) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=8) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=9) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=10) and Reward 1.000] with Probability 0.100\n",
      "From State Frog_State(position=7):\n",
      "  With Action 0:\n",
      "    To [State Frog_State(position=8) and Reward 0.000] with Probability 0.300\n",
      "    To [State Frog_State(position=6) and Reward 0.000] with Probability 0.700\n",
      "  With Action 1:\n",
      "    To [State Frog_State(position=0) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=1) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=2) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=3) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=4) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=5) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=6) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=8) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=9) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=10) and Reward 1.000] with Probability 0.100\n",
      "From State Frog_State(position=8):\n",
      "  With Action 0:\n",
      "    To [State Frog_State(position=9) and Reward 0.000] with Probability 0.200\n",
      "    To [State Frog_State(position=7) and Reward 0.000] with Probability 0.800\n",
      "  With Action 1:\n",
      "    To [State Frog_State(position=0) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=1) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=2) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=3) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=4) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=5) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=6) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=7) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=9) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=10) and Reward 1.000] with Probability 0.100\n",
      "From State Frog_State(position=9):\n",
      "  With Action 0:\n",
      "    To [State Frog_State(position=10) and Reward 1.000] with Probability 0.100\n",
      "    To [State Frog_State(position=8) and Reward 0.000] with Probability 0.900\n",
      "  With Action 1:\n",
      "    To [State Frog_State(position=0) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=1) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=2) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=3) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=4) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=5) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=6) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=7) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=8) and Reward 0.000] with Probability 0.100\n",
      "    To [State Frog_State(position=10) and Reward 1.000] with Probability 0.100\n",
      "Frog_State(position=0) is a Terminal State\n",
      "Frog_State(position=10) is a Terminal State\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Frog_MDP = Frog_MDP_Finite(num_lily_pads=10)\n",
    "\n",
    "print(\"MDP Transition Map\")\n",
    "print(\"------------------\")\n",
    "print(Frog_MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desiging a Policy\n",
    "\n",
    "To figure out what the best policy would be best we are just goint to test them all. Thant means that for n steps we have $2^n$ possible policies if we have 2 actions per policy. Now let's create a function that takes in an array of 0s and 1s that represents what actions to take at what state and returns a FinitePolicy Class:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "For State Frog_State(position=1):\n",
       "  Do Action 0 with Probability 1.000\n",
       "For State Frog_State(position=2):\n",
       "  Do Action 1 with Probability 1.000\n",
       "For State Frog_State(position=3):\n",
       "  Do Action 1 with Probability 1.000\n",
       "For State Frog_State(position=4):\n",
       "  Do Action 0 with Probability 1.000\n",
       "For State Frog_State(position=5):\n",
       "  Do Action 1 with Probability 1.000\n",
       "For State Frog_State(position=6):\n",
       "  Do Action 0 with Probability 1.000\n",
       "For State Frog_State(position=7):\n",
       "  Do Action 0 with Probability 1.000\n",
       "For State Frog_State(position=8):\n",
       "  Do Action 1 with Probability 1.000\n",
       "For State Frog_State(position=9):\n",
       "  Do Action 0 with Probability 1.000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Create_Policy(action_array):\n",
    "    \"\"\" action_array should be a 1 dimensional array with len n-1 values that are either zero or 1. The function then returns\n",
    "    a finite discrete policy all ready to be used.\"\"\"\n",
    "    policy_dict = {}\n",
    "    for state,action in enumerate(action_array):\n",
    "        policy_dict[Frog_State(position=state+1)] = Constant(action) #Constant makes it pair to 1\n",
    "        \n",
    "    return FinitePolicy(policy_dict)\n",
    "\n",
    "Create_Policy([0,1,1,0,1,0,0,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create function to generate all of the possible action arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 1.],\n",
       "       [0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 1., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 1., 0., 0., 0.],\n",
       "       [1., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 1., 1.],\n",
       "       [0., 0., 0., 1., 0., 1., 1.],\n",
       "       [0., 0., 0., 1., 1., 0., 1.],\n",
       "       [0., 0., 0., 1., 1., 1., 0.],\n",
       "       [0., 0., 1., 0., 0., 1., 1.],\n",
       "       [0., 0., 1., 0., 1., 0., 1.],\n",
       "       [0., 0., 1., 0., 1., 1., 0.],\n",
       "       [0., 0., 1., 1., 0., 0., 1.],\n",
       "       [0., 0., 1., 1., 0., 1., 0.],\n",
       "       [0., 0., 1., 1., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 1., 1.],\n",
       "       [0., 1., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 0., 0., 1., 1., 0.],\n",
       "       [0., 1., 0., 1., 0., 0., 1.],\n",
       "       [0., 1., 0., 1., 0., 1., 0.],\n",
       "       [0., 1., 0., 1., 1., 0., 0.],\n",
       "       [0., 1., 1., 0., 0., 0., 1.],\n",
       "       [0., 1., 1., 0., 0., 1., 0.],\n",
       "       [0., 1., 1., 0., 1., 0., 0.],\n",
       "       [0., 1., 1., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 0., 0., 0., 1., 0., 1.],\n",
       "       [1., 0., 0., 0., 1., 1., 0.],\n",
       "       [1., 0., 0., 1., 0., 0., 1.],\n",
       "       [1., 0., 0., 1., 0., 1., 0.],\n",
       "       [1., 0., 0., 1., 1., 0., 0.],\n",
       "       [1., 0., 1., 0., 0., 0., 1.],\n",
       "       [1., 0., 1., 0., 0., 1., 0.],\n",
       "       [1., 0., 1., 0., 1., 0., 0.],\n",
       "       [1., 0., 1., 1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0., 0., 1.],\n",
       "       [1., 1., 0., 0., 0., 1., 0.],\n",
       "       [1., 1., 0., 0., 1., 0., 0.],\n",
       "       [1., 1., 0., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 1., 1., 1.],\n",
       "       [0., 0., 1., 0., 1., 1., 1.],\n",
       "       [0., 0., 1., 1., 0., 1., 1.],\n",
       "       [0., 0., 1., 1., 1., 0., 1.],\n",
       "       [0., 0., 1., 1., 1., 1., 0.],\n",
       "       [0., 1., 0., 0., 1., 1., 1.],\n",
       "       [0., 1., 0., 1., 0., 1., 1.],\n",
       "       [0., 1., 0., 1., 1., 0., 1.],\n",
       "       [0., 1., 0., 1., 1., 1., 0.],\n",
       "       [0., 1., 1., 0., 0., 1., 1.],\n",
       "       [0., 1., 1., 0., 1., 0., 1.],\n",
       "       [0., 1., 1., 0., 1., 1., 0.],\n",
       "       [0., 1., 1., 1., 0., 0., 1.],\n",
       "       [0., 1., 1., 1., 0., 1., 0.],\n",
       "       [0., 1., 1., 1., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 1., 1., 1.],\n",
       "       [1., 0., 0., 1., 0., 1., 1.],\n",
       "       [1., 0., 0., 1., 1., 0., 1.],\n",
       "       [1., 0., 0., 1., 1., 1., 0.],\n",
       "       [1., 0., 1., 0., 0., 1., 1.],\n",
       "       [1., 0., 1., 0., 1., 0., 1.],\n",
       "       [1., 0., 1., 0., 1., 1., 0.],\n",
       "       [1., 0., 1., 1., 0., 0., 1.],\n",
       "       [1., 0., 1., 1., 0., 1., 0.],\n",
       "       [1., 0., 1., 1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 1., 0., 1.],\n",
       "       [1., 1., 0., 0., 1., 1., 0.],\n",
       "       [1., 1., 0., 1., 0., 0., 1.],\n",
       "       [1., 1., 0., 1., 0., 1., 0.],\n",
       "       [1., 1., 0., 1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0., 0., 0., 1.],\n",
       "       [1., 1., 1., 0., 0., 1., 0.],\n",
       "       [1., 1., 1., 0., 1., 0., 0.],\n",
       "       [1., 1., 1., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 1., 1.],\n",
       "       [0., 1., 0., 1., 1., 1., 1.],\n",
       "       [0., 1., 1., 0., 1., 1., 1.],\n",
       "       [0., 1., 1., 1., 0., 1., 1.],\n",
       "       [0., 1., 1., 1., 1., 0., 1.],\n",
       "       [0., 1., 1., 1., 1., 1., 0.],\n",
       "       [1., 0., 0., 1., 1., 1., 1.],\n",
       "       [1., 0., 1., 0., 1., 1., 1.],\n",
       "       [1., 0., 1., 1., 0., 1., 1.],\n",
       "       [1., 0., 1., 1., 1., 0., 1.],\n",
       "       [1., 0., 1., 1., 1., 1., 0.],\n",
       "       [1., 1., 0., 0., 1., 1., 1.],\n",
       "       [1., 1., 0., 1., 0., 1., 1.],\n",
       "       [1., 1., 0., 1., 1., 0., 1.],\n",
       "       [1., 1., 0., 1., 1., 1., 0.],\n",
       "       [1., 1., 1., 0., 0., 1., 1.],\n",
       "       [1., 1., 1., 0., 1., 0., 1.],\n",
       "       [1., 1., 1., 0., 1., 1., 0.],\n",
       "       [1., 1., 1., 1., 0., 0., 1.],\n",
       "       [1., 1., 1., 1., 0., 1., 0.],\n",
       "       [1., 1., 1., 1., 1., 0., 0.],\n",
       "       [0., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 0., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 0., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 0., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 0., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 0., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Generate_Action_Arrays(num_lily_pads):\n",
    "    Action_List = []\n",
    "    #Create possibe Macrostates: \n",
    "    for i in range(num_lily_pads):\n",
    "        temp_array = np.zeros((1,num_lily_pads-1))\n",
    "        temp_array[0,0:i] = 1\n",
    "        \n",
    "        #For each macrostate create the microstates and store them:\n",
    "        microstate_actions = distinct_permutations(temp_array[0],num_lily_pads-1)\n",
    "        #microstate_actions = itertools.combinations(temp_array[0],num_lily_pads-1)\n",
    "\n",
    "        \n",
    "        for microstate in microstate_actions:\n",
    "            Action_List.append(microstate)\n",
    "             \n",
    "    return np.array(Action_List)\n",
    "Generate_Action_Arrays(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing out a Policy Implimentation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implied MP Transition Map\n",
      "--------------\n",
      "From State Frog_State(position=1):\n",
      "  To State Frog_State(position=2) with Probability 0.900\n",
      "  To State Frog_State(position=0) with Probability 0.100\n",
      "From State Frog_State(position=2):\n",
      "  To State Frog_State(position=0) with Probability 0.100\n",
      "  To State Frog_State(position=1) with Probability 0.100\n",
      "  To State Frog_State(position=3) with Probability 0.100\n",
      "  To State Frog_State(position=4) with Probability 0.100\n",
      "  To State Frog_State(position=5) with Probability 0.100\n",
      "  To State Frog_State(position=6) with Probability 0.100\n",
      "  To State Frog_State(position=7) with Probability 0.100\n",
      "  To State Frog_State(position=8) with Probability 0.100\n",
      "  To State Frog_State(position=9) with Probability 0.100\n",
      "  To State Frog_State(position=10) with Probability 0.100\n",
      "From State Frog_State(position=3):\n",
      "  To State Frog_State(position=0) with Probability 0.100\n",
      "  To State Frog_State(position=1) with Probability 0.100\n",
      "  To State Frog_State(position=2) with Probability 0.100\n",
      "  To State Frog_State(position=4) with Probability 0.100\n",
      "  To State Frog_State(position=5) with Probability 0.100\n",
      "  To State Frog_State(position=6) with Probability 0.100\n",
      "  To State Frog_State(position=7) with Probability 0.100\n",
      "  To State Frog_State(position=8) with Probability 0.100\n",
      "  To State Frog_State(position=9) with Probability 0.100\n",
      "  To State Frog_State(position=10) with Probability 0.100\n",
      "From State Frog_State(position=4):\n",
      "  To State Frog_State(position=5) with Probability 0.600\n",
      "  To State Frog_State(position=3) with Probability 0.400\n",
      "From State Frog_State(position=5):\n",
      "  To State Frog_State(position=0) with Probability 0.100\n",
      "  To State Frog_State(position=1) with Probability 0.100\n",
      "  To State Frog_State(position=2) with Probability 0.100\n",
      "  To State Frog_State(position=3) with Probability 0.100\n",
      "  To State Frog_State(position=4) with Probability 0.100\n",
      "  To State Frog_State(position=6) with Probability 0.100\n",
      "  To State Frog_State(position=7) with Probability 0.100\n",
      "  To State Frog_State(position=8) with Probability 0.100\n",
      "  To State Frog_State(position=9) with Probability 0.100\n",
      "  To State Frog_State(position=10) with Probability 0.100\n",
      "From State Frog_State(position=6):\n",
      "  To State Frog_State(position=7) with Probability 0.400\n",
      "  To State Frog_State(position=5) with Probability 0.600\n",
      "From State Frog_State(position=7):\n",
      "  To State Frog_State(position=8) with Probability 0.300\n",
      "  To State Frog_State(position=6) with Probability 0.700\n",
      "From State Frog_State(position=8):\n",
      "  To State Frog_State(position=0) with Probability 0.100\n",
      "  To State Frog_State(position=1) with Probability 0.100\n",
      "  To State Frog_State(position=2) with Probability 0.100\n",
      "  To State Frog_State(position=3) with Probability 0.100\n",
      "  To State Frog_State(position=4) with Probability 0.100\n",
      "  To State Frog_State(position=5) with Probability 0.100\n",
      "  To State Frog_State(position=6) with Probability 0.100\n",
      "  To State Frog_State(position=7) with Probability 0.100\n",
      "  To State Frog_State(position=9) with Probability 0.100\n",
      "  To State Frog_State(position=10) with Probability 0.100\n",
      "From State Frog_State(position=9):\n",
      "  To State Frog_State(position=10) with Probability 0.100\n",
      "  To State Frog_State(position=8) with Probability 0.900\n",
      "Frog_State(position=0) is a Terminal State\n",
      "Frog_State(position=10) is a Terminal State\n",
      "\n",
      "Implied MRP Transition Reward Map\n",
      "---------------------\n",
      "From State Frog_State(position=1):\n",
      "  To [State Frog_State(position=2) and Reward 0.000] with Probability 0.900\n",
      "  To [State Frog_State(position=0) and Reward 0.000] with Probability 0.100\n",
      "From State Frog_State(position=2):\n",
      "  To [State Frog_State(position=0) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=1) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=3) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=4) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=5) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=6) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=7) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=8) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=9) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=10) and Reward 1.000] with Probability 0.100\n",
      "From State Frog_State(position=3):\n",
      "  To [State Frog_State(position=0) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=1) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=2) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=4) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=5) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=6) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=7) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=8) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=9) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=10) and Reward 1.000] with Probability 0.100\n",
      "From State Frog_State(position=4):\n",
      "  To [State Frog_State(position=5) and Reward 0.000] with Probability 0.600\n",
      "  To [State Frog_State(position=3) and Reward 0.000] with Probability 0.400\n",
      "From State Frog_State(position=5):\n",
      "  To [State Frog_State(position=0) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=1) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=2) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=3) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=4) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=6) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=7) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=8) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=9) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=10) and Reward 1.000] with Probability 0.100\n",
      "From State Frog_State(position=6):\n",
      "  To [State Frog_State(position=7) and Reward 0.000] with Probability 0.400\n",
      "  To [State Frog_State(position=5) and Reward 0.000] with Probability 0.600\n",
      "From State Frog_State(position=7):\n",
      "  To [State Frog_State(position=8) and Reward 0.000] with Probability 0.300\n",
      "  To [State Frog_State(position=6) and Reward 0.000] with Probability 0.700\n",
      "From State Frog_State(position=8):\n",
      "  To [State Frog_State(position=0) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=1) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=2) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=3) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=4) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=5) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=6) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=7) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=9) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=10) and Reward 1.000] with Probability 0.100\n",
      "From State Frog_State(position=9):\n",
      "  To [State Frog_State(position=10) and Reward 1.000] with Probability 0.100\n",
      "  To [State Frog_State(position=8) and Reward 0.000] with Probability 0.900\n",
      "Frog_State(position=0) is a Terminal State\n",
      "Frog_State(position=10) is a Terminal State\n",
      "\n",
      "Implied MRP Reward Function\n",
      "---------------\n",
      "{Frog_State(position=8): 0.1,\n",
      " Frog_State(position=3): 0.1,\n",
      " Frog_State(position=9): 0.1,\n",
      " Frog_State(position=7): 0.0,\n",
      " Frog_State(position=5): 0.1,\n",
      " Frog_State(position=4): 0.0,\n",
      " Frog_State(position=6): 0.0,\n",
      " Frog_State(position=1): 0.0,\n",
      " Frog_State(position=2): 0.1}\n",
      "\n",
      "Implied MRP Value Function\n",
      "--------------\n",
      "{Frog_State(position=8): 0.5,\n",
      " Frog_State(position=3): 0.5,\n",
      " Frog_State(position=9): 0.55,\n",
      " Frog_State(position=7): 0.5,\n",
      " Frog_State(position=5): 0.5,\n",
      " Frog_State(position=4): 0.5,\n",
      " Frog_State(position=6): 0.5,\n",
      " Frog_State(position=1): 0.45,\n",
      " Frog_State(position=2): 0.5}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_policy = Create_Policy([0,1,1,0,1,0,0,1,0])\n",
    "Frog_MDP = Frog_MDP_Finite(num_lily_pads=10)\n",
    "Implied_Frog_MRP : FiniteMarkovRewardProcess[Frog_State] = Frog_MDP.apply_finite_policy(test_policy)\n",
    "    \n",
    "print(\"Implied MP Transition Map\")\n",
    "print(\"--------------\")\n",
    "print(FiniteMarkovProcess(Implied_Frog_MRP.transition_map))\n",
    "\n",
    "print(\"Implied MRP Transition Reward Map\")\n",
    "print(\"---------------------\")\n",
    "print(Implied_Frog_MRP)\n",
    "\n",
    "\n",
    "print(\"Implied MRP Reward Function\")\n",
    "print(\"---------------\")\n",
    "Implied_Frog_MRP.display_reward_function()\n",
    "print()\n",
    "\n",
    "print(\"Implied MRP Value Function\")\n",
    "print(\"--------------\")\n",
    "Implied_Frog_MRP.display_value_function(gamma=1)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding The Best Action Array: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0.67578125, 0.703125  , 0.70996094, 0.71289062, 0.71484375,\n",
       "        0.71679687, 0.71972656, 0.7265625 , 0.75390625]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Find_Optimal_Policy(num_lily_pads):\n",
    "    #Create a Markov Decision Process\n",
    "    Frog_MDP = Frog_MDP_Finite(num_lily_pads=num_lily_pads)\n",
    "\n",
    "    #Generate the action array:\n",
    "    All_Actions = Generate_Action_Arrays(num_lily_pads)\n",
    "\n",
    "    #Preemtively Create array to store the actions with the best value functions:\n",
    "    Best_Value = np.zeros((num_lily_pads-1))\n",
    "    Best_Policy = np.zeros((num_lily_pads-1))\n",
    "    #Loop Through all of the actions and store the values of the value funciton\n",
    "    for action_array in All_Actions:\n",
    "\n",
    "        #Convert Action Array to Policy\n",
    "        frog_policy = Create_Policy(action_array)\n",
    "\n",
    "        #Apply Policy to Markov Decision Process to create a Markov Reward Process\n",
    "        Implied_Frog_MRP : FiniteMarkovRewardProcess[Frog_State] = Frog_MDP.apply_finite_policy(frog_policy)\n",
    "\n",
    "        #Get Value Function:\n",
    "        Value_function_array = Implied_Frog_MRP.get_value_function_vec(gamma=1)\n",
    "\n",
    "        for i,a in enumerate(action_array):\n",
    "            if Value_function_array[i] > Best_Value[i]:\n",
    "                Best_Policy[i] = a\n",
    "                Best_Value[i] = Value_function_array[i]\n",
    "\n",
    "    return Best_Policy, Best_Value\n",
    "\n",
    "Find_Optimal_Policy(num_lily_pads = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Map\n",
      "----------\n",
      "For State Frog_State(position=1):\n",
      "  Do Action 1.0 with Probability 1.000\n",
      "For State Frog_State(position=2):\n",
      "  Do Action 0.0 with Probability 1.000\n",
      "For State Frog_State(position=3):\n",
      "  Do Action 0.0 with Probability 1.000\n",
      "For State Frog_State(position=4):\n",
      "  Do Action 0.0 with Probability 1.000\n",
      "For State Frog_State(position=5):\n",
      "  Do Action 0.0 with Probability 1.000\n",
      "For State Frog_State(position=6):\n",
      "  Do Action 0.0 with Probability 1.000\n",
      "For State Frog_State(position=7):\n",
      "  Do Action 0.0 with Probability 1.000\n",
      "For State Frog_State(position=8):\n",
      "  Do Action 0.0 with Probability 1.000\n",
      "For State Frog_State(position=9):\n",
      "  Do Action 0.0 with Probability 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_LILIY_PADS = 10\n",
    "Frog_Decision_Policy = Create_Policy(Best_Policy)\n",
    "    \n",
    "print(\"Policy Map\")\n",
    "print(\"----------\")\n",
    "print(Frog_Decision_Policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Finite Markov Reward Class to apply the policy to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implied MP Transition Map\n",
      "--------------\n",
      "From State Frog_State(position=1):\n",
      "  To State Frog_State(position=0) with Probability 0.100\n",
      "  To State Frog_State(position=2) with Probability 0.100\n",
      "  To State Frog_State(position=3) with Probability 0.100\n",
      "  To State Frog_State(position=4) with Probability 0.100\n",
      "  To State Frog_State(position=5) with Probability 0.100\n",
      "  To State Frog_State(position=6) with Probability 0.100\n",
      "  To State Frog_State(position=7) with Probability 0.100\n",
      "  To State Frog_State(position=8) with Probability 0.100\n",
      "  To State Frog_State(position=9) with Probability 0.100\n",
      "  To State Frog_State(position=10) with Probability 0.100\n",
      "From State Frog_State(position=2):\n",
      "  To State Frog_State(position=3) with Probability 0.800\n",
      "  To State Frog_State(position=1) with Probability 0.200\n",
      "From State Frog_State(position=3):\n",
      "  To State Frog_State(position=4) with Probability 0.700\n",
      "  To State Frog_State(position=2) with Probability 0.300\n",
      "From State Frog_State(position=4):\n",
      "  To State Frog_State(position=5) with Probability 0.600\n",
      "  To State Frog_State(position=3) with Probability 0.400\n",
      "From State Frog_State(position=5):\n",
      "  To State Frog_State(position=6) with Probability 0.500\n",
      "  To State Frog_State(position=4) with Probability 0.500\n",
      "From State Frog_State(position=6):\n",
      "  To State Frog_State(position=7) with Probability 0.400\n",
      "  To State Frog_State(position=5) with Probability 0.600\n",
      "From State Frog_State(position=7):\n",
      "  To State Frog_State(position=8) with Probability 0.300\n",
      "  To State Frog_State(position=6) with Probability 0.700\n",
      "From State Frog_State(position=8):\n",
      "  To State Frog_State(position=9) with Probability 0.200\n",
      "  To State Frog_State(position=7) with Probability 0.800\n",
      "From State Frog_State(position=9):\n",
      "  To State Frog_State(position=10) with Probability 0.100\n",
      "  To State Frog_State(position=8) with Probability 0.900\n",
      "Frog_State(position=0) is a Terminal State\n",
      "Frog_State(position=10) is a Terminal State\n",
      "\n",
      "Implied MRP Transition Reward Map\n",
      "---------------------\n",
      "From State Frog_State(position=1):\n",
      "  To [State Frog_State(position=0) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=2) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=3) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=4) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=5) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=6) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=7) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=8) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=9) and Reward 0.000] with Probability 0.100\n",
      "  To [State Frog_State(position=10) and Reward 1.000] with Probability 0.100\n",
      "From State Frog_State(position=2):\n",
      "  To [State Frog_State(position=3) and Reward 0.000] with Probability 0.800\n",
      "  To [State Frog_State(position=1) and Reward 0.000] with Probability 0.200\n",
      "From State Frog_State(position=3):\n",
      "  To [State Frog_State(position=4) and Reward 0.000] with Probability 0.700\n",
      "  To [State Frog_State(position=2) and Reward 0.000] with Probability 0.300\n",
      "From State Frog_State(position=4):\n",
      "  To [State Frog_State(position=5) and Reward 0.000] with Probability 0.600\n",
      "  To [State Frog_State(position=3) and Reward 0.000] with Probability 0.400\n",
      "From State Frog_State(position=5):\n",
      "  To [State Frog_State(position=6) and Reward 0.000] with Probability 0.500\n",
      "  To [State Frog_State(position=4) and Reward 0.000] with Probability 0.500\n",
      "From State Frog_State(position=6):\n",
      "  To [State Frog_State(position=7) and Reward 0.000] with Probability 0.400\n",
      "  To [State Frog_State(position=5) and Reward 0.000] with Probability 0.600\n",
      "From State Frog_State(position=7):\n",
      "  To [State Frog_State(position=8) and Reward 0.000] with Probability 0.300\n",
      "  To [State Frog_State(position=6) and Reward 0.000] with Probability 0.700\n",
      "From State Frog_State(position=8):\n",
      "  To [State Frog_State(position=9) and Reward 0.000] with Probability 0.200\n",
      "  To [State Frog_State(position=7) and Reward 0.000] with Probability 0.800\n",
      "From State Frog_State(position=9):\n",
      "  To [State Frog_State(position=10) and Reward 1.000] with Probability 0.100\n",
      "  To [State Frog_State(position=8) and Reward 0.000] with Probability 0.900\n",
      "Frog_State(position=0) is a Terminal State\n",
      "Frog_State(position=10) is a Terminal State\n",
      "\n",
      "Implied MRP Reward Function\n",
      "---------------\n",
      "{Frog_State(position=6): 0.0,\n",
      " Frog_State(position=5): 0.0,\n",
      " Frog_State(position=4): 0.0,\n",
      " Frog_State(position=9): 0.1,\n",
      " Frog_State(position=2): 0.0,\n",
      " Frog_State(position=1): 0.1,\n",
      " Frog_State(position=3): 0.0,\n",
      " Frog_State(position=8): 0.0,\n",
      " Frog_State(position=7): 0.0}\n",
      "\n",
      "Implied MRP Value Function\n",
      "--------------\n",
      "{Frog_State(position=6): 0.717,\n",
      " Frog_State(position=5): 0.715,\n",
      " Frog_State(position=4): 0.713,\n",
      " Frog_State(position=9): 0.754,\n",
      " Frog_State(position=2): 0.703,\n",
      " Frog_State(position=1): 0.676,\n",
      " Frog_State(position=3): 0.71,\n",
      " Frog_State(position=8): 0.727,\n",
      " Frog_State(position=7): 0.72}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Implied_Frog_MRP : FiniteMarkovRewardProcess[Frog_State] = Frog_MDP.apply_finite_policy(Frog_Decision_Policy)\n",
    "\n",
    "print(\"Implied MP Transition Map\")\n",
    "print(\"--------------\")\n",
    "print(FiniteMarkovProcess(Implied_Frog_MRP.transition_map))\n",
    "\n",
    "print(\"Implied MRP Transition Reward Map\")\n",
    "print(\"---------------------\")\n",
    "print(Implied_Frog_MRP)\n",
    "\n",
    "\n",
    "print(\"Implied MRP Reward Function\")\n",
    "print(\"---------------\")\n",
    "Implied_Frog_MRP.display_reward_function()\n",
    "print()\n",
    "\n",
    "print(\"Implied MRP Value Function\")\n",
    "print(\"--------------\")\n",
    "Implied_Frog_MRP.display_value_function(gamma=1)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOydd3hUVfrHP3dqMsmkTgrpCYReAxGptkURRRFd+67rugr2ZbGs2HsDC4qKBevPshbEgh2VKhBqCD0hvfdMJpNp5/fHhJBAAhGSzCScz/PMk9x7zz3nvYH5zjvvec97FCEEEolEIum9qDxtgEQikUi6Fin0EolE0suRQi+RSCS9HCn0EolE0suRQi+RSCS9HI2nDWgLk8kkEhISPG2GRCKR9Bg2bdpULoQIa+uaVwp9QkICaWlpnjZDIpFIegyKouS0d02GbiQSiaSXI4VeIpFIejlS6CUSiaSXI4VeIpFIejlS6CUSiaSXI4VeIpFIejlS6CUSiaSXI4VeclJQ9+uvVH/+hafNkEg8ghR6SZfS4GggtzbXszbsyKDgP3Op+ugjhMPhUVskEk8ghV7SpTy/6Xku+foSyhvKPTK+vbCQvBtnowkOJvbVV1A0XrkYXCLpUqTQS7qMtYVr+Wj3R1ycfDEmX1O3j++sqyNv1iyEtZHYxa+hCWuzDIhE0uuR7o2kS6hprOH+NfeTGJjI7Sm3d/v4wmYj/7bbaMzOIe6N19EnJ3e7DRKJtyA9ekmX8OSGJ6lsqOTJiU/io/Hp1rGFEBQ9+BCWdX/Q59FH8Dv11G4d/2hs/iGH3IwKT5shOcmQQi/pdH7I/oFvs77lhhE3MMQ0pNvHL3/1VWqWLsV0yy0EzZjR7eO3x8ZvD7BuaSb7N5d62hTJSYYM3Ug6lTJLGY/+8ShDQ4fyr2H/6vbxa776ivKFLxF44YWYbr6p28dvCyEEG74+QNrybAaeGsnpVw30tEmSkwwp9JJOQwjBg2sfxOqw8vikx9GqtN06fv36DRTeex+GsWPp8+gjKIrSreO3hRCC9cuy2PR9DoPG9+H0qweiUnneLsnJhQzdSDqNz/Z9xqqCVcwZPYekwKRuHbsxM5P8W29FFx9HzEsLUXS6bh2/LYQQrFuayabvcxg8MYozpMhLPIT06CWdQl5tHs9ufJaxfcZyxcArunVsR3k5eTfMQtHpiH1tMeqAgG4dvy2EEKz5fD/bfs5j6ORoJl/eH0WKvMRDSKGXnDBOl5N719yLRtHw2ITHUCnd90XR1dBA3o034aisJP6999DFRHfb2O0hhGD1p/vYviKfYWfEMOnSZK8II0lOXqTQS06YdzLeYUvpFp6Y+ASRfpHdNq5wOim4406sGRnEvPwyvsOGdtvY7dokBKs+3kv67wWMODOWCX/tJ0Ve4nFkjF5yQuyp3MPLW19mSvwUzk86v1vHLnn6acy//ELEvHkYzzyjW8duC+ES/P6RW+RHTonzWpFvtFj48tlHKdizy9OmSLoJKfSS48bmtHHP6nsI1AVy/6n3d6uoVb73HlXvvU/INdcQcvVV3TZuewiX4Lf/203GygJSzoln/My+XinydZXlfPLQ3WRt3khNabGnzZF0EzJ0IzluFm1dxL6qfSw6axHBPsHdNm7dL79Q8uRTGKf8hfC77uy2cdvD5RL8+v4udq8rZsy0BE6ZnuiVIl+Wm80XTz1EY309M+9+kISRoz1tkqSbkELfWxEC6svAP7xLut9cspm3d7zNxckXMzlmcpeM0RYN6ekUzL0Dn2HDiHrmGRS1utvGbguXS7Di3V3sWV9M6vmJnHJ+okftaY/cHdtYNv9xtD4+XP7w04QndG/6q8SzyNBNb2Xfj/D8UMjb2Old19vrmbd6HlH+UdyZ2n0etS0/n7zZN6IxmYh99RVUvr7dNnZbuJwufn57J3vWFzP2Au8V+Z0rV/D5Ew9iDDVx5WPzpcifhEiPvjciBKycD/4REDWy07ufnzafQnMh70x9Bz+tX6f33xbOmhryZs1GOBzEvr4YTWhot4zbrj1OFz8v2cn+TaWcOiOJ0VMTPGpPWwghWL/0f6z55H1iBw/jgjvuxcfP39NmSTyAFPreSPYqyN8A0+aDunPLEKzMX8lnez/j2qHXkhKR0ql9t4ew2ci/9TbsubnELXkLfZJnPVKn08VPb2aQuaWMcTP7knJ2vEftaQuX08nPb71C+i8/MGji6Zw9+3Y02u4tSSHxHqTQ90YOevOj/tap3VZZq3hgzQMkBydzy8hbOrXv9hBCUHT//Vg2bCDq2WcxpKZ2y7jt4XS4+OGNHRzYVs6ES/ox8i9xHrWnLWzWBr55/ikObN3EKTP+ysTL/+6Vk8OS7kMKfW8jPw0O/A5THgVt59WBF0Lw6B+PUmOrYfGUxejU3VNLpvyll6lZ9hVh/76dwOndm6d/OE67i+/f2EH29nImXZbM8DNiPWpPW9RXV/HFUw9Rln2Av/zrZkZMOdfTJh0V4RKyNEQ3ICdjexurFoBvMIz5Z6d2++2Bb/kp5yduHnkzA0IGdGrf7VH9xVLKX3mFwItnEjprVreM2R4Ou5PvFqeTvb2cyZf390qRr8jP48P75lJZmM+Mu+73epG35dVR8sJm7KUWT5vS65EefW+iJAP2LIfT54G+8ybdiuuLeeKPJxgZNpJrh1zbaf0ejfp16yh64AH8xo+jz0MPeTT04LC5RT43o5LTrxrAkEmer6dzOPk7d/Dl/EdRa7Rc9uBTRPb17q0T69NKqPpyH2qjDuEUnjan1yOFvjexagHo/GHsDZ3WpUu4uH/N/TiEgycmPoFa1fV569a9e8m/9Tb0SUlEv/giigcnEe02J9+9up283VWc8beBDJ4Q5TFb2mP3mt/5/pXnCQiP5OJ7HiIwvPvqDf1ZhNNF9TdZ1K8rQt8viJArBqL2k5PEXY0U+t5CRSZkLIXxt7pDN53Ex7s/5o+iP3hg3APEBnR9uMJeWkre7NmofH2JXfwaaqOxy8ds15ZGJ9++sp2CvVWc9fdBDBzXx2O2tIUQgrSvv2Dl/71N9MDBXHjn/fj6e+7vdSycdTYq/m8Xtuxa/CdFEzg1EUUt4/PdgRT63sLq50Ctg3Gdlw1zoOYAz296nknRk7gk+ZJO67c9XPX15M++EWd1DQkfvI+2j+eE1WZ18O2i7RTtr+Yv/xjMgLHe5SW7XE5WvP062378lv7jJnHuTXPQeMFmK+1hy6uj4oOduCwOQi4fgGFk16zYlrSNFPreQHUebPvYPQHbSSUPHC4H81bNQ6/R8/D4h7s8Ri6cTgrm3oF1925iX30Fn8GDu3S8o2GzOvjm5W0UZ9Uy5Z9DSE6N8JgtbWG3Wvlm4TNkbdrAmOkzmXzlP1BU3ptXUb+phKql+1D76wi7cQS6KLloq7uRQt8bWLvQ/XP8bZ3W5Rvpb7CjYgfzT5tPmCGs0/ptCyEEJY8/gfm334h86EH8TzutS8c7GrYGB1+/tJWS7DrOvm4I/UZ7l+dpqalm6dMPU5KVyZn/nM2oczybcno0hNNFzbcHMK8tRN83kJArB8l4vIeQQt/TMZfC5vdgxOUQ1Dkx9IzyDF7f9jrTEqdxTsI5ndLn0ah8512qPvyQkOv+SfDll3f5eO3RaLHz9UvbKMup45x/DaFvineJfGVhAV889SD1VVVcMHce/VJP9bRJ7eI0N8XjD9TiPzGawHNlPN6TSKHv6axbBE4bTPxPp3RndViZt3oeIb4hzBs7r1P6PBq1P/xI6TPPYDznHMLnzu3y8drDWm/n64VbKc83c84NQ0ka2bXfYv4sBbt38uWzj6KoVFz6wBP0Se6etQzHgy2/jor3e1Y8vr6+Hj+/7qnb5Amk0PdkGqpg41sw5CII7dspXb64+UWyarJYPGUxgfrATumzPRq2bqXwrrvwHTGCqKef8lic2Wq2s+zFLVQW1XPurGEkDDd5xI722Lt+Dctfmk+AKYyZ/32YoEjvyv5pSat4/OwR6KK9Ox7vcDhYsWIFmzZtYvbs2QQHd9++Ct1Jh4ReUZSpwIuAGnhTCPHUYdfvBA5u86MBBgFhQohKRVGygTrACTiEEGM6yXbJ+tfBVtdp3vyGog18sOsDrhh4BeOjxndKn+1hy80l76ab0UREEPPKIlQ+nVeu4c/QYLax7IWtVBdb3CI/zLtEftO3X/Lb+2/RJ3kAM+68H0NA1374Hi89MR5fVVXFZ599RkFBAampqfj7e/eH0olwTKFXFEUNLAKmAPnARkVRvhJC7DzYRgjxLPBsU/vpwBwhRGWLbs4QQpR3quUnO41mWP8q9D8XIk98U+w6Wx33rrmXhIAE5oye0wkGto+zupq8G2aBy0Xc64vRhIR06Xjt0VBnY9kLW6gubWDajcOIG+LZ0sctcbmc/P7eW2z+7iuSTxnPubfORavTe9qsNnHH43djO1DTY+LxO3fuZNmyZQBceumlDPZglld30BGP/hRgvxAiC0BRlI+BC4Gd7bS/Avioc8yTtEvaEnfoZvIdndLdUxueosxSxnvnvoevpus29HDZbOTdcgv2wkLi3nkbXUJCl411NCy1bpGvLWvgvJuGEzvIMx82bWG3NfLdSwvYt2EtKedewGl/vw5VN6xIPh7c8fhdOOvthFw2AMMo747H2+12fvzxRzZu3Eh0dDSXXHJJrw3XtKQjQh8N5LU4zgfGttVQURQDMBVouWpHAD8qiiKAxUKI19u59wbgBoC4OO8r/epV2K2w7mVIPA1iTjwS9kvOL3yV+RWzhs9ieNjwTjCwbYTLRdE982hI20T0cwswpHRPPfvDqa9pZNnzW6irtHLeLSOIGeA9b3RLbQ1fPvsoRfv2cPrfr2f0eRd62qR2aRmPD7/R++Px5eXlfPrpp5SUlDB+/HjOPPNMNJqTY5qyI0/Z1new9qoQTQfWHBa2mSCEKFQUJRz4SVGU3UKIlUd06P4AeB1gzJgxssrR0djyPphL4OI3T7ir8oZyHl73MINCBjFrRNdWiCx7cSG1335L2Nz/EDBtWpeO1R7mqkaWvbAFc3Uj028dQVSy94h8dXERXzz1IHXl5Uyf81/6j53gaZPapFU8PimQkCsHovb33lW5ANu2beObb75Bo9Fw5ZVX0r9/f0+b1K10ROjzgZYJ2jFAYTttL+ewsI0QorDpZ6miKEtxh4KOEHpJB3HaYc1CiDkFEiadUFdCCB5e+zD19nqenPQkWlXXTZ5VffopFYsXE3TppYT+619dNs7RMFdZ+fK5LVhqbVxw6wj69AvyiB1tUbRvD0ufeQThcnHJ/Y8TPWCQp01qk54Wj7fZbCxfvpytW7cSHx/PzJkzCQz0zgntrqQjQr8RSFYUJREowC3mVx7eSFGUQOA04OoW5/wAlRCirun3s4FHOsPwk5b0T6EmF86bDydYluDL/V/yW/5v3DnmTvoGdU56ZluYV6+h+KGH8Zs0icgH7vdIyeHa8gaWvbCFBrOdC24fSWSS97zZ92/8g28XPotfcDAz//swIVHeVwYZWsfjgy8bgJ+Xx+NLSkr49NNPKS8v57TTTmPy5Mmo1d4513EQl6sRlarzJ92PKfRCCIeiKLcAP+BOr1wihMhQFGV20/XXmppeBPwohKhvcXsEsLTpja0BPhRCfN+ZD3BS4XLCqucgYhgkn31CXeXX5fPUhqdIjUzl6sFXH/uG48SSlkbB7bejT04m+vnnUboxJupyCfJ2VrJzdSHZ28vR6NVukU/0HpHf8sM3rHh7MZFJ/bjo7gcxBHrPt4yW1G8uoeqL/aj9tV4fjxdCsHnzZr777jt8fHz4+9//TpKH9xk+Fk5nA/szn6G2Np3RKR+jUnXu+6RDvQkhlgPLDzv32mHH7wDvHHYuCxhxQhZKDrHrK6jYB5e8fULevNPl5L4196EoCo9NeAyV0vkLlezFxZQueI7ar79GGx3tLjns3z0rD+sqrexaU8iutUWYqxrxNWoZflYsw06LJsDUdRlFfwbhcrHyw3dI+/oL+o4Zy3m33onWQ2sJjoZwuqhZfgDzmp4Rj7darXz99ddkZGSQlJTEzJkzvT4/vrZ2Oxk752KxZBEb8w+EcNLZa1lPjinn3oAQsHIBhCbD4BPLxPhg1wdsKtnEoxMeJcq/czfScFmtVCxZQsUbb4LTSejsWZiuvx5VFy8vdzpd5GyvIGN1Ibk7KwCIHRTChEuSSRxhQq3xnuqODpuN7195nj3rVjHynPM44x83eGX6pNNso/LD3TRm1eA/IYrAaUleHY8vLCzk008/pbq6mrPOOosJEyag8uKqni6XnezsV8jOWYROF8aoke8REtI1E/BS6HsK+36EknS48BU4AVHYV7WPFze/yJmxZ3Jh385L3RNCUPf995Q8+yyOwiJ37Zo770AXE9NpY7RFdanF7b2vK6ah1oZfoI4x5yYwaHwfr/HeW9JgrmPZs49RsDuDyVddy5jpMz26TWJ72ArMVLy/E6fZTvCl/fFL8a5SzS0RQrB+/Xp+/PFH/P39ufbaa70+Rbu+PpOMnXOpq0snMmIG/fs/iFYb0GXjSaHvCQgBK+dDYBwMv/S4u7E77cxbPQ+jzsgD4x7oNIFpyMig5MknaUjbhH7gQKKeegq/U07plL7bwmF3krW1jJ2rCynYU42iUogfGsrgiVHEDwlBpfYuL67RUk/2ti0c2LKRrM0bsTVYOO+2Oxk4wXPlmI9G/ZZSqj7fh9pPS/js4ehivHfXKovFwrJly9izZw8DBgzgwgsvxGAweNqsdhHCRX7+e+zPfAa12sDQoS8TEd71m7hLoe8JZK+C/A0wbT6ojz8F8tVtr7K7cjcvnvEiob4nvtzfUV5O6QsvUPP5F6iDg4l8+GGCLrkYpYsyGyoL69m5upDd64torHdgDPVh7AVJDBzXB/9g7ykPIISgsiCPrM0bObAljYI9O3E5nfj4+ZMwcjSjpk4nqv9AT5t5BMIpqFme1WPi8bm5uXz22WeYzWamTp3K2LFjvfLb0UGs1kJ27rqbqqq1hIaezqCBT6LXd0/mkhT6nsDK+eAXDqP+dtxdbC3dyls73mJGvxmcGXfmCZnjstmoev99yl95FVdjIyH/+Aemm27skv1d7Y1O9m8qYefqIoqzalCpFRJHhDFkYhQxA4NRVN7xxrbbGsnPSCdry0ayNqdRW1YCgCkugTHnX0RiSipRyQNReWl635Hx+EQUL/tmdBCXy8WaNWtYsWIFQUFBXHfddURHe2dKKjRtrFPyFXv2PogQTgYOeIyoqMu79UNJCr23k58GB36HKY+A9viyMix2C/euvpdIQyR3p9593KYIITD/+hslTz+FPScX/9NOI/zuu9EnJR53n+1RlltHxupC9m0oxmZ1EhRhYPzF/Rh4aiS+Ru/wMmvLSzmwJY2szRvJ3bEdh60RjU5P3LARnHLhxSSOGkOAybtzzaFlPN5G8F/74zfae+PxZrOZpUuXkpmZyZAhQ5g+fTo+XpitdBC7vYrdu++ntOw7AgNTGDxoPgZDfLfbIYXe21m1AHyC3PvBHifPbXqOvLo83jrnLfx1x5dq1rhvHyVPPkX92rXokpKIfeN1/Ced2MrcI8ZocLBvYwk7VxdSlluHWquiX0o4gydG0adfoMe/lrucTgr37iJrSxoHtqRRnpsNQGB4BEPPmEJSSiqxg4d59Sbdh2PZUkplczx+hFfH47Oysvjiiy+wWq1Mnz6dlJQUj/+fOBrl5b+ya/c92O3V9E26k/j463EXA+5+pNB7MyUZsGc5nH4P6I/vDbimYA2f7PmEvw/+O6mRqX/6fmd1NWUvvUzVxx+j8vMjYt48gq+4HEXbOeUShBAUZ9Wyc3UB+zeV4rC5CI32Z9Jl/el/SgQ+Hq5pbqmtIXvbZrI2byRn22as9WZUajXRA4cw+ep/kjQqlZDoGK8WnLYQTkHNdwcwry5AlxhI6FXeG493Op38/vvvrFy5EpPJxN/+9jciIrz3W4fDUc/+/U9SUPgRfn79GTliCUajZ8sgS6H3ZlYtAJ0/nHLDcd1e01jDA2seoG9gX25L+XMbhwuHg6qPP6HspZdw1dURfPllmG69FU0nlXS1mu3sWV9MxupCqorq0erV9D8lksETowiPN3pMOIUQlOUcIGvzRrK2bKRo3x4QAkNgEH3HnEpSyhjih49Cb+h5284JhwtbgRlbTi0NO8qx5dbhPz6KwPO8Nx5fU1PD559/Tm5uLiNHjmTatGnovPgbU3XNJnbuvIOGhjzi4q6nb9KcLilp8GeRQu+tVGRCxlIYfysYjq9W+uN/PE6ltZKXz3oZvbrj/9nMa9ZQ+tRTNO7bj+HUU4m45x58Bpx4tT/hEhTsrWLn6kIyt5bhcgjCEwI44+qB9BsTjs7HM/8dbdYGctK3cqApS8Zc5S6+GpGUzLiLLydpVCoRSf08ttXh8eI027Dl1tGYU4stpxZbfh043IVh1aE+Xh+P37t3L0uXLsXhcHDRRRcxYoT3LrJ3uWxkHVhITs5ifHyiSBn1IcHBXZdi/GeRQu+trH4O1DoYd8ux27bBuxnv8l32d9w66lYGhXasEqItO5uSZ57FvGIF2thYYl5+Cf+zzjph77q+ppHd64rYuaaI2rIG9AYNQyZGM3hiFKYYzyxPryou5MDmjWRtSSN/ZzpOhwOdr4GE4aNITEklceRo/IK8p4TxsRAugaPMgi3nkLA7yhvcF9UKumh//MdFoY8PQBcfgNpLJrTbwuFw8Msvv7Bu3ToiIiL461//isnkXVs8tsRs3kPGzrmYzbuI6nMpycnz0Gi8a65DCr03Up0H2z52T8D6//msjTe2v8HCLQuZEj+Ffw499iSu02ym/NVXqXzvfVRaLWFz/0PINdegOoGvyNWlFnLSK8hOL6dwbzUulyAqOYhTzk+k76gwNLrunZRy2GwU7NnZtGgpjaqiAgBComIYOXU6SaNSiR44GHUP2YjCZXNizz8o6u6fosEBgMpPgy4uAMOYCPQJAeiijSjanvFt5PB9XM8++2y0nTQf1NkI4SQ3bwmZmc+h0RgZPmwxYWF/8bRZbdIz/lefbKxd6P45/k/G1YVg0dZFLN6+mPOTzufRCY+iOUoVPOF0UrN0KaXPv4CzspLAiy4i7N+3ow3/8x8uTqeLov015KSXk51eQXWJBYDgSAMjp8QycFwfgiO7Pq4tXC6qS4spz82mPDeH8rwcynOzqSouRLhcqLVaYocMZ9TU80kclUpQRGSX29QZOGsamz31xpxa7IX14HKHYTThvhiGmtDFG9HFB6Ax+fa4yWHoWfu4NjTks3PXnVRXbyDMNIWBAx9Hp/OePYcPRwq9t2Euhc3vwfDLISj22O2bEELw/KbneTvjbWYmz+SBUx9AfZSaOJa0NIqfeILGnbvwHTWKiNdew3fYn9tkvKHORk5GBTnpFeTurMTW4EClUYjuH8zQ06JJGBZKYFjXLUe31FRT1izo2W5xz8/F0djobqAoBIVHYoqLp/+4iUT2TSZuyAivrBLZEuES2Ivr3aKe7RZ3Z7X7mRStCm2MEePkGHQJAejjjKgM3unxdhS73c4PP/xAWlqa1+/jKoSgqOhz9u57FIBBg56mT+TFXv/BKoXe21i3CJw2mDinw7e4hIunNzzNh7s/5LIBlzFv7Lx2Sw/bCwoomT+fuu++RxMZSdSC+QRMm9ah/6hCCCoKzGRvryBnRznFB2pBgCFAR9+UMBKGmogZFNzpk6p2q5Xy/By3oOdmu0U9LxdLTXVzG9+AQMLi4hl+5jmY4hIwxcVjion3elEHcFkdrSdNc+sQNicAqgCdO64+MRp9fADaKD+vzZA5HnrSPq42Wzm7dt9LefnPBAWNZfCgZ/H19d4VuS3xzr/oyUpDFWx8CwbPAFO/Dt3iEi4eWfcIn+/7nGsGX8PcMXPbFG2XxULFm29S8dYSUBRMN99M6L+uQ+V79AqPdpuTgt1VZKeXk7OjAnOV27MMjzeSel4iCcNCCYs1dkopApfTSVVxYWsPPTeH6tJid2E3QKPXY4qJIyklFVNsPKZYt6j3lIlTIQTOqkNhGFt2LfaSevcuzApoI/0wpIS7Y+txAaiD9V7vLXYUu91OaWkpJSUlFBcXU1xcTGFhIVqt1uv3cS0r+4ldu+fhdJpJ7jeP2NhrUbpgH4euQgq9N7H+dbDVwaS5HWrucDl4YM0DfJ31NdcPu55bR916hCgIIaj95htK5y/AUVJCwLRphN8xF21U+3Xo6yqtzbH2/D1VOO0utHo1sYNCSD0/lPihofgFHn9usBACc1XFIQ891+2hVxTk4rTbAVAUFUF9oghPSGLw5DPdHnpcAoHhEV5Zu/0gwuHCWWvDWW3FUWPDWd2Is6bR/bO6EUd1I8LqnjRV9Gp0cUYChsahiw9AF2tE5aEU087GbDY3i3lxcTElJSWUl5cjmj6wdTodERERpKSkMGHCBK/dx9XhqGPvvscoKvoMf//BDBm8AH9/7/1Aao/e8b+qN9BohvWvQv9zIfLYsXK7y849q+7hh+wfuGXkLcwaMeuINg3p6ZQ8/gQNW7fiM2QI0c8twDB69BHtXC5BSVYN2TsqyEkvp6LAvRtkgMmHIROjSBhmIio5CPVxZG40WizNE6JuL939u7Xe3NzGLzgEU2w8I4eeT1hcAqbYeEJiYtHqPL/QpCXCJXCZbTgOE29nTWOTqFtxme1u77wFKoMGdaAedZAeXUIA2ggDuvgAtJF+XlOU7XhxOp1UVFQ0i/lBYa+vP7SjaGBgIBEREQwaNIjIyEgiIyMJCgry6k1BAKqq1rNz151YrUUkxN9EYuKtqFTem5Z6NKTQewtpS9yhm8l3HLOpzWnjjt/v4Ne8X5k7ei7/GPqPVtftJaWUPf88NV9+idpkos/jjxN40YxWC36s9XbydlaSnV5ObkYl1no7ikohql8g42f2I2F4KEERhg6HDZwOO5WFBYdEPTeb8rwcastKm9tofXwxxcaRfOoETLEJhDV56b7GrttwoaMIIRANDreIHxTymsbDjm3NmS4HUbQq1EFuEdcOCEEdqEfTdHxQ3FXdnEraVVit1iMEvaysDIfD/Q1FrVYTFhZGcnIykZGRREREEBkZie8xwoPehtPZSFbWAv5DhpIAACAASURBVHLzluDrG8eY0Z8QGJjiabNOCCn03oDdCutehsTTIGbMUZtaHVbm/DaH1QWrueeUe7hy0JXN11yNjVS+8y7lixeD3U7ov64jdPZs1P7+7hrpRfXuWHt6BUWZNQiXwMdPS/zQUOKHhRI3OAT9MTI4hBDUlZc1ZbtkNwt7ZWEBLmdTHrdaTXCfaPokD2T4WVMJjY0nLC6eAFO4R1aXCiEQNifOGluzJ+5oIeYHPXNhd7W+UaWgDtShDtSjjw84JN5NAq4J0qP4anpNDP0gQgiqq6tbhV2Ki4uprj40+W0wGIiMjCQ1NbXZSzeZTKi9tAxzR6mryyBj51zq6/cRHX0lyf3uQa323o1MOooUem9gy/tgLoGZbxy1mcVu4bYVt7GheAMPjXuIi/tfDDSJ708/UfrMs9jz8/E/6ywi7roTdVQsBfuqyE4vJCe9nNpyKwCh0f6knB1H/DATEYkBqNoJH1jNZspzsylrMTFanpeDrcHS3MYYGoYpLp7ElFTCYt0eenBUDJpuWOTianTiMttw1tlwme04zTacdfamc/ZW144QcQVU/jq3Jx5hwKd/MOogH9RBuiav3AeVv7bHh1aORVsTpCUlJTQeTFEFTCYT0dHRjB49utlTNxo9V4+oK3C5HOTmvk7WgYVotcGMGPEWptDTPW1WpyGF3tM47bBmIcSkQuLkdpuZbWZu/uVmtpZt5fGJjzO973QArHv2UPLEk1jWr0efnEzskrcwRw5h1e+FZG5ahb3RiVqrImZgMKPOjid+aCjGkNYphw67ncqCPLeot/DSzZUVzW30fn6YYhMYNOkMd8glNoHQ2Dh8/Dq3hIHL5sRVZ8NpPlKwnWZ7q2vC5jqyAwVUBi0qfy1qozs1UWXUofbXoQrQoWnyxtUBOhQv2jC8s3E4HFgsllav+vr6Vr+XlZW1OUE6fPjw5rBLeHi4VxcRO16EENjtFdRbDmCxZFFU+Ck1tVsID5/GwAGPoNX2jCyujiKF3tOkfwo1uTDtWWjHQ6pprOHGn29kV8Uunp78NFMTpuKorKRs4UKq//cpaqORoHseoCRqHOt/LqGycBMavZrk0eEkjQojekAwWp0a4XJRU1rCvo2tPfSqogKEyy2aao2GkOhYYocMxxQb754cjUvAPyT0uD04YXfirHN73K7mn22Jub05f/xwVH4atwfur0UXZ0Ttr0Nt1DafaxZzPy2Kuvd4muDeUclqtbYr2m0Jus1ma7c/X19fDAYDJpOpx02Q/llcLhuWhhwsliws9QewWDKbxd3hqGlup9EEMWTw80RETO9V31QOIoXek7icsOo5iBgG/c9ps0mVtYpZP81if/V+Fpy+gDP6TKLy3Xcpe3kRTosFx8U3khs1gQMba3A6sgiPN3L6VQNIHBFEeW4m5blr2LPGnZdekZeLvdHa3HdgRCSm2AT6jx3vXmQUG09QZNSfqvcinK7Wse/DslGcNY24LI4271UZNG7P21+HNsaIT7NgHxJutVHbJN69R4BsNttRve3Dzzc0NDR73Yej1WoxGAzNr5CQEAwGA35+fq3OH3z5+vr2+Dj64QghsNkrsNRnuQXdkkV900+rNR8hDjkPOl04foYkIiLOw2BIws+QhMGQhI9PlMc2BekOpNB7kl1fQcU+uOTtNr358oZyrv/xevLq8lh45kJGZrrImj0Dc14ZFadeRkFoKrVlDvT1dQyeGEXcEC3VRbvYu3Y5P7/u3toOwNcYgCkugaFnTmnKdnGHXXQ+R8+GcKcT2g9ln9Q0tsoLd1Q34jLbjkgnVHw1zSESXXwA6gAdaqOutYj7aXtU6MTpdNLY2EhjYyM2m6359/bOHe3Y6Wz7W4uiKK1E2WQyERcX165oGwyGXhlWaQ+XqxGLJQdLk0deb8ls/t3hqG1up1LpMfgmYDQOISLi/BaCnuh1VSW7C6U9T8GTjBkzRqSlpXnajK5FCHhtEjiscPN6OGwRUEl9Cf/68V+UWEpY1G8e4Uu+Jy+9jOK+f6HUfwBCQGRff/okNmBryCRn2ybK83IAt6eeNCqVhJEpRCT2wxAY1OZCKtHgcOd/1zTirLbirLY1ibq12UvH2U46YeChFELNYccqvXd4Rk6n84RF+eDxwRTCY6HVatHr9eh0OvR6ffOr5fHB0MnhLx8fn14XOvmzCCGw2cqP8MwtliwaGvKBQ/Myel0EBj+3R24wJOJn6NvCOz/5/o6KomwSQrSZtic9+k7GYnNw/Xtp3Hx6P8b3O0oN7X0/Qkk6XPjKESJfaC7kuh+uw1pTyaL9f6HsgzVsj5yCdXgwOh8bUbEluBwHKNq1ney0Q1vbnXb1P0lMSSUkKgYEOCqtOMusWPaXtJlSeMRkpkpxe99BenRxRjSBpiNEXWXo2nRCl8t13F7z4ec6Ks4ajeYIUQ4ICGhTpI92rNPpel1YpDNxOhtxOOtwOupwNL3sjhoaLNmHeed1zfeoVHoMhiSMxqFERFzQJOaJTd65Z/Yy6IlIoe9kNmZXsWZ/BbNP69t+IyFg5XwIjIXhl7a6lFubyw3Lr2fcugj6l05nvTEZEV2Bf8ABfMVKqoqyqC1q2tou9VSSUlKJHzYSrdoHW14ttt11lP+QgS23FmFtHSJQGbWoA/Vowwz4JAe3EnFNkB6Vv+6E0wntdvsR8eaGhoYOC7a9qQTCsdBoNEcIrtFo7JAgH35OivPREcKF01nfJM5mHM5DQu10mJt/dzhb/O6ow9l8bMbhMCNE+xPEen0kBkMSEREXNsfN3d55n5PSO+9spNB3Mmszy9GqFcbEH2X7v+xVkL8Bps0H9aF88/SsPXy2+CMuLL8Ou6imTL0XrKuwN9ZRWQeRfZMZd/EVJKWkEhocgz2vjsbsWqo37MVeZHZ/q1VAE27AMCIMXawRTYhP8yKfPxsTd7lc7WZ2tDeBeDShVqvVRwiuv79/uwLc3rFOp/PaCoeeRAgXLlcjLlcjTpcVl7MRl8vadNzYfM3ltOJyWXE0i3cLwXYeFOaWgl3PERMxR6BCo/FHozGiUfuj1hjR6cIxGJKazhnRaIyoD7Y5eE4bgK9PLBpNz9uDtych3y2dzLrMCkbFBeN7tGXvqxaAXziMuhqn3UXW1jLWfbWGqrwsguylWBybAZd7a7sRKSSNTCWmzyBUFcJd8fB/lZTUFAOg6FToYo0YT491l7ONC0Dle+Q/qxDiiBS9Y4l2Q0NDu4+g0+maszv8/f0JDw9vFW9uOYHo6+uLXq/v9eLsFlo7QtiaRNV26CVsiObjlkJsayHG1hZCbGt9rql9K8E+eF9T+6N5zEdDpdKhVhsPCbXGiMGQ0CzY7nP+zWJ98Lj5mtoftdqvV6Yl9hZ69zuvm6mx2EkvqOH2s5Lbb5S/CbJ+oyzlSTYs/oUDmzdia8hEuNzLy/18DAw+8wISY0dhdAZjzzNjW1FHnT0TAHWgHl1CIPo4I7qEQHdhrKa8cbvdTlFpSfMqx/Ly8lYC3l62h0qlaiXOkZGRR0wUHp754S2iLYRACEcL0WtDCJsFtBGXcIutOEyED/4uOniu+XzLdqJjYaejo6BS+aBS6VGrfVCpdKhU+uZzGrUfKl1I0zn3S910TaVu+qnSHXGu+VilO9SXxg+NxohK5V3F4ySdj3e8W3sJfxyoQAiY0M4kbFVJKRsXvUVmwV+x7PoJsAMq+jgCMQb1ZUjqRRgajDj2W2CfBbPKgraPP36pke4ytvEBaILcb0qz2UxucTHF64pbCfvBLCqtVkt4eDhBQUFERUUdVbT1+s6teS6E89DXf6cZp8PcIqRw0GM9WljhoDjb2jjX2OrldFppmYlxfChNAqhDUXTNvze/FB2KSodGHXBILJvaKYe1a+6njXMHz6tbCLdK5YNKrW86p0dRtNIzlnQ6Uug7kXWZFfhq1YyICQLA5XJSvH8fO35bQ2baBiw1BahQE+qbwICgwfRxGvAJjEWrdVdvVIrVaOL1GIaHoUsIQBdjRGigoqKCvJJiijdubbMMbEBAAJGRkQwcOLB5lWNwcPBxpeq1lRnRegLOfOhaq8m2pnuc5qaY7p9DUbQtPNSW3qnbq9Vqg9C3OndIHNv0XNVteLzNHm1rUVeU3leYTCJpiRT6TmRtZjmpCcHkbP6DPevWkrU5DZXNRahPNMmGUfSJnUaAKgClacPuKlHKJlMuE049E1P/GBxGFaVlpWQWF1G8Ywslv5RQWlranCaoUqkIDw+nX79+zYIeERGBwdC6up7L5cBqzaGhIQ+Ho7b9rIjDBPxYmREHUal8W8VzNWojen1Ei+MW1zRG1GoDKrVva2FuDk0c9GRl5otE0lVIoe8kyuoa2V9cw7SKlWxfaybcN4kpYddgbM71deGszMJelYZ5kJEHIn4hyNiHS/tcyu8FWyje9F2rMrC+vr7NZWAPFpgymUytYuN2ezUWy24Ki7Kal3/XWw7Q0JDTTry4KTOiSYjdmRFhh2VGtJ5kaynYGo0/arU/KlXP3oxaIjnZkELfSazZVcAlJT8zPiCFqLC+ONQK2lgjKqWUvF8/payuiPoYAwXjR1BeV8PIsnFQBhuyNhAaGkp0dDQpKSnNnvrBMrAulx2rNZ96SwYFBe5FJQdXC9rtlc3jK4oWX994/AyJhJnOcq8Y9E1Aow08JOwyM0IiOSmRQt8JmCsrKH77Va4N/Qs+GiPbE6ox68oo3LePGq0WMTAKiEKlclFmLUIdquaS0ZcQHx1PREQEOp0Ou73KLeD12ygtXcqB7Cwszd75oRWeWm0ofoYkwkx/weDXt7mGh49PLCqV/OeUSCRH0iFlUBRlKvAioAbeFEI8ddj1O4GrWvQ5CAgTQlQe696eTnleDmueeZtzjFOpwspPpnQqiysx1NcTZLHQf+Ag4gPzySj5iifDNEyMHMDckdfgasygzvwNJaUHvfOq5j4VRYfBEI+fX1/Cwqa0Wimo1XrnJsoSicR7OWZRM8U9S7YXmALkAxuBK4QQO9tpPx2YI4Q488/ee5CeUtQsd/s29r3+CwP8U9kq8tlkyETnspO68XeSzh2JbtooGhoz2b/nfaoNakwaUCmH/t46nemIgkxu7zxGeucSieRPcaJFzU4B9gshspo6+xi4EGhPrK8APjrOe3sMu1b8inlZLn39U1iu2kGhrgQTMNLyGba5Ng4oeZDzNUIoVBjUuFQhxMVdTIB/cgvv3PObYkskkt5PR4Q+GshrcZwPjG2roaIoBmAqcMtx3HsDcANAXFxcB8zyDEIINn+4FMNmNTpfE59oN2BRW1Dbaxme+BWNSYLw8Gn0iZzJd9nreDbjQ85QB/PsFb+gU588tcMlEon30JEVNW2labQX75kOrBFCHEwH6fC9QojXhRBjhBBjwsLCOmBW9+NyOln/wgcEbwsgR1fHl/pNCIMLY1AOEycuwxmjZsiABQwdspDPCvbwTMZHnGO2MP/MhVLkJRKJx+iIR58PxLY4jgEK22l7OYfCNn/2Xq+m0WJh69OfEd4Yy6/adPK0lcRGBZHc73fsbEedZSB15pf4Bifx0paXeCP9DS5osPNI0BjUUaM8bb5EIjmJ6YhHvxFIVhQlUVEUHW4x/+rwRoqiBAKnAcv+7L3eTl1xObsf+Qa9LZTPdOvI01QydrSWfv3fwW5LR/2ZH3Hjl2EI6cs3Wd/wRvobXBwwkEeLi1BPvtPT5kskkpOcY3r0QgiHoii3AD/gTpFcIoTIUBRldtP115qaXgT8KISoP9a9nf0QXUnZ9kwqPthFkcbGeu0uNGrBmZNzsLtWoi40YHzLwDMpc/loUCLF9cU8uf5JRpqGc/+ujagSJ0NsqqcfQSKRnOR0KIdPCLEcWH7YudcOO34HeKcj9/YU8r/fTP2KUtbossjTVBJmNDNk9GocooqQ9CT0bxby9ITriT1lJALBfWvuwyEcPBE0CrX5G5j5hqcfQSKRSDoUujnpEC5B1lsrKfm9kGU+myjQljEoYTcDRy1F7xNAwqaz8Xk1H3HrXfwelMz4fqF8tPsj1het587R/yF2w7sQkwqJkz39KBKJRCJLIByO02Ina+Fv7K0rIU23n0D/WoYNWIfOv5TY2GsJXmWi/M0XCb3+X3w1aBJk7SI6zMxNvz3PpOhJXNLghJpcmPYsyLoyEonEC5BC34LGojoyX1nNGvZRqCsnKSaTPvEb0OnCGTr0fTQbail4Zg7Gc6cSNmcOa9/bRGKYD89tfQgfjQ8Pn/oAytvnQ8RQ6H+Opx9HIpFIACn0zdRtLSLjf3/wu2YnKr9yRg9IwzegkGDj+Qwb+Sj29Exy77oR31GjiHrqKRwC1mdVMGjwH2RUZDD/tPmE5ayDin1wyRLpzUskEq/hpBd64RKUfb2LPzZsZLM2i4S4/UTHbUI49cT3WUC/QTOw5eSQf9NNaPv0IeaVRaj0erbkVNGgymFf41LOSzqPc+LPhtcmQWg/GDzD048lkUgkzZzUQu9qcJC5ZD0rijdS4V/AmCFp+PrnYikdQeq454iMT8BRVUXuDTcAEPv6YjTBwQCs2leAT9QnhPqauOeUe2Dfj1CSDhcuApXcLUkikXgPJ63Q20vqSXvjZ363pxMYuZdTBmxCuAQVO67lrEv+TWiUP67GRvJvvgVHUTFx77yDLj6++f4vc99ErS/jsYmLCdQFwMr5EBgLwy/z4FNJJBLJkZyUQm9OL+GHT5ezS7eHAYPWExKei7UymYodNzB99jkER/ohXC6K7rmHhs2biX7heQwph8oYrMxbS4X6F/r5nMP4qPFwYBXkb4Bp80Ett9mTSCTexUkl9MIlyPtmB9+k/YTdtJMx/deh1Tmo3PVX6vPOY8ac0QRFuDfaLnv+BWqXf0f4nXcQMHVqcx+1tlruW30frkYTs8bc6j65aj74hcOoqz3xWBKJRHJUThqhd1kdbHzrF34tX0ts//VERu1Dr+1H1i9/QzQmctHcUQSGuUW+6pP/UfHGGwRdfhkh//xnq36e3vA01bYKbMU3MaFvFORvgqzfYMojoPX1wJNJJBLJ0TkphL6hsJZvlnxOnmEjw0evQedrJjTwGtI+moBO58tFc0cRYHKLtHnlSoofeQS/yZOIvO++Vptp/5zzM19lfkWIbRr9TEMx+mjd3rxPEIz5Z3vDSyQSiUfp9UJf8Md+ln7/Bf4Jqxkem4FKhBAf9SYrXlfj46fhwjmjCAh1i7x11y4K/j0Hff/+RD/3PIrm0J+nvKGch9c9zIDgQWz5YyLTTzNBSQbsWQ6n/Rf0Rk89okQikRyVXiv0wiXY8PGvrMn/hn4pK/Hzr8JPfRoxUY+xfNE+fI1aZswZhTHEBwB7cTF5s2ajCggg9rVXUfv7HepLCB5a+xAWu4WZMXeQ5ipjfN9QWHUX6Pxh7CxPPaZEIpEck14p9I11DSxb/DE1QcsZlrIVl01NdMj9BPjP4JuXtmEI1DFjzij8g90i7zSbyZs1G1d9PfEf/h/aiIhW/S3dv5Tf83/nrtS7OHDAH52mgtHGSshYCuNuAUOIJx5TIpFIOkSvE/rC3bl8uexNIpN/IimolPpCE2Mnv4WzMZqvX9qGf5CeGXNG4RekB0DY7RT8ew6N+/cTu3gxPgMGtOovvy6fpzc8zSmRp3DVoKuY/vMaRscFo//jRVBp3UIvkUgkXkyvEXohBH98s5Lt+W/Sb9RGcAmqM0ZxztWvU12q8O2ibRhDfLhwzij8AvXN9xQ/8gj1q1fT57FH8Z84oVWfTpeTe1ffi0pR8diEx6ixONhZVMsDEwNg08cw+lowRrRljkQikXgNvaYefU1pIQUND9FvwFqsZb5Ydl/IBdd/QFUxfPvyNgJMvsz4T0qzyANUvPEm1Z9+RujsWQRdcskRfb6/8302l27mv6f8lz7+ffgjqwIh4Dzzp+4GE27vrseTSCSS46bXePR+QcEE6JwUrIskKvIqzrxtFvm7qln+WjpB4QYu/PdIfI265vY133xL2XPPEXD++YTdfqRg763ay8ItCzkz9kwu6HsBAGszK4jT1RG27xMYfjkExR5xn0QikXgbvUbohVAoXZ/CwFMnknrBxeTsqOD7xTsI7mPgwttH4eN/qDSBJS2NonvuwXfMaPo88XirXHkAu9POvFXzMOqMPDj+webrazPLuTtoBUptI0yc063PJ5FIJMdLrxF6nY8vVzz6LGqNlgPby/n+9XRCo/y54PaR+PgdEvnGrAPk33wL2pgYYl9+GZVOd0Rfr2x7hT1Ve1h4xkJCfNwZNSW1VsrKSpji9w0MmQGmft32bBKJRHIi9BqhB1BrtGRtLeOHN3ZgijVywW0j0BsOibyjspK8WbNArSb29cWog4KO6GNr6VaW7FjCRf0u4oy4M5rPr8us4B/qH9E562HS3G55HolEIukMepXQZ24u5cc3MwiLNzL9tpHofQ89nstqJf/Gm3CUlhL/7jvoYo+Mr1vsFuatnkcfvz7clXpXq2sb9+Ryp/Z7RPI5KJHDuvxZJBKJpLPoNUJvNdv55b1dhCcEMP3WEehaiLxwuSi8624atm8n+oUX8B05ss0+FqQtIL8un7fOeQt/nf+h+4Ugcv/HBGGGyXd2+bNIJBJJZ9JrhN7HX8sFt40kJMoPnU/rxyqdv4C6H38k/O67CTjn7DbvX5W/iv/t/R/XDL6G1MjUVtfySqu4zP4lRaGn0Cc2tc37JRKJxFvpNXn0AJFJgUeIfOWHH1K5ZAnBV11FyD+uafO+ams1D659kH5B/bg15dYjrpeufItwpRrXRBmbl0gkPY9e49G3Rd1vv1Hy2OP4n3EGEfPuOSKN8iCPr3+cKmsVi85ahF6tb33RaSdxzxukK/0ZOrLtbwMSiUTizfQqj74lDTsyKPjPXHwGDSJ6wXwUddsbdi/PWs732d9z48gbGRQ66IjrYvv/CHWUsC76HyiqXvvnkkgkvZhe6dHbCwvJu3E26qBAYl59BZXB0Ga7kvoSHlv/GMPDhvPPoW1sHOJyYv99AZmuOIKGT+9iqyUSiaRr6HUuqrOujrxZsxANVuIWL0YbHt5mOyEED6x9AIfLwRMTn0CjauMzb9dX6KozWeS4kHH9TF1suUQikXQNvUrohc1G/m230Xggm5iXFqJPTm637Sd7PmFt4Vrmjp5LfEB8G50JWLmAYm0MOwJPJzak7W8FEolE4u30GqEXQlD00MNY1v1Bn0cewW/cuHbb5tTmsCBtAROiJnDpgEvbbrTvJyhJZ5F9Oqf2a/tbgUQikfQEeo3Qu2pqaNi8GdNNNxE086J22zlcDuatmodOrePh8Q+3nYkjBKyaj80/mo+s4xjXN7QLLZdIJJKupddMxqqDgkj47FNUfn5HbbdkxxK2l2/nmcnPEOHXzqYh2ashbz0bku/GUa6RQi+RSHo0vUboAdT+/ke9vrNiJ69ufZVzE87l3MRz22+4aj74hfN2wySSw12EG3062VKJRCLpPnpN6OZYNDobmbdqHsE+wdx76r3tN8zfBFm/4Tj1Ztbm1DNeevMSiaSHc9II/UubXyKzJpNHJjxCoD6w/Yar5oNPENsiZ9JgdzKur0yrlEgkPZuTQug3Fm/kvZ3vcdmAy5gYPbH9hiUZsGc5jJ3N6pxGFAXGJUmPXiKR9Gw6JPSKokxVFGWPoij7FUX5bzttTlcUZauiKBmKovze4ny2oijpTdfSOsvwjmK2mblv9X3EGmP5z+j/HL3xqudA5w9jZ7E2s5yhUYEEtti4RCKRSHoix5yMVRRFDSwCpgD5wEZFUb4SQuxs0SYIeAWYKoTIVRTl8MTzM4QQ5Z1od4d5euPTFFuKeXfquxi0R1n0VJEJGV/AuFto0ASyJbeaayckdJudEolE0lV0xKM/BdgvhMgSQtiAj4ELD2tzJfCFECIXQAhR2rlmHh8rclfw5f4vuW7odYwMb3uzkWZWPw8qLYy7hbScSmxOl0yrlEgkvYKOCH00kNfiOL/pXEv6A8GKovymKMomRVH+3uKaAH5sOn9De4MoinKDoihpiqKklZWVddT+dqloqODhdQ8zMGQgN4648eiNa/Jh28eQ8ncwRrA2swKNSiE1IeSE7ZBIJBJP05E8+raKuIs2+hkNnAX4AusURflDCLEXmCCEKGwK5/ykKMpuIcTKIzoU4nXgdYAxY8Yc3v+fQgjBI+seoc5Wx5tnv4lWfYw4+5qF7keacDsAazMrGBkbhJ++Vy0zkEgkJykd8ejzgZY7accAhW20+V4IUd8Ui18JjAAQQhQ2/SwFluIOBXUpX2V+xYq8FdyecjvJwe0XNgPAXAqb34Xhl0NQLLVWO+n51YyX1SolEkkvoSNCvxFIVhQlUVEUHXA58NVhbZYBkxRF0SiKYgDGArsURfFTFMUIoCiKH3A2sKPzzD+SQnMhT254ktERo7l60NXHvmHdInA0wsR/A7AhqxKXQC6UkkgkvYZjxiaEEA5FUW4BfgDUwBIhRIaiKLObrr8mhNilKMr3wHbABbwphNihKEoSsLSpcJgG+FAI8X1XPYxLuLhvzX0IIXhswmOoVW3vKtVMQxVsfAuGzACT2/Nfm1mBXqNiVFxQV5kpkUgk3UqHgtBCiOXA8sPOvXbY8bPAs4edy6IphNMdfLDzAzYWb+SR8Y8QY4w59g0b3gBbHUw6tOn32sxyUhNC0GuO8SEhkUgkPYReszK2prGGl7e+zOkxpzOj34xj39Bohj9egf5TIXIYAOXmRnYX18m0SolE0qvoNWklgfpA3jz7TaL8o9quMX84m952h24m3dF86o+sCkDG5yUSSe+i1wg9wPCw4R1raLfC2pcgcTLEpjafXptZgVGvYVj0UYqeSSQSSQ+j14Ru/hRbPwBzSStvHmBdZgVjk0LQqE/OP4tEIumdnHyK5rTD6hchJtXt0TdRWN3AgfJ6WZZYIpH0Ok4+oU//FGpy3Zk2LWL5azNlfF4ikfROTi6hdzndpYgjhrqzbVqwNrOcED8dHepjiQAACYNJREFUAyKMHjJOIpFIuoaTS+h3fQ0V+2DSf1p580II1mVWMC4pFJWqAxk7EolE0oM4eYReCFi1AEL7weDWefbZFRaKaqwyf14ikfRKTh6h3/cTFG+HiXPgsNIIazPde6JMkIXMJBJJL+TkEHoh3Jt+B8bC8MuOuLw2s4I+gT4khB5lByqJRCLpoZwcQp+9GvLWu+vNH1ab3uVqis/3De3YilqJRCLpYZwcQr9qPviFw6gjyxbvKamjst7GeJk/L5FIeim9X+jzN8H/t3e3sXWWdRzHv78+se6BdQ88boxtMMQhTGDAxqIhomZRI7zAhBiFKImSgKAxMegb3xpBfEhEghMlkbAYXAJRghoUwbXAYEPcg4SeMkrdoO3pniisW9e/L84ZdFu3HaDt1V337/OmPfe57nP+50rPr3f/5+p9dzwJy2+BxuYj7j64ft4fxJpZrvIP+qd/ApNa4LKbRry7rdTL/FmTmdNy5C8BM7Mc5B30b26Cl/8MV9wMJx35j1CDB4Z4tqPPlw00s6zlHfRP3w1NU+GKb45498Ztu9kzMOjTHphZ1vIN+nIJNq2BpV+HyTNHHLK2vbJ+ftlCB72Z5SvfoP/XT6GuEZbfetQhbaUy558+jdlTTxrHwszMxleeQb+rC/69Gi65AaadNuKQgcEDrNva59U2Zpa9PIN+7S+AgBW3HXXIhs6dDAwOef28mWUvv6B/qxvWP1A51UHLvKMOay2VqRNcvmDk/r2ZWS7yC/pn7oHBgcrJy46hrdTLhXNbmN7ceMxxZmYnuryC/p0d8NwquOBamL3oqMP6BwbZ0LnTyyrNrBDyCvrnfg379lQuE3gM67b2MTgUDnozK4R8gn7grUrb5ryVcPqFxxzaVirTWC+Wnu3+vJnlryF1AaOmsRk+d1flClLH0Voqc/G8GTQ31R93rJnZiS6fI/q6erjwOjjz48cctuvt/WzctsttGzMrjHyCvkbPvFomwpcNNLPiKFzQt7b30txYz5K5LalLMTMbF8UL+lKZyxbMpKmhcC/dzAqqUGnXvWcvr3S/5f68mRVKoYK+rXrZQAe9mRVJ4YJ+2qQGLjhzeupSzMzGTaGCvrVUZtnCWdTXKXUpZmbjpjBB/3rf23T2vc0Kt23MrGAKE/Tv9ue9ft7MCqamoJe0UtLLktol3XGUMVdJelHSJkn/fD/7jofWUi+zpzax6NSpqUowM0viuOe6kVQP/BL4DNAFrJP0aERsHjamBbgHWBkRnZJOrXXf8RARtJbKLD9nNpL782ZWLLUc0V8OtEdER0TsA1YD1xw25svAmojoBIiI7vex75gr9fTTvWfAyyrNrJBqCfo5wOvDbndVtw13HjBD0pOSXpB0w/vYFwBJ35D0vKTne3p6aqu+Rm2lXsDr582smGo5TfFIvY4Y4XEuBa4GmoE2Sc/UuG9lY8R9wH0AS5cuHXHMB7W2vcyclmbmzZw8mg9rZnZCqCXou4Czht2eC2wbYUxvRPQD/ZKeApbUuO+YGhoK2jrKfHbxae7Pm1kh1dK6WQcskrRAUhNwPfDoYWMeAT4hqUHSZOAKYEuN+46pzdt3s+ud/Vx5rts2ZlZMxz2ij4hBSbcCfwHqgfsjYpOkm6v33xsRWyQ9DrwEDAGrImIjwEj7jtFrGdHB9fPLF3r9vJkVU02XEoyIx4DHDtt272G37wTurGXf8dRa6mXhKVM4ffqkVCWYmSWV9X/G7j8wxHOv9nm1jZkVWtZB/1LXTvr3HeDKc9y2MbPiyjroW9sP9ud9RG9mxZV30JfKLD7jZGZMaUpdiplZMtkG/d79B3ihc4f782ZWeNkG/frXdrBvcMjr582s8LIN+tZSmfo6cdn8malLMTNLKtugX1vq5aK505k2qTF1KWZmSWUZ9Hv27uelrl2s8LJKM7M8g37d1j4ODIU/iDUzI9Ogb20v09RQxyVnz0hdiplZcnkGfanMpfNmMKmxPnUpZmbJZRf0O/r3sXn7brdtzMyqsgv6to7KaQ+8ft7MrCK7oG8t9TKlqZ6L5rakLsXMbELIMOjLXL5gJo312b00M7MPJKs0fGPXXjp6+n1aYjOzYbIK+raOXgCW+4NYM7N3ZRX0re1lpjc3sviMk1OXYmY2YWQT9BFBa6nM8oWzqKtT6nLMzCaMmi4OfiIYGBxixbmzWHGu+/NmZsNlE/STGuv58XVLUpdhZjbhZNO6MTOzkTnozcwy56A3M8ucg97MLHMOejOzzDnozcwy56A3M8ucg97MLHOKiNQ1HEFSD/Ba6jo+pNlAb+oiJgjPxaE8H4fyfLznw8zF2RFxykh3TMigz4Gk5yNiaeo6JgLPxaE8H4fyfLxnrObCrRszs8w56M3MMuegHzv3pS5gAvFcHMrzcSjPx3vGZC7cozczy5yP6M3MMuegNzPLnIN+FEk6S9I/JG2RtEnS7alrSk1SvaQNkv6UupbUJLVIeljSf6s/I8tT15SSpO9U3ycbJT0kaVLqmsaTpPsldUvaOGzbTEl/k/RK9euM0XguB/3oGgS+GxEfBZYBt0hanLim1G4HtqQuYoL4OfB4RJwPLKHA8yJpDnAbsDQiPgbUA9enrWrc/Q5Yedi2O4AnImIR8ET19ofmoB9FEbE9ItZXv99D5Y08J21V6UiaC3weWJW6ltQknQx8EvgNQETsi4idaatKrgFoltQATAa2Ja5nXEXEU0DfYZuvAR6ofv8AcO1oPJeDfoxImg9cDDybtpKkfgZ8DxhKXcgEsBDoAX5bbWWtkjQldVGpRMT/gLuATmA7sCsi/pq2qgnhtIjYDpUDR+DU0XhQB/0YkDQV+CPw7YjYnbqeFCR9AeiOiBdS1zJBNACXAL+KiIuBfkbpz/ITUbX3fA2wADgTmCLpK2mrypeDfpRJaqQS8g9GxJrU9SS0AviipK3AauBTkn6ftqSkuoCuiDj4F97DVIK/qD4NvBoRPRGxH1gDXJm4pongTUlnAFS/do/GgzroR5EkUenBbomIu1PXk1JEfD8i5kbEfCofsv09Igp7xBYRbwCvS/pIddPVwOaEJaXWCSyTNLn6vrmaAn84PcyjwI3V728EHhmNB20YjQexd60Avgr8R9KL1W0/iIjHEtZkE8e3gAclNQEdwNcS15NMRDwr6WFgPZXVahso2KkQJD0EXAXMltQF/BD4EfAHSTdR+WX4pVF5Lp8Cwcwsb27dmJllzkFvZpY5B72ZWeYc9GZmmXPQm5llzkFvZpY5B72ZWeb+D8b79SC/k+j9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Graphing Optimal Escape Probabilitys:\n",
    "for n in range (3,12):\n",
    "    Best_Policy, Best_Value = Find_Optimal_Policy(num_lily_pads = n)\n",
    "    Steps = np.arange(1,n)\n",
    "    plt.plot(Steps,Best_Value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, so I read the problem and wow, imma just read that again and try an take some stuff away from it.... ALright, so we have infinite states and actions meaning the values that s or a can take are  $-\\infty \\to \\infty$. If you take action a you then transition to the next state s' (which again, is any realy number) with a probability of the normal distribution that has a fixed varience. The normal distribution is centered around the state that you are starting in so it's likely that you will just kinda stay around your starting state for a while. Anyways, each time we transition we have a cost of $e^{as'}$ which we want to minimize. It is important to note that the action a does not effect what s' we transition to and only effects the cost\n",
    "\n",
    "Let's solve this logically first before we do it mathematically. Given that we want to minimize $e^{as'}$ we know that the smallest value that we can get is when $as' \\to -\\infty$. We also know that a doesn't effect the s' and only s effects s'. This means that we can link a and s' to s.\n",
    "\n",
    "Okay, some math: (Normal Distribution)\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{P}(s'|s) = \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left( \\frac{s' -s}{\\sigma}\\right)^2}\n",
    "\\end{equation}\n",
    "\n",
    "I expect that this approach is going to be similar to how I approached problem 2. Since $\\gamma$ is 0 in this senario it makes sense that we can calculate the expected value of the cost for a state. we can do this by multiplying all of the probabilites for each of the potential states with there associated costs and summing them up. However the state space is continuous so it makes sense to just integrate over it. \n",
    "\n",
    "Let the cost function be:\n",
    "\n",
    "\\begin{equation}\n",
    "C(a,s') = e^{as'}\n",
    "\\end{equation}\n",
    "\n",
    "We then have that the expected cost function for any s should be:\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle C(a,s') \\rangle = \\int_{s' = -\\infty}^{s' = \\infty}\\mathcal{P}(s'|s) C(a,s')\n",
    "\\end{equation}\n",
    "\n",
    "Once we have the expected cost function we can take the derivative of it with respect to the action and then we can solve for action as a function of s.\n",
    "\n",
    "**Multivariate log normal in appendix of the book or use wikipedia**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
