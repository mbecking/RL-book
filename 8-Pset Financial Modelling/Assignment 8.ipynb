{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume you are the owner of a bank where customers come in randomly everyday to make cash\n",
    "deposits and to withdraw cash from their accounts. At the end of each day, you can borrow (from\n",
    "another bank, without transaction costs) any cash amount y > 0 at a constant daily interest rate\n",
    "R, meaning you will need to pay back a cash amount of y(1 + R) at the end of the next day. Also,\n",
    "at the end of each day, you can invest a portion of your bank’s cash in a risky (high return, high\n",
    "risk) asset. Assume you can change the amount of your investment in the risky asset each day,\n",
    "with no transaction costs (this is your mechanism to turn any amount of cash into risky investment\n",
    "or vice-versa). The key point here is that once you make a decision to invest a portion of your\n",
    "cash in the risky asset at the end of a day, you will not have access to this invested amount as\n",
    "cash that otherwise could have been made available to customers who come in the next day for\n",
    "withdrawals. More importantly, if the cash amount c in your bank at the start of a day is less than\n",
    "C, the banking regulator will make you pay a penalty of K ⋅ cot(\n",
    "π⋅c\n",
    "2C\n",
    ") (for a given constant C > 0\n",
    "and a given constant K > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# States:\n",
    "\n",
    "c = cash in bank\n",
    "b = amount borrowed\n",
    "i = amount invested\n",
    "\n",
    "$\\mathcal{S} = \\{(c,b,i) \\in ([0,\\infty),[0,\\infty),[0,c+b])\\}$\n",
    "\n",
    "# Actions:\n",
    "\n",
    "$\\mathcal{B} =$ Borrow money from bank (Continuous action) this is an amount\n",
    "\n",
    "$\\mathcal{I} = $Invest Money into risky asset (Continuous action) this is an amount\n",
    "\n",
    "$\\mathcal{A} = \\{\\mathcal{B},\\mathcal{I} \\in ([0,\\infty),[0,c])\\}$\n",
    "\n",
    "# Rewards:\n",
    "\n",
    "The rewards can be split up into 2 separate conditions, we are making some amount on our risky asset (or losing) and we conditionally have to pay a fine if we go below a certain amount. There isn't a penalty for not being able to pay back the other bank, and there also isn't a penalty for not having enough money for people to withdraw. \n",
    "\n",
    "\\begin{gather}\n",
    "\\mathcal{R}(s,s') = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "\\mathcal{N}(\\mu, \\sigma^2) -K \\cot \\left(\\frac{\\pi \\cdot c}{2C}\\right) & c < C \\\\\n",
    "\\mathcal{N}(\\mu, \\sigma^2)  & s \\in \\mathcal{S}\n",
    "\\end{array}\n",
    "\\right\\}\n",
    "\\end{gather}\n",
    "\n",
    "\n",
    "\n",
    "# Transtions:\n",
    "\n",
    "If we include the reward in the transition space then we can figure out exactly what next state we are going to and therefore the probability of getting there is 1. I think that there might be a place to implement a probability that depends on the distribution of the reward that we can get from our risky asset, but I am not sure.\n",
    "\n",
    "\\begin{gather}\n",
    "\\mathcal{P}_T(s,a,s') = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "1 & s'(c' = c + \\mathcal{B} + \\mathcal{R}(s,s'),& b' = \\mathcal{B}, & i' = i + \\mathcal{I})& s \\in \\mathcal{S} \\\\\n",
    "\\end{array}\n",
    "\\right\\}\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the MDP\n",
    "\n",
    "To actually solve for this MDP, I would use Approximate dynamic programming. Since we have a continous action and state space, we cannot account for all possible states and transitions and therefore we need to this approximately. Firstly we need to represent our value function and our policy with function approximations. Now we start with any value function and start iterating where we start with a deterministic policy that will be represented by our function approximation. We then solve for the value function using the bellman operator on the previous value functino. We then continue this process until the difference in value functions from the previous until now is small. Then we take a max over all of the actions for that state to decide which action is the correct choice. Finally we have our proper value function with an optimal policy. Both of these are still function approximations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
